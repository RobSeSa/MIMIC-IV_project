{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 244B Project: Medical Code Prediction MIMIC-IV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea8cd9a63d9642ac9fd3ab2efbc21e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dccc6d99748c4864aa45f69157ef9a88",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4185f14b94f34595860b8423db36c426",
              "IPY_MODEL_b7a31430cf3440b592785268a07eed46",
              "IPY_MODEL_7d8c677566124fc38393005aaaed4875"
            ]
          }
        },
        "dccc6d99748c4864aa45f69157ef9a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4185f14b94f34595860b8423db36c426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3d74212526147aa9d00b9ae71f68d6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2837b51200d46878772d10116b48b8f"
          }
        },
        "b7a31430cf3440b592785268a07eed46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68c45272fef445e380b6a473ea08f721",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aafca9973ac04fb7b2389d3842528c4b"
          }
        },
        "7d8c677566124fc38393005aaaed4875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_056b6f9ccbc1478bb38ce940dd589bee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:00&lt;00:00, 2.15kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32c15fbd125d41a19477d0c08a81901a"
          }
        },
        "b3d74212526147aa9d00b9ae71f68d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2837b51200d46878772d10116b48b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68c45272fef445e380b6a473ea08f721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aafca9973ac04fb7b2389d3842528c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "056b6f9ccbc1478bb38ce940dd589bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32c15fbd125d41a19477d0c08a81901a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a14ddb82b6ae4cf4b68f78cafd13c4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc10731b33784f2887afe9a766b5adc5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_486f4a0e1056420cbfd0890e36985e48",
              "IPY_MODEL_ce1c3f6c63cc4b838f2c6abe662046b5",
              "IPY_MODEL_44312e6719e94822bd70a766e92816b2"
            ]
          }
        },
        "fc10731b33784f2887afe9a766b5adc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "486f4a0e1056420cbfd0890e36985e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_15a8cee741944578b738b5c3ecaec74a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddfa436cb642449aaa64d96d404d6f48"
          }
        },
        "ce1c3f6c63cc4b838f2c6abe662046b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_362de3582b5e47bf990573f9321864c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f718913a03cd49aa9bcb971a52d3cea2"
          }
        },
        "44312e6719e94822bd70a766e92816b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_677032e1ec42492c9c4f74cb7a699847",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 208k/208k [00:00&lt;00:00, 7.30kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc6d8a890db44303849130d0e1221de9"
          }
        },
        "15a8cee741944578b738b5c3ecaec74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddfa436cb642449aaa64d96d404d6f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "362de3582b5e47bf990573f9321864c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f718913a03cd49aa9bcb971a52d3cea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "677032e1ec42492c9c4f74cb7a699847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc6d8a890db44303849130d0e1221de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4bc92657cee34705833178195d1154f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ddc813ade7a4a08b92becb1315ff110",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a8a8669974e947358612067f6546ec2a",
              "IPY_MODEL_f68ed96dceab421ca0beed203fa69e43",
              "IPY_MODEL_7402a4ee46174cf58cd76b104a16506c"
            ]
          }
        },
        "8ddc813ade7a4a08b92becb1315ff110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8a8669974e947358612067f6546ec2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8713c78f66a46aab8417821f1e5d57b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_037a3360953249d5b5b083f1e7a1255e"
          }
        },
        "f68ed96dceab421ca0beed203fa69e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_890b1aff425a43adbeade261b255a0a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65d04c9204d846debda066ab59526f74"
          }
        },
        "7402a4ee46174cf58cd76b104a16506c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf1c9571d97a4385a3e351cb82b18a8e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 12.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18abb3254b2649ac95f9e47ff9cc177a"
          }
        },
        "c8713c78f66a46aab8417821f1e5d57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "037a3360953249d5b5b083f1e7a1255e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "890b1aff425a43adbeade261b255a0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65d04c9204d846debda066ab59526f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf1c9571d97a4385a3e351cb82b18a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18abb3254b2649ac95f9e47ff9cc177a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ea0efa4c93d4c0183c63a129200c801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2e9a975483c43f89daaf6d744c1a003",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cce224aa4a6645f8b797386f961bd2c6",
              "IPY_MODEL_e89c14ac2c384647bf51b84d459470ae",
              "IPY_MODEL_3b2d763527a44e6eb198f5d02b73d642"
            ]
          }
        },
        "d2e9a975483c43f89daaf6d744c1a003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cce224aa4a6645f8b797386f961bd2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_737252d098834af484f5279207803952",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_214e7b4b15fa4371b4c356bcc6a7fc84"
          }
        },
        "e89c14ac2c384647bf51b84d459470ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2374a933323544fe8bdcc6d36f3b065e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb8e25f452bd4221a584278fdc3ba9f8"
          }
        },
        "3b2d763527a44e6eb198f5d02b73d642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b98a8c84c0c645c0a7ba1db76d17a4de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 511M/511M [01:00&lt;00:00, 7.20MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fa5abe7f60542d796f4228ebe5271ff"
          }
        },
        "737252d098834af484f5279207803952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "214e7b4b15fa4371b4c356bcc6a7fc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2374a933323544fe8bdcc6d36f3b065e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb8e25f452bd4221a584278fdc3ba9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b98a8c84c0c645c0a7ba1db76d17a4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fa5abe7f60542d796f4228ebe5271ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Medical Code Prediction MIMIC-III\n",
        "Robert Sato\n",
        "\n",
        "Dr. Xin Wang\n",
        "\n",
        "\u0010CSE 244B - Natural Language Processing\n",
        "\n",
        "Winter 2022"
      ],
      "metadata": {
        "id": "IIQBM7m8UckW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "- setup environment\n",
        "    - setup github for this notebook\n",
        "    - do not let the data be publicly accessible\n",
        "- data analysis/preprocessing\n",
        "    - understand the structure of the data\n",
        "    - load into a readable/pliable format (pandas)\n",
        "    - visualize some of the data\n",
        "    - find the clinical notes\n",
        "    - find the codes corresponding to the clinical notes (labels)\n",
        "    - helper functions\n",
        "        - create dataset (X, y)\n",
        "            - shuffled randomized w validation split\n",
        "- load a pretrained model\n",
        "- read in the text as input\n",
        "- multi-label classification\n",
        "    \n",
        "**Note**\n",
        "MIMIC-IV note data is not available...\n",
        "just use MIMIC-III for now\n",
        "\n",
        "mimic-iii-dia-data was created from CAML-MIMIC github\n",
        "- https://github.com/jamesmullenbach/caml-mimic\n",
        "- [Explainable Prediction of Medical Codes from Clinical Text]\n",
        "    - https://arxiv.org/abs/1802.05695\n",
        "\n",
        "\n",
        "- data from CAML has much shorter text (they preprocessed) but their labels include words for some reason\n",
        "- data from ICD prediction MIMIC has longer text but succint label column"
      ],
      "metadata": {
        "id": "SSSLIyTZV1i8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT5ZAtkdToE7",
        "outputId": "20abf8f8-3d92-4469-877a-a9b60145dfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0SKqO78Q3I_M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading from pickle (ignore and use CAML data)\n",
        "- from [Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical Notes]\n",
        "    - https://arxiv.org/pdf/2008.01515.pdf\n",
        "    - https://github.com/3778/icd-prediction-mimic\n",
        "- using other data bc already preprocessed (somewhat)"
      ],
      "metadata": {
        "id": "6L-eB7XiXlmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"/content/gdrive/MyDrive/CSE_244B_W2022/project/mimic3_data.pkl\")"
      ],
      "metadata": {
        "id": "Vq4igaIDXnaG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0_WocoIvXnig",
        "outputId": "fbe6e6cc-532e-419e-c59d-8a4030e63ac3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d56d7de5-88bf-40f4-9b47-9f8cc3065c8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>ICD9_CODE</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100001</td>\n",
              "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
              "      <td>58526</td>\n",
              "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100003</td>\n",
              "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
              "      <td>54610</td>\n",
              "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100006</td>\n",
              "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
              "      <td>9895</td>\n",
              "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100007</td>\n",
              "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
              "      <td>23018</td>\n",
              "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100009</td>\n",
              "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
              "      <td>533</td>\n",
              "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d56d7de5-88bf-40f4-9b47-9f8cc3065c8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d56d7de5-88bf-40f4-9b47-9f8cc3065c8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d56d7de5-88bf-40f4-9b47-9f8cc3065c8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
              "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
              "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
              "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
              "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
              "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
              "\n",
              "                                                TEXT  \n",
              "0  Admission Date:  [**2117-9-11**]              ...  \n",
              "1  Admission Date:  [**2150-4-17**]              ...  \n",
              "2  Admission Date:  [**2108-4-6**]       Discharg...  \n",
              "3  Admission Date:  [**2145-3-31**]              ...  \n",
              "4  Admission Date:  [**2162-5-16**]              ...  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "O5igzzpFXnk3",
        "outputId": "a0607440-a4a2-42eb-c091-6fea810868cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2cb76f51-164b-433e-9aac-8d12bdc5e8be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52722.000000</td>\n",
              "      <td>52722.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>149987.468230</td>\n",
              "      <td>35177.283449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>28910.706251</td>\n",
              "      <td>28570.362362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>100001.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>124922.250000</td>\n",
              "      <td>12566.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>150098.500000</td>\n",
              "      <td>25251.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>175033.250000</td>\n",
              "      <td>57170.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>199999.000000</td>\n",
              "      <td>99999.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cb76f51-164b-433e-9aac-8d12bdc5e8be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cb76f51-164b-433e-9aac-8d12bdc5e8be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cb76f51-164b-433e-9aac-8d12bdc5e8be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             HADM_ID    SUBJECT_ID\n",
              "count   52722.000000  52722.000000\n",
              "mean   149987.468230  35177.283449\n",
              "std     28910.706251  28570.362362\n",
              "min    100001.000000      3.000000\n",
              "25%    124922.250000  12566.000000\n",
              "50%    150098.500000  25251.000000\n",
              "75%    175033.250000  57170.500000\n",
              "max    199999.000000  99999.000000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('average diagnosis length: ', data.TEXT.str.split().str.len().mean())\n",
        "print('stdev diagnosis length: ', data.TEXT.str.split().str.len().std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bSUDsIhlpdc",
        "outputId": "2e5a936d-92b4-4ce9-ea12-35980076432d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average diagnosis length:  1551.5281666097644\n",
            "stdev diagnosis length:  781.001080162633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Data\n",
        "- MIMIC-IV note data not available\n",
        "- TF, use MIMIC-III\n",
        "\n",
        "Data Needed for Diagnosis Classification\n",
        "- NOTEEVENTS.csv - actual notes from doctor's diagnoses\n",
        "- ADMISSIONS.csv - admission info (in time/out time)\n",
        "- DIAGNOSES_ICD.csv - ICD to predict"
      ],
      "metadata": {
        "id": "cfOZb1ydVKvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MIMIC_IV_data_path = \"/content/gdrive/MyDrive/CSE_244B_W2022/project/mimic-iv-1.0/\"\n",
        "MIMIC_III_data_path = \"/content/gdrive/MyDrive/CSE_244B_W2022/project/mimic-iii_dia_data/\""
      ],
      "metadata": {
        "id": "KMDortreUHcD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $MIMIC_III_data_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjiv8ZHUUHeT",
        "outputId": "1270c46b-6a5a-42b4-9a44-9a774ec21309"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIA_PLUS_adm_test.csv  DIA_PLUS_adm_train.csv  DIA_PLUS_adm_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(MIMIC_III_data_path + 'DIA_PLUS_adm_train.csv')\n",
        "test = pd.read_csv(MIMIC_III_data_path + 'DIA_PLUS_adm_test.csv')\n",
        "val = pd.read_csv(MIMIC_III_data_path + 'DIA_PLUS_adm_val.csv')"
      ],
      "metadata": {
        "id": "8_MBLD6b2_6g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nJZmPBPn2_9z",
        "outputId": "d9bc91c5-7548-409f-9e31-087b6d2f7d8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d3fc24bc-693c-4c29-9827-a463619571c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>159643</td>\n",
              "      <td>CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: This 60 ...</td>\n",
              "      <td>,276,2765,2767,412,414,4140,424,4241,427,4273,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>147171</td>\n",
              "      <td>CHIEF COMPLAINT: Substernal Chest Pain\\n\\nPRES...</td>\n",
              "      <td>410,4101,414,4140,427,4271,4273,428,4280,4282,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>199961</td>\n",
              "      <td>CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: The pati...</td>\n",
              "      <td>,250,2506,285,2859,403,4039,707,7070,720,7200,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>136812</td>\n",
              "      <td>CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: This is ...</td>\n",
              "      <td>,244,2449,276,2765,280,2800,426,4261,427,4278,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>175700</td>\n",
              "      <td>CHIEF COMPLAINT: s/p rollover MVC with prolong...</td>\n",
              "      <td>285,2851,327,3272,481,518,5180,799,7990,807,80...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3fc24bc-693c-4c29-9827-a463619571c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3fc24bc-693c-4c29-9827-a463619571c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3fc24bc-693c-4c29-9827-a463619571c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id                                               text  \\\n",
              "0  159643  CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: This 60 ...   \n",
              "1  147171  CHIEF COMPLAINT: Substernal Chest Pain\\n\\nPRES...   \n",
              "2  199961  CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: The pati...   \n",
              "3  136812  CHIEF COMPLAINT: \\n\\nPRESENT ILLNESS: This is ...   \n",
              "4  175700  CHIEF COMPLAINT: s/p rollover MVC with prolong...   \n",
              "\n",
              "                                              labels  \n",
              "0  ,276,2765,2767,412,414,4140,424,4241,427,4273,...  \n",
              "1  410,4101,414,4140,427,4271,4273,428,4280,4282,...  \n",
              "2  ,250,2506,285,2859,403,4039,707,7070,720,7200,...  \n",
              "3  ,244,2449,276,2765,280,2800,426,4261,427,4278,...  \n",
              "4  285,2851,327,3272,481,518,5180,799,7990,807,80...  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('average diagnosis length: ', train.text.str.split().str.len().mean())\n",
        "print('stdev diagnosis length: ', train.text.str.split().str.len().std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_B7balkkvjr",
        "outputId": "e9573555-1d0f-4ce6-9141-7d362fdc4678"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average diagnosis length:  395.8866270518327\n",
            "stdev diagnosis length:  235.62215600801855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the labels for one training example\n",
        "labels = train['labels'].iloc[1]\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLyPDGgYlExH",
        "outputId": "3ddad786-f6b3-4954-a067-6449081bcbb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "410,4101,414,4140,427,4271,4273,428,4280,4282,458,4582,997,9971,acute,anterior,artery,atherosclerosis,atrial,cardiac,care,classified,complications,congestive,coronary,elsewhere,episode,failure,fibrillation,heart,hypotension,iatrogenic,infarction,initial,myocardial,native,paroxysmal,systolic,tachycardia,unspecified,ventricular,wall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_labels = list(train[train['id']==100001]['labels'])\n",
        "data_1_labels = list(data[data['HADM_ID']==100001]['ICD9_CODE'])\n",
        "print(train_1_labels, \"\\n\", data_1_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnXJlN09qeAz",
        "outputId": "e7decf8c-9bc6-4a2f-d61b-4aca54e58e8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['250,2501,2504,2505,2506,2508,337,3371,362,3620,403,4039,458,4580,536,5363,578,5780,584,5849,585,5853,707,7078,V135,V1351,V586,V5867,acute,autonomic,background,chronic,classified,current,diabetes,diabetic,disease,disorders,elsewhere,failure,fracture,gastroparesis,hematemesis,history,hypertensive,hypotension,iii,insulin,iv,juvenile,ketoacidosis,kidney,longterm,manifestations,moderate,neurological,neuropathy,ophthalmic,orthostatic,pathologic,peripheral,personal,renal,retinopathy,sites,specified,stage,type,ulcer,uncontrolled,unspecified,use'] \n",
            " [array(['25013', '3371', '5849', '5780', 'V5867', '25063', '5363', '4580',\n",
            "       '25043', '40390', '5853', '25053', '36201', '25083', '7078',\n",
            "       'V1351'], dtype=object)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['id']==100001].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "8rR4pWy3rCrM",
        "outputId": "3184f6aa-bd04-487d-cbc6-7ecfc2d10fe0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2e07b5cb-3488-4878-b931-f0cd3b8cd8e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25218</th>\n",
              "      <td>100001</td>\n",
              "      <td>CHIEF COMPLAINT: nausea, vomiting\\n\\nPRESENT I...</td>\n",
              "      <td>250,2501,2504,2505,2506,2508,337,3371,362,3620...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e07b5cb-3488-4878-b931-f0cd3b8cd8e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e07b5cb-3488-4878-b931-f0cd3b8cd8e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e07b5cb-3488-4878-b931-f0cd3b8cd8e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           id                                               text  \\\n",
              "25218  100001  CHIEF COMPLAINT: nausea, vomiting\\n\\nPRESENT I...   \n",
              "\n",
              "                                                  labels  \n",
              "25218  250,2501,2504,2505,2506,2508,337,3371,362,3620...  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['HADM_ID']==100001].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "01z1rMdrrUQD",
        "outputId": "424a0f66-ac86-4aba-e1cc-d63f34b6ee56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b41f7578-a05c-4e02-a874-d842d510a7a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HADM_ID</th>\n",
              "      <th>ICD9_CODE</th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100001</td>\n",
              "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
              "      <td>58526</td>\n",
              "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b41f7578-a05c-4e02-a874-d842d510a7a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b41f7578-a05c-4e02-a874-d842d510a7a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b41f7578-a05c-4e02-a874-d842d510a7a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
              "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
              "\n",
              "                                                TEXT  \n",
              "0  Admission Date:  [**2117-9-11**]              ...  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create vector representations for the texts"
      ],
      "metadata": {
        "id": "R-u41ixQe3ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the ICD code lists to a one hot vector"
      ],
      "metadata": {
        "id": "9JrISbTVxb2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find all unique codes in the ICD9_CODE column\n",
        "all_codes = set()\n",
        "for i, row in data.iterrows():\n",
        "    codes = row['ICD9_CODE']\n",
        "    for code in codes:\n",
        "        all_codes.add(code)\n",
        "print(all_codes)\n",
        "code_count = len(all_codes)\n",
        "print(code_count, \"unique codes found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mDTj8a6uFlS",
        "outputId": "c49dc4f1-6e76-4386-9797-787dac67c86d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'44289', '0542', '45350', '81383', '76408', '74489', '38905', '07052', 'E8768', '74742', '30491', '20218', '7013', '37820', '41404', '8251', '7859', 'E9687', '72743', 'V580', '73316', '27906', '78799', '75265', '4401', '80182', 'V153', '25030', '20963', '5691', '4440', '8488', '28859', 'E8494', '5272', '9894', '2769', '20903', '71589', 'E8793', '7459', '7012', '4370', 'E8532', '30989', '3492', '80420', '4599', '3221', '3510', '53570', '92810', '5523', '36616', '8371', 'V058', '1516', '76070', '77183', '37741', '3577', '25541', '53190', '01896', '64893', '59010', '4843', 'V1204', '9580', '01504', 'E9470', 'V0259', 'V155', 'E8353', '75261', '1401', '36283', '1965', '9040', '36846', '73002', '99939', '45371', '42781', '53370', '20028', 'V444', '7843', '99769', 'V8745', '64252', '34481', '71530', '71593', '2771', 'V5873', '430', '3337', '78451', '0310', '9065', '7299', 'V6441', '6023', '83920', '83301', '6159', 'E9571', '9054', '7862', '85229', '85406', '44589', '56982', 'V5831', '734', '1410', '5233', 'V420', '1409', '7350', '29600', '9581', '34882', 'E9307', '20288', '9535', '23879', '1913', '20252', '34541', '45374', '4404', '81012', '2156', '86121', '3410', '76711', '2821', '7910', '74912', '2829', '7100', 'E9000', '78961', '25061', 'E8841', '6145', 'V5301', '2140', 'V3101', '30183', '19881', '20382', '2882', '0311', '38600', '3526', '4439', '07799', '1542', '83904', '9553', '6951', '71155', '7530', '28246', '99668', 'V4586', '64843', '37999', 'E8586', '5100', '45620', '2334', '80122', '20070', '5970', '47820', '3308', '7384', '7280', '2250', '2454', '22802', '86113', '34981', '75501', '2449', 'V421', '82382', '64862', '31381', '38900', '2651', 'E9411', '44581', '6938', '2537', '2752', '7613', '74910', '81219', 'V1559', '41020', '46410', '1905', '78061', '0527', '55092', '47400', 'E8700', '78602', '4772', 'E8718', '75470', 'E8654', '80123', '47824', 'V4459', '2309', '1361', '20148', '33510', '13109', '5993', '36251', '78830', '82001', '27652', '71104', 'E9425', 'E8840', 'V851', '81601', '30403', '80032', '38917', '1108', '71680', '6950', 'V192', '87269', '7810', '7660', '1623', '7612', 'E9354', '2979', 'E8624', '7994', 'E8851', '9991', '6926', '7512', '8875', '9301', '48241', '4420', '8363', '80610', '7080', '95901', '5890', '37500', '4871', '04105', '34202', '75889', '6802', '38612', '51634', '24960', '73349', '71840', '7701', '9983', '51901', '99883', '9462', '3688', '8190', '7888', '34670', '5771', '38871', 'E9677', '44771', '81249', '81259', '78869', '990', '37612', 'E8140', '2375', '58389', '3154', '66932', '7048', '7420', 'E9351', '71942', 'E9175', '9260', '2469', 'V5861', '5589', '7220', '37530', '37432', '38000', 'V174', '20260', '7700', 'E9670', '71531', 'V173', '83902', 'E989', '3649', '53270', '3550', '45111', 'V1819', '3319', '53510', '6249', '80001', '04089', '5931', '6011', 'V4589', 'E8141', '82380', '81401', '9047', '5720', '7245', '78031', '70521', '3502', 'E9379', '2360', '7937', '481', '53230', '11519', '64783', '9996', '7336', 'E8613', '86414', 'V1079', 'E8162', '7234', '37555', '81202', '29502', 'E8161', '8870', '95209', '41400', '9050', '74353', '2763', '1270', '86814', '85200', '82342', 'E8180', '9198', 'V065', '9597', 'V425', 'E9805', '7482', '5511', 'E9651', '9104', '6021', '27951', '42099', '6143', '85135', '30749', '37956', '1612', 'V4965', '3580', '1348', '31539', '08240', 'V1241', '75982', '59654', '86345', '44032', '73089', '9010', '5291', '5722', 'V554', 'V1209', 'E9421', '20943', '64811', '29622', '3361', '94321', '4928', '42610', '37993', '81392', 'E9018', '62570', '7538', '2339', '95892', '27949', '30303', '2631', '1508', '71909', 'E9229', '6213', '29634', '20205', '7945', '73732', '37632', '0701', '9851', '7209', '0790', '6144', '2860', 'E9225', '74721', '66624', '412', 'V4573', '72293', '72671', '4847', '85256', '72888', '4801', '85222', 'E8740', '79902', '3091', '80502', 'V272', '5824', 'E9850', '24210', 'E9445', '55012', '36253', '36234', '4660', '43321', '1463', '5839', '9858', '7094', 'V062', '9540', '25081', '75450', '80430', '7754', '7759', '36641', '42613', '5060', '38014', '3599', '72690', '6390', '1363', '0919', '65671', '71687', '3203', '56981', '20023', '5969', '56961', '2758', '2555', '5789', '9953', '4230', '1101', '74362', '7511', '55841', '3729', '64683', 'E9353', '30981', '30523', '1892', 'V5869', '76529', '4411', '30012', '9074', '317', '86331', '81201', '7535', '9052', 'V6443', '5258', '30750', '76076', '3619', '20198', '75263', '59589', '73819', '23876', '30285', '8082', '2978', '30481', '81511', '1418', '11284', '74710', '9108', 'E9382', '5305', '37922', '5300', '5278', '9696', '7466', '90441', '94519', '7683', '5950', '5830', '71844', 'E8767', '4019', '63411', '7756', '7705', '48231', '55320', '17332', '58381', '1177', '38100', '80360', '90287', '74363', '01136', '64801', '90082', '55321', 'E0000', 'E8123', 'V024', '85140', '39899', '7449', '71222', '2132', '0400', 'E8791', 'E8211', '49392', '65821', 'E9871', '7212', '7463', '2689', '72290', '37005', '7617', '2142', 'V152', '5560', '40310', '1983', '31401', 'E8810', 'V4576', '53211', '72972', '80034', '86610', '44329', '04182', '70409', '85215', '7802', 'E9854', '1738', 'E9204', '6171', '37272', '44030', '64203', '86502', '80144', '57461', '8364', '4558', '36801', '53551', '37992', '6264', 'E975', '7355', '85011', '59011', '86342', '7768', '81210', 'V2501', '7673', '2830', '81311', '6162', 'E0073', '4808', 'E8240', '86349', '78606', '85224', '43884', '66401', 'V581', '77211', '4786', 'E9250', '7292', '51900', '35800', '67454', '33384', 'V403', 'E970', '01085', '66921', '5695', '2312', '1941', '20078', '80220', '07070', '2559', '9089', 'V0189', '30015', '36511', '7661', '82130', '6251', 'E927', '73734', '63421', '1968', '262', '0478', '9593', '27650', '76503', '01193', '4272', '41000', '9878', '71516', '3344', '75310', '475', '78601', '75312', '3529', '87202', '6923', '41403', '5735', '20920', '38003', '07819', '5650', 'E8849', '5533', 'E8251', '82392', 'E8145', 'E0010', '1561', '3430', '81231', 'E8121', '1961', '86229', '24901', '7806', 'E8881', '8460', '53521', '29640', 'V071', '53261', '64842', '6809', '3200', '29900', '75614', '1885', '6930', '88001', '8191', '5693', '0709', '80315', '0979', '2122', '80011', '2732', '28311', '87343', '2713', 'V1020', '37410', '2879', '3238', '81613', '71489', '2781', 'V553', '3668', '44282', '1628', '2929', '6238', '78605', 'E8129', '2822', 'V020', '57401', '33391', '3491', '8873', 'V078', 'E8190', '4296', '9664', '0846', '9708', '7626', '41031', '42518', '75313', '29564', '36589', 'E8853', '80125', '1731', '78607', '72887', '99701', '52563', '3343', '55090', '2102', '2330', '58889', '20590', '99563', '1125', 'V1084', 'E9100', '7861', '2154', '9972', '9962', 'E8171', 'E9674', 'V1389', '40591', '1540', '71891', '61171', '99582', 'E8540', '1610', '8830', '61689', '4359', 'E986', 'V5041', '8793', '87341', '2458', '84200', '78940', '36552', '8832', '71831', '38912', '00861', '4259', '03844', '76494', '4294', '4781', '1123', '73396', '9189', '74365', '20957', '9724', '8822', '3083', '2865', '8787', '2362', '76389', '3419', '4519', '05479', '75016', '45375', '7635', '64823', '6910', 'E8584', '29681', '20243', '8602', 'E8880', '7488', 'E9803', '80425', '78630', '2388', '43883', '7762', '47874', '38023', '4200', 'V180', '5644', '66021', '6038', '83109', '1622', '7078', 'E8548', '44284', '30592', '27789', '8076', '58281', '1501', '37611', '2873', 'V653', '8785', '8902', '44021', '3960', 'V568', '99802', '3373', '69510', '9690', '8360', 'V8524', '64313', '7388', '63502', '85239', '72882', '4412', '485', '83921', '36289', '25031', '17322', '40200', '66551', '7840', '4481', 'E954', '66574', '3798', '5219', '75611', '8461', '0329', '9081', '20410', '67414', '7630', '7871', '85201', '67432', '47870', '37482', '99679', '81003', '80432', '67004', '0041', '30541', '4920', '58289', '4260', '5600', '92820', '7510', '7608', '9032', '42689', '76526', '8790', '34400', '71926', '45821', '81610', '7633', '5238', '0545', '34510', '28244', '25063', '7103', '80223', '2419', '3910', '37452', '1105', '5994', '3698', '94504', 'V1006', '9140', '5521', '43853', '80232', '6920', '71895', '35922', '85190', '80426', '7601', '7296', '72610', '60091', '9039', '2518', '86415', '1109', '78659', '4532', '78839', '7776', '6072', '75739', 'E9240', '20401', '5723', 'V1381', 'E8556', '2254', 'V1042', '80165', '78499', '80225', '71856', '72763', 'V620', '72660', '79381', '53010', '0380', '99601', 'V1259', '1455', '9090', '8960', 'V4281', '81250', '7942', 'E8554', '2381', '53110', '8024', '75557', '72761', '9219', '71887', '78838', '67154', '7421', '42830', '20974', '3540', '48801', '36565', '79552', '60490', '82021', '8820', 'E8133', '7912', '4878', '20150', 'V3001', '07022', '33989', 'V0389', '2693', 'V1053', 'E9303', '71509', '56941', '8977', '5234', '65951', '23770', '8703', '95204', '9694', 'E8192', '99562', '9001', '7429', '38010', '77212', '44031', '9152', '24280', '77439', '94325', '38420', '7402', '74560', '20281', '0270', '80106', '986', '27903', '63380', '3051', 'E8257', '51884', '7260', '7615', '98989', '81301', '71949', '3578', '39891', '80504', 'E8110', '99581', '72619', '71784', '56401', '5173', '9243', '75321', '9517', '5888', '55010', '56213', '8748', '80136', 'E8318', '51911', '393', '81254', '72886', '99812', '4139', '37300', '87200', '33182', 'E0032', 'E9572', '85316', '9031', '86819', 'E8405', '30250', '71855', '80602', 'V1072', '5880', '80501', '7825', '80704', '3480', '81200', 'E9559', '436', 'E9352', '40311', '64844', '51283', '6031', '64781', '4910', 'V171', '45342', '5759', '73315', '67002', '0391', '1714', '1603', '3694', '19882', '2750', 'E9401', '29654', '33385', '502', '7628', '13101', '4416', '3942', '59080', '43400', '6254', '78906', '9684', 'V4502', '2512', '7791', '81600', '2724', 'E9800', '1420', '3483', '7864', '66942', 'V1061', 'E9688', 'E9412', '1370', '30570', '81241', '8690', 'E8859', '5401', '24980', '01304', '5764', '80025', '9942', '2698', 'V6149', 'V1202', 'V1000', '20722', '80626', 'V182', '1121', '99569', '0049', '566', '3060', '4280', 'V6103', 'E8241', '3888', '3152', '80621', '29040', '37900', '36847', '62210', '8246', 'V860', 'E9383', '59001', '80151', '74684', '55221', '6178', '64881', '5699', 'V230', '82003', '28959', '3180', '73007', '4279', '1458', 'E8762', '7824', '42979', '80105', '28241', 'E8493', '4290', '1110', '34403', '6012', '7461', '9587', '6208', '5768', '79989', '2875', 'V5411', 'E9399', 'E8744', 'E9463', '9697', '80230', '5779', 'V510', '99989', '78051', '9041', '7140', '5161', '3314', '37689', '6827', '72691', '7708', '6110', '9713', '3879', '78059', '9754', '4809', '01205', '0880', '94423', '42842', '7109', 'E9358', '72292', '29012', '66811', '73609', '6255', '6829', '85102', '80170', 'V1586', '20502', '7891', '1733', 'E9588', '8220', '5181', '20312', '87360', '7354', '20282', '6200', '8505', '4144', '9890', '5651', 'V1749', '4010', '4512', '2827', 'V608', '94421', '90002', '3481', '9802', '9181', '4160', '79500', '99982', '9893', 'V1641', '34939', '78729', '20018', '72272', '3489', '78060', 'V4981', '75251', '37140', '86339', '76521', '17372', '7102', '51909', '2333', '79311', '7687', '64784', '95206', '7248', 'E8742', '65701', '99644', '2662', '5821', '9523', '42090', '86800', '20480', '9631', 'V1001', '45189', 'E9671', '57421', '2890', '2849', '7786', '43850', '3003', '33390', '37800', '34881', 'V066', 'V667', '44422', '2731', 'V4451', '1468', '326', '92310', '44100', '99639', '74193', '81220', '2513', 'V5811', '30924', '1611', '5601', '4419', '76493', '5989', '9122', '37501', '07071', '3629', '40492', '67153', '8221', '2738', '75520', '2897', '3509', '75219', 'V1043', '53520', 'V1059', '9982', '9619', '2376', '7901', '53649', '75616', 'E9888', 'E8800', '27401', '74349', '4820', '27503', 'V1004', '20971', '17341', '05312', 'V1012', '0414', '2390', 'E9313', '03810', '8780', '59969', '65421', '5109', '36859', '80326', '5528', '76522', 'V172', '95911', '7428', '83305', '78891', '59972', '90182', '70583', '81100', '7273', '9100', '75526', '37749', '2766', '64841', '80150', '87261', '80506', '5302', '4473', '66541', '2948', '56409', '6205', '07811', '1119', '2777', '72665', '25082', '6979', '27501', '29614', '40391', '7718', '7702', '61800', '0085', '55121', '2144', 'V113', '72883', '20203', '3963', '81230', 'E8705', '1307', '60886', '67404', '07033', '9642', 'E9222', '40511', 'V1301', '43814', '46450', '61650', 'E9600', '1562', '261', '7788', '75560', '25062', '49390', '25041', 'E9200', '80030', '38321', '76514', 'V291', '71107', 'V430', '99641', '04112', '53200', '87374', '28733', '99739', 'E8311', 'V3100', '75529', '5920', '3363', '01164', '72402', '67412', '83309', '7798', 'E9109', '29544', '46431', '4389', '1869', '7906', '72991', '00841', '4822', '81409', '34571', '7822', '30590', '90251', '53084', '36207', '73399', '01803', '74100', '41181', '8795', '0479', '73381', '3530', '74103', '71947', '2521', '85211', '2127', '56081', '5371', '71917', '40211', '47412', '8922', '78842', '56738', '9994', '2767', '5187', '7242', '80071', '7522', '72884', 'E9424', 'V8544', '78701', '7092', '88102', '2410', '68110', '99677', 'E8528', '9043', '4376', '72190', '5164', '94204', '99567', '87342', '73302', '92232', '7333', '64673', '4559', '6868', '36001', '25040', '43490', '41405', '1179', '5960', '74101', '4749', '40390', '5753', 'E8233', '4178', '25053', '8058', '71865', '60820', '99649', '9110', '6119', '80332', '25012', '3009', '64133', '9670', '38022', '27709', '7778', '86813', '77213', '20283', '85205', '7689', '9693', '2114', '81010', 'E9343', '5570', '3542', '85125', '87201', '9809', '2650', 'E8120', '25002', '1535', '72679', '71941', '7536', '92800', '3201', '42292', '78194', '60011', '71666', '85402', '9070', 'V652', '6009', '9767', '2840', '1891', '2441', 'V0382', 'E8227', '7830', '30460', '83905', 'E916', '46451', '4170', 'E8582', '53400', '81418', '85245', '8028', '1106', '38915', '67333', 'E8131', '42611', '99673', '4289', '73023', '2881', '36510', '37214', '31234', '0529', '36201', 'E9342', 'V1009', '1952', '66531', '0498', '3575', '3010', '0860', '5763', '37943', '80142', '53081', 'E9306', '1734', '80130', '6198', '69012', '6260', '71905', '40301', '78064', '92421', '04189', '6924', '28862', '37730', '1760', '28262', '7916', '2148', '37313', '5226', '53220', '66822', 'V5881', '34590', '1990', '2440', '51189', '78079', '82520', '8439', '2396', 'E9422', '49320', '94534', 'E8189', 'V0980', '1912', 'V4512', '07031', '43810', '64274', '99676', 'E8641', '75981', 'V1642', '3438', '56781', 'E848', '57450', '56944', '43320', '82312', '35921', '80031', '67484', 'E8142', '99669', '76382', '58189', '86232', '0859', '77016', '03289', '80601', '20192', '81243', '78723', '20206', '73741', '74740', '80843', 'E9289', 'E9554', '2113', '80009', '27411', '7919', '80616', '28411', '2392', '20925', '56962', '29573', '59000', '25043', '9556', '29604', '37855', '99659', 'E0190', '2872', '1917', 'V5812', 'V1007', '80306', '92401', 'V8801', '4011', 'V1869', '71592', '36250', '71103', '77981', 'V3401', '24990', '96903', '20300', '7335', '8786', '96502', '6944', '6982', '78603', '38860', '80075', '3315', '8438', '7842', '74686', '5169', '6151', '34620', 'E9238', '4549', '75613', '2861', '9992', '5373', 'V1049', '80113', '7571', '78071', '7455', '23771', 'E9179', '2762', 'V602', 'E9347', '74922', '03811', '20003', '66944', '4423', 'E9314', 'V4509', '7575', '2101', '5361', '3897', '75989', '44024', '7755', '2652', '63522', '4476', '1808', '99683', '2380', '40400', '5647', '73015', '75482', '1449', '5854', '81509', '5080', 'E9455', '2751', '6172', '1602', '64264', '7581', '75839', 'E9193', 'E8786', 'V08', '56201', 'E9325', '64214', '56731', '30540', '32723', 'E963', '83979', '45352', '20024', 'V290', '1969', '5279', '86239', 'V812', '52462', '35579', '80324', '936', '36100', '0931', '71537', '64671', '71198', '38021', 'E9319', '28981', '1945', '3071', '9605', 'V0981', '1730', '73605', '4548', '20020', '30490', 'V708', '47833', 'E8314', 'E9392', '42653', '5551', '5111', '99732', '1453', '1489', '5225', '7028', 'E9324', '42989', '0940', '74782', '9171', '11289', '48284', '27788', '90450', '99645', '80164', '66941', '84512', '36012', '1761', '42832', '9055', '42983', '30551', '1977', 'V1589', '5303', '74902', '2553', '38830', '2191', '75460', '38611', '7686', 'E8850', '30009', '3384', 'V1252', '9083', '5831', 'E8781', '1743', 'E857', '70706', '99593', '79589', '4220', '41061', '7783', 'V5842', '74732', '85146', '9552', 'V1062', '0415', '7243', '80859', 'E0031', '92410', '01186', '85105', '73340', '7244', 'V202', '28522', '47832', '36206', '74429', '3612', 'V4966', '80412', '37274', '463', '3434', '2121', '32727', '85241', '486', 'E8589', '71996', '51852', '6806', '28952', 'E9682', '64903', 'E8860', '82009', 'V558', '4618', '20969', 'V053', 'E9335', '75169', '3898', 'V1060', '34600', '4460', '1231', '40493', '04104', '81109', '7358', '60889', '4821', '71946', 'E8714', '43889', 'E9262', '4539', '34489', '9330', 'E8219', '7313', 'E9010', '7424', '41092', '34982', '86513', '6931', 'V123', '78003', '52512', '9592', '0074', '7821', '80109', '33722', '71430', '9222', '95891', '2150', 'E9309', '4770', '86611', '86412', '37775', '66634', '226', '2818', 'E9108', '33912', 'E8845', '5935', '80718', '6209', '4542', '74861', '57512', 'E9538', '81612', '1984', '56782', '9779', '4592', '41512', '85216', '80503', '65961', '59389', '7672', '95219', '8240', '2352', '74749', 'V1509', '30473', '35981', '59800', '28529', 'E8152', '65953', '8054', '2165', '99682', 'E8782', '04111', '29212', 'V4361', '8691', '81407', '587', '86613', '7716', '20011', '29520', '74682', '87412', '8509', '1953', '65423', '73012', '6989', '78821', '99811', 'V167', '88023', '82030', '80196', '41012', '9557', '5562', '00843', '25000', '9712', 'E8788', '30502', '80015', '27669', '75610', '4441', '5128', '3442', '92303', '30302', '53241', '05881', 'V029', '29980', '8361', '82332', '5531', '7712', 'E8846', 'E9081', 'E9011', '0521', '8920', '71990', '29653', '7707', '80320', '9029', '2116', '6018', '24290', 'E9317', '63311', '82539', '80085', '1749', '85225', '57140', '7921', '65651', '8400', 'V5862', '3552', '2125', '67012', '9160', '05313', '2329', '37182', '66411', '2230', 'V489', '62402', '5671', '8411', '67323', '7320', '53100', '7503', '71597', '27730', '1619', '1122', '77089', '38861', '71239', '27941', '74781', '83961', 'V5417', '7731', 'V1501', '3212', '2331', '30392', '44629', '5071', '4275', 'V583', '5719', '77083', '3524', '57511', 'E8854', 'V1649', '70715', '3181', '28983', '5192', '8074', '77589', '51902', '7827', '28989', '73088', '90220', 'V462', '56987', 'V4986', '78609', '2832', '6945', '67434', '80375', '42654', 'E8216', 'E9360', '27702', 'V0252', '3952', '48232', '2755', '1649', 'E9991', 'E9318', '73009', '41513', '7851', 'V160', '58881', '20936', '86601', '56400', '7583', '4142', '2859', '78469', '76496', '4295', '72403', '99933', '20270', '8861', '78199', '7383', '45981', '56989', 'E8147', '88110', 'V143', '3330', '9563', '6396', '73670', '38832', '20890', '43885', '05471', '7443', '8471', '31531', '7902', '86413', '32341', '9594', '30553', '9278', '71986', '06641', '3689', '67402', '80161', '30751', '30150', '62130', '2722', '20191', '71913', '8020', '9248', '7235', '9561', '9245', 'V5422', '74869', 'V4578', '8700', '73017', '7818', 'V4588', '69282', '1550', '7249', '4299', '88100', 'V5309', '2982', '78639', '8440', 'V1250', '5206', '2870', '7785', '41001', 'E9804', '1729', '30583', 'V1364', '1129', '8711', '75453', 'E0080', '38872', '88111', 'V331', '64233', '75315', '80475', '9162', '1548', '44409', '65963', '9711', 'E8669', '5762', '66554', 'E8888', '59581', '3080', 'V270', '2528', '36320', '85240', '96501', '3334', '2811', '77214', '9196', '84209', '30571', '78009', '5750', '03285', '86321', '83114', '4536', '1642', '92320', '1373', '6175', '37739', '79579', '30420', 'V8531', '5538', '7062', '36107', '37273', 'E0299', '4541', '56960', '7717', '86409', '20285', '7750', '45382', 'V5841', '4387', 'V8536', '9191', '78262', '29524', '41412', '96509', '325', '71904', '73689', '47829', '81406', '49322', '7962', '7176', '46400', '42971', '7093', '83200', 'V5865', '07032', '0840', '7083', '6268', 'V170', '78862', '6253', '2700', '34693', '6952', '99832', '2704', '03819', '71235', '36315', '73630', '83501', '5168', '3094', '71590', '92311', '00862', '36274', '52101', '23691', '20972', '29632', '8794', '53170', '42291', '30410', 'V1507', '37312', '9558', '82129', 'E8497', '9068', '55129', '0092', 'V6285', '3379', 'V4987', '76492', '65583', '5756', '36101', '1515', '29411', '78052', '3962', '9998', '6140', 'E9380', '2137', '83503', '30089', '80322', '47822', '5734', '9912', '38908', '78054', '2539', '4480', '64421', 'E0026', '6820', '36000', '2839', '42652', '71180', '7089', '37881', '95215', '56721', '74729', 'E9294', '71950', '1560', '5799', 'E8830', '85175', '6214', '0360', '40501', '04110', '82133', '4417', 'V4989', '90289', '75431', 'V422', '72709', '4531', '78951', '7224', '7753', '79319', '7523', '36910', '25080', '9661', '75262', 'V0971', 'E8542', '3814', '29532', '29689', '80175', '9120', '79402', '70713', '29624', '7291', '632', 'E9359', '7981', '42732', '7271', '9093', '048', 'E8178', '1513', 'E9386', '725', 'V162', '8940', '64254', '07953', '52333', '33729', '03849', '3693', '69279', '87371', '0071', '3968', '6261', '5997', '35801', '73312', 'V7651', '30422', '2866', '7595', 'E9659', '2411', 'V624', '9473', 'V1203', '24950', '72705', '9142', '80311', '7709', '01894', '83104', '38921', 'E9060', '57451', '6202', '1641', '20047', '92231', '72230', '9161', 'E9298', 'E8784', '81514', '29421', '47811', '8027', '5285', '0093', 'E9064', '64813', '25050', '78552', '9060', '74920', '1529', '7792', '1450', '4730', '4371', '6988', '87411', '37420', '83101', 'V5426', '67324', '90223', '25639', '4918', '43811', '2442', '37842', '51289', '2710', 'E9378', '3949', '2761', '79439', 'E8502', '48242', '3019', '1910', '80362', '0048', '5119', 'V6405', 'V468', '37203', '37143', 'V0482', '9729', '44481', '37630', '38903', '97081', '66111', '3202', '0088', '4320', '83650', '52579', '34540', '25092', '99830', '86393', '71835', '37990', '66511', '36019', '7585', '64863', '71911', '71594', '83901', '0849', '7808', '43813', '52100', '6181', '37923', 'E8212', '90254', 'V789', '2776', 'E9391', '6861', '85226', '74602', '7760', '9165', '5770', '82032', '83969', 'V1003', '7596', '79953', '36520', '36800', '45184', '94214', '75539', '8209', '01880', '80423', '9562', '81519', '3545', '27802', '2552', '90141', '44389', 'V016', '82529', '55001', 'V427', '0463', 'E0061', '37311', 'V453', '1809', '5235', '34281', 'V9081', '6398', '81333', '3831', '74441', '81331', 'V9103', '45621', '6266', '2842', '94108', '5131', '1390', '7325', '9224', '45829', '7854', 'E8619', '80660', '5778', '38015', '7597', 'V1081', '9390', '6082', 'V6111', '4479', '0902', '9238', '6071', '51853', '4958', '5852', 'E0009', '9952', '79415', '51633', '96909', '7933', '29522', '20302', '75510', '57491', '76416', 'V537', '64294', 'V601', '78449', '28249', '71930', '5264', '43300', '23872', '8744', '4957', '2581', 'V2652', '9698', '1830', '7218', '7960', '9695', '20973', '3158', '43840', 'E9241', '7318', '78341', '77182', '8742', '6146', '2270', 'E9444', '7621', '28310', '85189', '53171', '37710', '90301', '6180', '7015', '25073', '2874', '64241', '3439', '73008', '8370', '71907', '37941', '05412', '70701', '53240', '7010', '25071', '23875', '85209', '34680', '71296', '44281', '6028', 'E9570', '82320', '5752', '57410', '80132', '7908', 'V5302', '042', '82120', '71966', 'E9209', '3659', '80162', '64833', '4540', 'E9315', '4489', '4554', '64103', '69515', '61610', 'V1309', '28319', '80224', 'V5339', '80422', 'E8637', '4219', '74401', '59971', 'E8552', '81602', '75617', '20001', '83402', '7462', '37240', '78491', '74763', '72291', '71191', 'E9889', 'V4575', '6170', '82111', '4561', '67311', '5820', '75564', '80609', '9986', '64931', '85246', '7386', '33920', '83202', '5847', '6953', '2511', '81611', '1962', '25090', '40490', 'V4985', 'V9010', '460', '9725', '28264', '5968', '9899', '4269', 'V169', '8930', '01215', '85400', '5721', '01805', 'V6129', '71236', '23871', '20008', '7852', '24991', '75563', '78072', '1228', '1120', '71432', '73342', '2182', '25060', '69512', '44322', '9012', '99971', '64292', '8961', '8247', '27739', '90181', 'V168', '78934', '81203', '57142', '7816', '2753', '1715', '8472', '2869', '81405', '92321', '37942', 'V850', '8064', '8631', '1717', '80312', '2541', '6965', '99589', '8080', '11505', '1919', '4613', '7993', '0309', '00845', '4612', '57480', '60090', '7222', '5569', 'E8249', '52109', '3885', '3970', '9961', '80180', '1981', '7572', '75683', '30443', '7797', '36844', '6169', '2143', '30580', '6954', '78097', '1124', '36281', '74423', '5859', '86402', '90442', '1911', '65404', '72781', '25042', '7905', 'V596', '7746', '33720', '71916', '43822', '29420', '47411', '66632', '39890', '73629', '4748', '1273', '41022', '2889', '9228', 'V4282', '30472', '80222', '92411', '8840', 'V1201', '8676', '29010', '86501', 'E8702', '8921', '5811', '36362', '80485', '9051', 'E976', '20071', '53530', 'E8012', '7932', '71108', '86222', '86100', '2251', '937', '28982', '64403', 'V1041', '4465', 'V7281', '3959', '677', '64101', '4258', '85242', '36204', '99859', 'V1029', 'E8415', '4353', '42290', '59659', '7484', '8404', '27787', '83906', 'E9452', 'E9283', '38300', 'V3200', '24291', 'E0076', '8068', '80706', '52800', 'V8401', '56722', '71696', '07049', '74769', '4148', '27801', 'V8539', '5529', '5646', '75550', '1608', '2800', 'E8780', '30752', 'E9356', 'V5332', 'E9102', '32720', 'E0030', '48289', 'E9387', '87349', '20917', '34404', '1578', '59689', 'V5482', '7934', '4241', '8083', '5640', '92701', '1703', '37034', '37431', 'V5415', '9582', '7566', '86500', 'E9322', '7481', '4233', 'E8250', '81351', '43852', '4941', '44421', '49122', '30723', '55202', '28732', 'V641', '61882', 'E8529', '71846', '04119', '20287', '64204', '2532', '11599', '2387', '36840', '4881', '94800', '44102', '3519', '99609', '3239', '2851', '3431', '4659', '3559', '99662', '53340', '42833', '5566', '65221', '1160', '99702', '74343', '48282', '85305', '7236', '29042', '34710', '1452', '9999', '71845', '30471', '92612', '85403', '9895', '0539', '30522', '4264', 'V1090', '2163', '85219', '68102', '3013', '33523', '3061', '29620', '9514', '9212', '75566', '34889', '23877', '2598', '01895', '4378', '87364', '53013', '5761', '64782', '5952', '75162', 'V3201', '8677', '74741', '65613', '4239', '7793', '3229', '4610', '4243', '9500', 'E9329', '3969', 'E9308', '20975', '25013', 'E8785', '7452', '64511', '04184', '30742', 'E9650', '36563', '28412', '30552', 'E8058', '7704', '501', '7812', '30759', '541', '01890', '03842', '99799', 'E9208', '43310', '31230', '70712', '86384', '8243', '490', '53441', '30789', '30461', '33721', '28803', '25010', '71842', '1300', '38039', '3571', '7099', 'E8052', 'E8191', '52489', '78343', '9744', '6111', '5846', 'V644', '96561', '79009', '25072', '7197', '29631', '4778', '1431', '82535', '99984', '47830', '81512', '99741', '20212', 'E8663', '2823', '81510', 'V671', '33701', 'E8252', '86419', '37601', '52340', '4210', '8900', '6850', '30520', '0412', '2462', '1880', '72740', '86112', '8770', '87363', '46411', '82020', '7480', 'E8543', '47825', '9663', 'E9408', '49302', '3108', '38201', '99665', '45381', '81502', '94234', '71945', '7359', '5193', 'E9419', '53401', '81381', '61179', 'E8122', '1173', '4560', '4271', '8600', '40291', '1589', 'E9583', '075', '37871', '87330', '85131', 'V643', 'V452', '5670', '83908', '7467', '4570', '7600', '80172', '0499', '29383', '3069', '37850', '1510', '34581', '1613', '80853', '80708', '30000', '53021', 'E8278', '73301', '42091', '8418', '94840', '75559', '1309', '9132', '8504', '5565', 'E9413', '32361', '76072', '4739', '1726', 'V068', '6040', '3574', '9735', '61172', '3380', '3236', '78443', 'E8150', '40290', '8713', 'V298', '0340', '7795', '2382', '81305', '79092', '1174', '45351', '73020', '4292', 'E8384', '76497', '77084', '38918', '1588', '88121', '30591', '5758', '36217', '5692', '86804', '1720', '6201', '79509', '4329', '1618', '11283', '74689', '2972', '81353', '80145', '80228', 'E9395', '8745', '36960', '2510', '8448', 'V103', '80466', '71120', 'V037', '94203', '9881', 'E8794', '3241', '4422', '1571', '25091', '81344', '99813', '68100', '67482', '94850', '85223', '27911', '9515', '2812', '2692', '72669', 'E8261', 'E8655', '81332', 'V4569', '24900', '7936', 'E9601', '81400', '86509', '2538', '86512', '32726', '60782', '4430', '86802', '32719', 'V1505', '72981', '1440', '64261', '3331', 'V555', 'V1504', '42789', '71849', '56881', '74722', '34210', '75433', '8728', '01190', '75019', '9604', '7765', '56985', '29289', '73671', '73313', '4538', 'E9190', '2536', 'V4574', '20490', '60000', '33900', '1522', '80301', '37950', '51636', '85185', 'E9499', '75269', '7541', '20310', 'E8228', '99889', 'E8381', '95207', '9164', '36816', '71965', '82523', '8672', '81242', 'V550', '41032', 'E9509', '81002', '20042', '9588', 'V063', '30573', 'E9132', '4350', '3543', '85212', '30001', '82123', '40201', '64933', '46619', '83500', '33520', '8871', '8245', '6141', '27509', '4838', '6270', '80703', '25003', '4449', '76508', '82131', '78863', '7020', 'V426', '64793', '3079', '94532', '01505', '77989', 'V222', '52550', '3099', '5269', '86381', '2853', '5282', '72283', '8851', '99651', 'V8389', '8089', '68101', '5582', '4231', '6010', '8791', 'V5867', '64861', '85182', '51181', '1820', '73712', '2967', '7620', '8738', '83302', '56210', '4162', 'E9290', '52540', '27901', '86503', '8406', '71106', 'V4283', '81342', '33511', '5298', '44772', '32372', '53011', '9599', '86103', '179', '3159', '47871', '62989', '05371', '28860', '70721', '99654', 'V023', '2588', '61801', 'V4501', '30561', '9210', 'V135', '33828', '73022', '65571', '36211', '71580', '47879', '77581', '37854', '8761', '1330', '20190', '3369', '76510', '22809', '6273', '70909', '3490', '36573', '57411', '99932', '65451', '9747', '66612', '2117', '36813', '92300', '3234', 'E8792', '80625', '75567', '64664', '53160', 'V4031', '96979', '33522', '42650', '1639', '29572', '9509', '1320', '78340', '53541', '80605', '71616', '2535', '37851', '99833', '7475', '66534', '7238', 'E9291', '73739', '1916', '80470', '7580', '3482', '92821', '5199', '74320', '27800', '45181', '9710', '8081', '0530', '32737', '1724', 'V4963', '85109', '1640', '64234', '83400', '75511', '86803', '78057', '3332', '81419', '2639', '78702', '81412', '2253', '79400', '76402', '59381', '78441', '1700', '30122', '5642', '80600', '7763', '5296', '9569', '7757', '4720', '1978', '1539', 'V4971', '65444', '78703', 'E9457', '80473', '8408', 'V1069', '99685', '7720', '5959', '80082', '4377', '76381', '73004', '74609', '29680', '85203', '1906', '1763', '8750', 'E9466', '82534', '34830', '80366', '1982', '769', '2459', '9471', '2828', 'E9323', '316', '29284', 'E9051', '7804', '66914', '0793', '99590', '80604', '07030', '6392', '99680', '99527', '7631', '3592', '2306', '99681', '44321', '5563', '85106', 'V1508', '36811', '5738', '88000', '51284', '83300', '81001', '2913', '2863', '4581', '72971', '45376', 'V714', '86355', '76409', '99663', '3593', '6212', '76495', 'E9053', 'E8182', 'E9809', 'E9173', '7454', '7930', '28731', '33399', '6279', '99643', '23873', '7211', '3383', '78760', '7876', '44101', '2721', 'E9292', '99689', '37024', '65251', '34211', '70714', '3589', '94100', '71481', '3522', '78939', 'V4571', '1570', '3068', '72742', '73311', 'E888', '53989', '42761', '37700', '0540', '2900', 'E8580', 'V8533', '9989', '99561', 'E977', '8052', '7835', 'E9438', '86354', '80235', '4733', '27900', '8407', '9971', '73001', '2813', '9671', '34290', '6274', '85144', '96972', '7741', '75430', '07020', '41189', 'V1811', '76075', 'E9177', '72930', '2930', '2682', 'E8500', '36043', '30483', '81212', '9063', '2910', '1531', '76528', 'V1087', '20929', '36403', '5194', '36105', '7920', '30151', '5996', '6278', '05472', '78053', '4179', '36203', 'E912', '5275', '4785', 'V6542', '56949', '80013', '72210', '7517', '9239', '80023', '2373', '47834', '85231', '08881', '72211', '25093', '82524', '43311', '1537', '85194', '5933', '76523', '7833', '8053', '88013', '3321', '01485', '52181', 'V5481', '118', '2699', '72612', '2371', '78831', 'V4962', '4168', '9973', '5262', '71536', '3549', '79959', '7563', '92400', '70401', '29384', '3739', '55120', '34501', '8604', '5198', '78837', 'E8550', '80135', '7599', '9980', '2880', '85221', 'V1541', '4358', '41419', '99941', '32742', '6089', '53311', '45372', '7230', '33812', '5293', '9009', '6984', '95914', '86353', 'V560', '20290', '56030', '2989', '61189', '56212', '1971', '1764', '42760', '2728', '1490', '34441', '1980', '8502', '1574', '77181', '3824', '48283', '71944', '20411', 'E8600', '30391', '20048', '2155', '6210', '9654', '76404', '08882', '3081', '8749', '69289', '04103', '77985', 'E9063', '2760', '88112', '83103', '7508', 'V5863', '95912', '36233', '85142', '5982', '9033', '71912', '7991', 'V8538', '7969', '938', '9221', '76505', '6960', '7141', 'E9068', '79401', '77081', '5277', '34460', '452', '86514', '74330', '6918', '81408', '9513', '4870', 'V4614', '75029', '7915', '05320', 'E8170', 'E9174', '73341', '66614', '53201', '80036', '71101', '1630', '73681', '33119', '1881', '60781', 'V1502', 'E8801', '34431', '3576', '1479', '76501', '37940', '20022', '53082', '30503', '1511', '40300', '80100', '3568', '9879', '65661', '85184', '7850', '80021', '80114', '42840', '1308', '9351', '78864', '1960', 'V8525', '94323', '9993', '86809', '63491', 'E8551', '1848', '9766', '3569', '41071', '94320', '1874', '64852', '53087', 'V4587', '52310', '7931', '542', '76499', '56789', '27402', '9975', '94536', '7714', '07983', '78791', '36205', '1920', '43819', 'E956', '79399', '9683', '59960', '25021', '2734', 'E9345', 'V1582', '7811', '1991', 'E9288', '78960', '80111', '99580', '74190', '99791', 'V5883', '81209', '7286', '9600', 'E8498', '25208', '4919', '38401', 'V145', 'E8282', '5280', '4373', '75461', '85183', '5949', '5733', '24200', '2181', '78033', 'V1050', 'V5419', '59582', '57481', '80024', '80120', '7483', '11281', '25022', '29580', 'E9201', '7202', '30011', '72633', '42741', '2554', '8731', 'E9274', '73319', '2759', '66331', '99686', '86600', '29650', '4321', '45385', '9948', '78550', 'E9689', '87352', '71940', '9911', '28866', 'E9420', 'V045', '77210', '76079', '7730', 'V1542', '53642', '56984', 'E8799', '71533', '71698', '45340', '20021', '29633', 'E9501', '83651', 'V1585', '34690', '78093', '6184', '90234', '1707', '3335', '74923', '86120', '63320', 'E8343', '5951', '85186', '7790', '73013', 'V189', '74683', '99565', '37200', '9061', '03840', 'E8499', '3209', 'V110', '71695', '2808', '1713', 'E8341', '42843', '8171', '9048', '40599', '81391', '7533', 'V8532', '48239', '80171', '20050', '41042', '1748', '72673', '5369', '56031', '99709', '8479', '7324', '3699', '27482', '71195', '4169', '71828', '70705', '5289', '82330', '20060', 'V5427', '4242', 'V8530', '2564', '20211', '75289', '5265', '4530', '37633', '34551', '82019', '1500', '74359', '9630', '57510', '4409', '34931', '2774', '1638', '5769', '2841', 'V148', '07951', '2718', '82100', '36522', 'V8534', '57400', '29570', '20010', '43812', '75612', '41002', 'E8811', '37810', '7671', 'E0060', '55300', '32381', '65921', '2727', '23989', '80849', 'V451', '30531', '5712', '80115', '5643', '49391', 'V4984', '1572', '6168', '0091', '2871', '7676', '46611', 'E8136', 'E0162', '37701', '44502', '74711', '449', '5934', 'E8169', '37952', '5991', '7904', '44620', '36284', '7570', 'V655', '53140', '9595', '34401', '76491', '2894', '34550', '62211', 'V4961', '2452', '76407', '2669', '4251', '80226', 'V51', '94127', '5362', '99801', '1765', 'V090', '5259', 'V5391', 'E0069', '5602', '56983', '30521', '28984', 'E8193', '1504', '3572', '72271', '81251', '3129', '1609', '1429', '6084', '52108', '78066', '71598', 'E9479', '75513', 'E8232', '20400', '9064', '2273', '27549', '1520', '36841', '1488', '69010', '7722', '37652', 'E9320', '23773', '9472', '2794', '7784', '31400', '80238', '78002', '1272', '55220', '5409', '96500', '3210', '43821', 'V0253', '6079', '05319', '5966', '36252', '92801', '74600', '5728', 'E851', 'V1044', '0419', 'E8130', '5713', '20800', 'V721', '45910', '3669', '99675', '77012', '87354', '1921', '8702', '52342', '74789', 'E8852', '64661', 'V6141', '28419', 'V1529', '5400', 'V5849', '05314', '41040', '80070', '80000', '94224', '64824', '7068', '28730', '1469', '28850', '2271', '5941', '75323', '71843', '78905', '66001', '81318', '99772', 'V556', 'V0481', '9760', '53784', '9984', '62981', '70711', '70901', '28749', '1884', '0239', '1736', '3551', '4380', '86411', '5513', '8679', '3029', '72270', '3541', '9995', 'V872', '7947', 'V486', 'E9385', '71226', '1728', '5283', '53783', '43380', '51919', '73027', '86130', '80631', '30747', '1481', '80116', '1209', '33381', '80709', '0549', '30029', '7282', '86110', '99657', '75619', '04149', '32724', '51881', 'V640', '3453', '30401', 'V4983', 'V8541', '44103', 'V146', 'E8798', '70725', '9678', '80126', '4848', '29540', '2357', '36372', '53291', '7537', 'E8761', 'E8769', '82532', '2448', '82381', '80416', '99640', '2301', '73345', '57470', '9585', '0785', '44023', '38922', '71897', '86131', '80436', 'V5423', '99700', '7992', '6256', '29644', '78907', 'V454', '43411', '9679', '30431', '79099', '3759', '53490', '95214', '27542', '85314', '5304', '4471', '64943', '3899', 'E8146', '99931', '3682', '32351', '78930', 'V698', '30411', '9180', '96901', 'V1242', '78959', 'V074', '7935', '3561', '7610', '33189', 'V163', '76527', '6289', '6825', '76525', '24221', '72141', '7881', 'V0489', '36231', '36400', '1739', '5679', '7711', '90229', '85404', '07889', '37289', '7469', '78065', '1509', '4780', '34212', 'V5844', '4784', '8701', '80603', '5121', '80624', '7504', '3445', '1620', 'V017', '79380', '73029', '43881', '64131', '00847', '7586', '84503', '6981', 'V1086', '78909', '2534', '94209', '80508', 'V4973', '30021', 'E966', '4553', '7131', '90226', 'E8668', '30014', '9685', '36970', '1898', '83401', '41511', 'V8821', '3562', 'V448', '86511', '80711', '217', '9011', '5940', 'E9138', '95893', '80234', '6235', '73679', '8901', '45379', '6116', '67403', '7564', '2778', '7473', 'V4450', '85145', '7869', '88120', '40411', 'V4585', '8500', '57420', 'E0089', '80622', '64892', '43882', '5793', '7505', '78321', '37489', '66131', 'E0020', 'V1651', 'E911', '59370', 'E9242', 'E8716', '86350', '87321', 'V600', '8912', '79022', 'V8381', '3418', '04671', '920', 'V8522', 'V091', 'V1279', '9082', 'E9270', '29625', '5849', '7246', 'V6109', 'V716', '41519', '64663', 'E8698', '20240', '1985', '4372', '79093', '72142', '72879', '1538', '2733', 'E9301', '83813', '9102', '34831', '2867', '52511', '75160', '29990', '37909', '42490', '05443', '2572', '5780', '23772', '5253', '1329', '85232', '7279', '76511', '27403', '42820', 'E9205', '53410', '71902', '4467', '6262', '86383', '2337', '82531', '20000', '30430', '3342', '64814', 'V1553', '92811', '2394', '9341', '7828', '7070', '30390', '64243', '80321', '64821', '51631', '8441', '1759', '80410', '53501', '73733', '2166', '29211', 'V400', '2298', '70900', 'E9390', '2779', '56889', '2379', '2589', '9726', '76403', '9583', 'E8041', '1471', '8712', '9220', '0388', '9941', '01354', '37000', '80141', '41010', '99779', '3789', '37443', '0844', '80060', 'V0261', '22801', '36901', '1629', '1840', 'E0161', 'E8199', '1536', '28950', '8792', '41411', '72989', '72999', '81501', '64251', '27540', '75521', '71691', 'E9293', '9957', '04100', '92720', 'V0991', '81503', '85150', '67202', '82101', '75319', '1521', '86801', '85302', '1568', '80124', '07054', '29651', '30928', '1768', '1970', '5790', '32382', '30402', '9046', '90242', '7284', '25542', '2210', '5439', 'E0008', '7101', '74685', '1419', 'V301', '33902', '05440', '1882', '40413', '9053', '5184', '66582', '73620', '0413', 'V1551', '56729', 'E9330', '5363', '2384', '83311', '86320', '2824', '5988', 'E9198', 'V5489', '8241', 'E9364', '0051', 'E9478', '3222', '81244', '70709', '9701', '64653', '1914', '34839', '79094', 'E0064', 'E0138', 'V1253', '7726', '79671', '85100', '63552', '44020', '34500', '71847', '20213', '4111', '41401', '80705', '7622', '1832', '32089', 'E9453', '86405', '24230', 'E9426', '78050', '75249', '3518', '2112', '24201', '683', 'V181', '6929', '5561', '8261', '1118', '28489', '53085', '38869', '37924', '64904', '53020', 'V1022', '73300', '2515', '85405', '8281', '87264', '76518', '6948', '69018', '41041', '79001', '65801', '42981', '81411', '0279', '75651', '2971', '1464', 'E8717', '81382', '4431', '75733', '34402', '41072', '42293', '3971', '27953', '75671', '24981', '2690', '01405', '85310', '82132', '52410', '7540', '73600', '34430', '49301', '76513', '9149', '515', '3829', '74900', '2920', '90453', '7866', '71213', '64201', '7366', '88020', '9173', '99602', '36232', '81101', '70400', '45119', 'E8196', '44283', '78904', '37275', '7623', 'V551', '5710', '24220', '25801', '7539', 'E8790', '05821', '3017', '86102', 'V1272', '78651', '36202', '83303', '72751', '9518', '8025', '79923', '1460', '2749', '64303', '71848', 'V140', '95200', '6192', '9680', '32725', '3370', '86400', '99693', '64224', '67401', '29282', '28240', 'V5331', '29560', '0039', '87344', '38610', '0030', '4710', '4466', '6823', '43391', '2540', '7513', '53101', 'V6284', '2819', 'E8258', '60001', '7589', '71694', '1648', '4552', 'E8583', '28521', '79029', '5260', '38870', '78947', 'E9460', 'V5399', '82331', '80620', '33394', '31239', '71693', '49381', '9092', '1915', '82521', '90221', '9273', '28801', '37751', 'E9507', 'E9581', '2409', '6149', '71943', '3076', '7831', 'E9801', '70700', '70710', '1976', '71821', '4414', '0319', '07989', '6218', '07059', '3310', '30530', '0383', '2939', '8760', '1712', '71596', 'V1088', '05410', '94126', '3182', '2189', '99592', 'V1506', '7872', '4510', '83903', '7845', '78865', 'V707', '7863', '2858', '20037', '99603', 'E964', '42651', '4478', '2706', 'E9389', '7961', '6269', '6259', '03641', '87320', '36241', '7949', '94522', '79539', '80476', '49120', 'V6289', '55100', '41090', 'E8238', 'E9295', '7423', 'E8842', '28951', '25032', '7079', '70724', '3961', '74761', '83411', '8678', '87353', '75555', '37515', '30501', '64803', '80702', '2765', 'V0251', '7265', '7030', '36523', '75481', '34292', '37230', 'E9398', '5641', '78604', '99678', '8603', '3090', '86401', '1140', '9618', '53251', 'E8694', '3594', '80710', '87361', '83942', '7761', '57441', '87351', 'E9192', '29410', '8670', 'V175', '79389', '2899', '7105', '9598', 'V6104', '7592', '6080', '75531', '9094', '71595', '59382', '30016', '0470', '30560', '7627', '36470', '57431', '99591', 'V561', '34701', '6248', '80341', '41011', '3360', '4263', '6179', '78039', '9035', '36812', '80505', '0338', 'V4382', '41091', 'V642', 'V1271', '2302', '9614', 'E9451', '78704', '76383', '48249', '7217', '07044', '32721', 'V052', '2558', '8449', '1974', '81404', '1987', '2118', 'V3000', '53130', '4472', '71237', '8821', '1888', '20402', '7515', 'V632', '43401', '71535', '75689', '52469', '86504', '85304', 'V610', '5609', '34201', '86101', 'E887', '4238', '80076', '0382', '75569', '7011', '8783', '74681', '72813', '01325', '82121', '85220', '2548', '80102', 'V040', '80606', '42491', '51889', '27541', '42822', '36275', '38582', '44381', '70704', '2308', '5368', '75317', '5172', '035', '88122', '80701', 'V1021', '7678', '2729', '7964', '75432', 'V8521', '73729', 'V6442', '42841', '36040', '00863', '0490', '2911', '7706', '7873', '81314', 'V442', '5739', '64934', '9658', '64682', '56489', '0090', '74361', '80061', 'V138', '7801', '71958', '68601', '66622', '60785', '76517', '2354', '94401', '30543', '6185', '53450', '78652', '35781', '72762', '87365', '43491', '71615', 'E8187', '38910', '76502', '3004', '34700', '538', '94425', '7593', 'V6107', '04102', '7907', 'V239', '65971', '9627', '80101', '8409', '49300', '3499', 'E9447', '87340', '4293', '71901', '57430', '5518', '8073', '73672', 'E918', 'E969', '6101', '9393', '80020', '78899', '37852', '64293', '43331', '7636', '8300', '88029', 'V0179', '78829', '80121', '4262', '7892', '73719', '80160', '75500', 'E9441', '7779', '5855', '57490', '1950', '25052', '3320', '90241', '34282', '20005', '9916', '59655', '2980', '6190', '53500', '1369', '1922', '5244', '5772', '9733', '80152', 'V8545', '1519', '22804', '2592', '7602', '78820', '6160', '99674', 'E8160', '4551', '56723', '9348', 'E8231', '9752', '71230', '75983', '77431', '69589', 'V4284', '6182', '5716', '0218', '3839', '51883', '30593', '2825', '37991', '8621', 'E9305', '9352', '2167', '25070', '8716', '5122', 'V6282', '9531', '65261', '20280', '59984', '33379', '1963', '7534', '5921', '56032', '55000', 'E9170', '85196', '52330', '71515', '80110', '27651', '6173', '71534', '99688', 'V1581', '78931', 'V6110', '29389', '80400', '9964', '28802', '3950', '5307', '81252', '6073', '81393', '29189', '36002', '90089', '53440', '47819', '78551', '0839', 'V311', '9722', 'E9361', '13102', 'V5410', '80607', '64891', '38200', '7703', '29621', '80707', '78724', '53210', '27400', '90253', 'E9310', '81603', '90003', '72939', 'E8581', '33811', '28809', 'V434', '9080', '72782', '99670', '52400', '71216', '86351', '72789', '0031', '73024', '4841', '37210', '2550', 'V292', '00869', 'V638', '95909', '20158', '78721', 'E8789', '2252', '4464', '01330', '2149', '4270', '99585', '4253', '96971', 'V1255', '83661', '37921', '66602', '78442', '9524', '9528', '34989', '42682', '5964', 'E9203', '25512', '86329', '78833', '3101', '63512', 'E8708', '9720', '75679', '096', '80131', '78834', '3573', '7820', '80080', '3588', '42821', 'E9580', '80629', '8710', '4149', '8021', '9578', 'V1351', '75479', '29574', '5963', '6824', '87359', 'E8132', '30002', '32082', 'E8555', '3093', 'V703', '37702', 'V142', '9019', '5290', '80239', '7051', '94232', '30500', '20080', '37886', 'V5416', '2638', '2180', '591', '80623', '6962', 'V4611', '243', '53540', '500', '81102', '04082', '80639', '45183', '52689', 'V1302', '0578', '7213', 'E9821', 'V4972', '4846', 'V1011', '5881', '1528', '78841', '0794', '81322', '81402', '5781', '78902', '3109', '74561', '38589', '8029', '5810', '20412', '30441', '4240', '6826', '2893', '29630', '4571', '8797', '76719', '78096', 'V166', '86344', '3501', '5539', '70722', 'V2651', '78722', '7237', '2370', '5990', '7385', '73730', '53390', '44022', '30480', '73382', '53640', '71102', 'V8535', '9340', '2126', '1643', '30120', '76406', 'V1091', '20510', '9596', '82139', '82002', 'V4363', '8072', '3349', 'E8508', 'E030', '81352', 'V443', '1973', '7587', 'E9194', '0362', 'E8843', '4588', '1579', '92721', '20040', '4267', '20500', '76498', 'E9338', '72741', '60789', '9249', 'V1052', '586', '70219', '33392', '3432', '86132', '3082', '60783', '05829', '4550', '7764', '80012', '30181', '36189', '8469', 'E0011', '83801', '9309', '45989', '7289', '7226', '20380', '9056', '0970', '70723', '1625', '44324', '29285', '1211', '29575', 'E9300', 'V1002', '71109', '7929', '6851', '75522', '8798', '2891', '5306', '53191', '29699', '4110', '60010', '7732', '99851', 'E0070', 'V271', '7104', '41081', '99529', '43381', '2809', '7841', '36512', '27803', '5754', 'V1652', 'V1552', 'V6142', '27952', '99731', '20033', '53111', 'E0039', '99749', '6019', '4254', '81354', '8280', '33821', '80421', 'E8839', '3368', '90222', '9099', '81513', '44323', '86389', '4533', '27731', '86404', '30550', '87402', '3558', '9114', 'V8811', '53341', 'E8002', '44029', '2325', '73026', '3371', 'V1643', 'E8156', 'V604', '30300', '53019', '73019', '2218', 'E8764', '80500', '94420', '41021', '7380', '7885', '83652', '7018', 'V183', '2561', '80221', '1737', 'E9589', '61804', '53141', '1498', '3581', 'V1588', '7520', '64244', '86810', '86352', '4802', '0971', '83805', '65641', 'V1365', '35971', '3249', '72401', '3014', 'V8523', '3336', '5995', '7990', 'V1085', '25200', 'V1046', 'E9654', '5239', '53511', 'V4975', 'E8501', '4719', '7775', '81504', '9229', '2931', '45341', '71970', '38842', '09152', 'V0262', '71233', '60883', '29043', '8671', '72252', '1716', '36544', '80176', 'V440', '36815', '57460', '45384', '79901', '5932', '5715', '2110', '9034', '76077', '9392', 'V502', 'E8710', '0543', '43820', 'V109', '3918', '23874', '1478', '7946', '4731', '80376', '85141', 'E9328', '7231', '28861', '74783', 'V0254', 'E8126', '5853', '72992', '5822', '7108', '63401', 'V1741', '72700', '7019', '135', '81321', '88101', '0312', 'E8704', '70720', '2104', '01300', 'V8542', '9610', '36570', '9116', '56882', '3671', '5070', '74687', '4732', '87373', '4611', '83660', '74442', '7069', '1580', '85103', '99653', '25083', '3449', '56969', 'V431', '9950', '6203', 'V4960', 'V552', '3220', '9839', 'E9357', '42731', '1551', 'V9089', '4718', '9551', '17342', '64231', '04101', '56942', '52809', '75320', '71238', '92301', '4408', '1975', '65414', '2300', 'E8496', 'E9178', 'V5413', '1723', '0416', 'E9505', '72706', '63592', 'V8709', 'E8138', '73390', '5804', '78099', '1514', 'E9410', 'V461', '5160', '92619', 'E8348', '80316', '64111', '2862', '79021', '71918', 'E9394', '7142', '49382', '75311', '6002', '2130', '20951', '4291', '78650', '25051', '73005', 'V4289', '85204', '5564', '96905', '1893', '03843', 'E8796', '30781', '72273', '43410', '9213', '1986', '85306', '8170', '8075', 'V4581', 'V672', '4413', '2123', '27700', '57149', '75322', '78937', '30400', '3348', '88019', '75026', '29690', '72704', '2848', '1503', '0272', '9691', '4590', '4619', '2571', '78900', 'E9500', '3411', '27950', '4421', 'V122', '64864', '5672', '4470', '267', '76401', '4257', '07998', '73309', '76515', '1918', '9584', '20301', '99631', '75314', '76524', 'V433', '20780', '34511', 'V6406', '4779', '5829', '9510', '3341', '2391', '75732', '08249', '80662', '20030', '86394', '2903', '4940', '17331', '5581', 'E8062', '1505', '9038', '3100', '00581', '9172', '71500', '73018', '28863', '6271', '76504', '77430', '1448', 'V151', '73393', '20930', '0980', '4829', '70707', '37430', '03841', '30113', '2531', '31289', 'E899', '64804', '9331', 'E8844', '73006', '7799', '6142', '41402', '9013', '34440', '99881', '3207', '29530', '9947', '3674', '2358', '56089', '3595', 'E8703', '5718', '6272', '36210', '53150', '9755', '75489', '45377', 'E9430', 'E8151', '80851', '7441', 'V161', '4475', '9579', 'E9370', '4959', '99652', '1104', '25033', 'V118', '84500', '2720', '35782', '2395', 'V4582', '86812', '932', '5678', '72251', '8248', '3089', '0417', '3393', '99771', '29512', '2356', '5130', '20600', '6164', '42612', '1838', '5370', '06642', '80006', '4400', '00804', 'V5843', '53641', '7500', '72191', '85301', '9571', '7796', 'V1261', '5730', '138', '04185', 'E9299', 'V1859', '90081', '8244', 'E8709', '72400', '71697', '1966', '73710', '0318', '0993', '1972', '53531', '4281', '4351', '9020', 'V4577', '1103', '78459', 'V694', '7948', '74510', '80146', '04109', '25023', '53089', '5981', '44382', '5118', '30301', '4555', '82110', '99831', '56200', '1400', '4375', 'E8127', '2739', '4556', '4352', '1883', 'V8812', '42823', '8860', '20511', '2386', '1727', '82525', '4572', '0778', '76516', '7295', '2281', 'E0139', '5552', '7135', '3555', 'E9334', '94524', '8601', 'V626', '8088', '2141', '6805', '5568', '45182', '431', '36441', 'V1047', '48230', '6258', '67014', 'V061', '7836', '99564', '95899', '35571', 'V141', '52406', '43830', '36900', 'E8200', '29381', '72982', '64761', '33819', '2820', '36003', '78843', '20891', '3484', '56039', 'E8609', '5088', '9529', '6118', '69551', 'V1251', '2397', '88002', '3804', '86340', '42511', '51851', '7809', '99661', '34570', '1725', '1800', '4830', 'E9673', '1398', 'E8603', '2864', 'V1659', '83900', '0398', '5224', '7464', '3708', '9832', '77088', 'E8188', 'V195', '80435', '2730', '30563', '69514', '3911', 'E9433', '8972', '86403', '64831', '4255', '9539', '1532', '2355', '64623', '8782', '71903', '37556', '64894', '78055', '1943', '36500', '05413', '65441', '35989', '40210', '6232', 'E8735', '78600', '99762', '0059', '5191', '34409', '51882', '82301', '3940', '33818', '74762', '3523', '4474', '7294', '6221', '7200', '70719', '3709', 'E9585', 'V549', '8799', '80026', '7847', 'E8795', '82522', '5163', '41410', '3452', '83100', '700', '3951', '78261', '4150', '6391', 'E915', '2773', '90140', 'V1082', '3300', '73711', '30440', '37882', '6983', '74511', '73722', '29562', '92309', '8442', '0796', '73028', '20278', '2115', '8629', '8056', 'E856', 'E8797', 'E8588', '7837', '25202', '77189', '5186', '71906', '1175', '7041', '1208', '7634', 'E8163', '07053', '3301', '80372', '69550', '71888', '2707', '36646', '20979', 'E9506', '9913', '59371', '1414', '71650', '5760', '9778', '7590', '7913', '8911', '46430', '2798', '185', '76384', '1461', '28804', '25001', '8055', 'E9211', '5680', 'V8543', '2884', 'E8809', '53550', '90142', '7450', '5800', '56971', '1735', 'E8495', '20201', 'V4572', 'V2541', '1251', '9617', '66604', '7611', '64263', 'V1071', '81221', '5374', 'E8748', '75010', '85121', '63412', '28243', '85180', 'V741', '29623', '90233', '7138', '70703', '87350', '514', 'V536', '1958', 'E9393', '85210', '2810', '95919', '59651', '311', '1745', '1100', '2443', 'E8585', '64913', '92420', '47821', 'E9620', '2725', '9721', '6963', '3591', '80841', '85401', '72664', '82310', '30493', '4582', '07810', 'E9579', '71699', '78901', 'V1005', '01194', '0949', '66561', '76506', '53012', '7742', '9623', '99800', '96569', '684', '9570', '5798', '44773', '32362', '4131', '80441', 'E9249', '5755', '7963', 'E882', 'E9530', '7817', '76519', '73025', '01123', '53560', 'V5049', 'E9340', '7771', '47875', '8065', '37887', '77018', 'V618', '78062', '86343', '5559', 'V8409', 'E8918', '0520', '52510', '3007', '53561', '1541', '30022', '74259', 'V1851', '7582', '24911', '2630', '8242', '66524', '5120', '79409', 'V1051', '7728', '2726', '28260', '8730', '87323', '60784', '82122', '48240', '7458', '80350', '6869', '7625', 'V289', '33709', '74601', '2888', 'E9504', '99642', '29382', '3485', '1569', '5848', '53782', '7868', '7789', '6959', '53250', '3941', '8751', 'V1089', 'E8587', '30562', '3590', '5550', '319', '9042', '86510', '09181', '82322', 'V6549', '78903', '55200', '57471', '20002', '58181', '5967', 'V625', 'V463', '42831', '55011', '585', '29590', '4589', '81323', '4738', '58089', '9190', '7038', '9949', '75440', '3862', '1430', '59970', '8872', '81110', '71591', '7823', '80174', '34280', '79551', '37955', '1951', 'V861', 'E9502', '73314', '34461', '8605', '36230', '81303', '85181', '86122', '80229', 'V198', '34591', '5942', '7542', '2111', '82022', '7955', '0392', '2984', '77982', '9753', '79431', '94332', '34200', '5832', '3441', 'V4561', 'V9039', '2912', '7887', '7210', '99684', '7766', '83907', '72632', '64914', 'E9363', '73329', '462', '9095', '9125', '79095', '95202', '6822', '58081', 'V1369', '35789', '8208', '78094', '1599', '83313', '33822', '7310', '07999', 'V503', '82390', '2164', '3548', '74860', '56481', '99687', '4578', '19889', '05329', '5081', '9067', '76512', '2353', '25011', '86612', '59373', '33829', '81103', '6940', '1769', '60499', '5185', '0491', '90225', '56202', '2702', '9831', '5909', '27502', '53789', '7758', '2367', '20220', '04590', '72409', '36106', 'V252', '7685', '84509', '7670', '40401', '496', '7262', '8249', '3313', '82302', '3240', 'E8538', '80112', '7721', '73016', '3488', '7149', '20961', '27489', '5845', '64681', '2119', 'E0071', 'E9348', '2594', '8260', '7514', '43301', '85173', '5078', '3556', '9800', '4171', '72885', 'V1584', 'V8741', '80227', 'E8706', '390', '37603', '7856', '80507', '2169', '2599', '07041', '2120', '9233', '43831', '78932', 'V111', '4568', '2764', '5856', '23331', 'V4365', '43330', '75530', '1732', '6150', 'E9496', '5809', '49121', '73810', '64621', '72889', '85300', '3732', 'E8260', '1702', '1598', '29181', '86603', '37189', '67204', '90210', '20501', '20208', '36221', '85202', '5812', '79955', 'V446', '94335', '5183', '73743', '2530', '36213', '85101', 'E9278', '72703', '8796', '80842', '5082', '25201', '0389', '64121', '3769', 'E8490', '6039', 'V5412', 'V4976', '8630', '65411', '80016', '90001', '79929', '78492', '53300', '7061', 'E0291', '8831', '193', '9773', '81240', '77087', '41082', '80002', '71815', 'E9331', 'E8504', '37520', '72990', 'V0950', '1543', '99809', '6000', '7624', '80119', '470', '07051', '40403', '53290', '85162', '11590', '83809', '2519', '85206', '9075', '9635', 'E8908', '4789', '8250', 'V1254', '7724', '2768', 'E9503', '9534', '99666', '7560', '81302', '34580', '59789', '7944', '1573', '4580', '9252', '3102', 'V5864', '1710', '5939', '82321', 'V4579', 'E9196', '05379', '29041', 'V011', '25020', '82300', '78832', '7895', '6191', '20210', '2153', '5571', '45389', '2468', 'E8242', '2949', 'E9550', '34442', '83209', 'V449', '86602', '80700', '1534', '5178', 'E959', 'V441', 'V5401', 'V5866', '92710', '4374', '05419', '7662', '72611', '99583', '7874', '2793', '7529', '2788', '78933', '5583', '33382', 'V4511', '2898', 'E9304', '38302', '63451', '99672', '9112', 'E9344', 'E9220', '28652', 'V469', 'V8489', '37635', '0912', '44489', 'E8210', '3340', '2533', '7088', '67451', '75502', '7132', '66911', '03812', '8704', '30393', '80336', '9559', 'E9443', '9308', '5902', '40491', 'V7219', '7618', '99667', 'E9678', '8708', '70702', '20530', '30470', '5579', '9211', '72973', '33183', '9828', '7098', '64883', '66934', '7321', '80022', '9086', '2799', 'E9248', '55091', '28851', '71985', 'E8541', '78559', 'V188', '24281', '99586', '6188', 'E8783', '5819', 'E9384', '1467', '9920', '37716', '32713', '76621', 'V0990', '20152', '37853', '30423', 'E8230', '3062', '86399', '99594', '9349', 'E8503', '1889', '9651', '1718', '53120', '68609', '20200', '3869', '5731', '52460', '5834', '53430', '0038', '27661', '2883', '80236', '9974', '7283', '80325', '5851', '99671', '53260', '64944', '5273', '72672', '75881', '1552', '5711', '6828', '45386', '52331', '34691', '74512', '4379', '55093', '7485', '0579', 'E8298', '1744', '570', '95208', '9530', '37239', '94128', '5690', '34120', '38619', '8970', 'E8902', '1530', 'E9397', '78720', '7509', '7774', '2520', '1890', 'V854', '64822', '80010', '0780', 'E9679', '55229', '6821', '56211', '9170', '81403', '11285', '99656', 'V435', '8850', '81343', '605', '37733', '8620', '3536', '5180', '1940', '81500', '3570', '95205', '7780', '8362', 'V0381', '80608', '7323', '99664', '30421', '38630', '0700', '74569', '5379', '55329', '29281', '73320', 'E8248', '20512', '87322', '52801', '49321', '85309', '7773', 'V1352', 'E8889', '0622', '81000', '3553', '57440', '81308', '90451', '3643', 'E9001', '6183', 'V660', '7470', '2740', '4266', '86330', 'V293', '8026', '20921', '88012', 'E8197', '76405', '41051', '74720', '260', '2940', '1518', '5724', 'V8537', 'V0739', '99647', 'E8181', '65101', '05311', '78461', '6970', '34291', '8910', '63572', 'E9429', '69513', '7813', '7569', '6961', '5929', '29592', '5961', '05889', '5309', '25510', '77082', '5200', '9130', '5110', '38906', '42742', '4232', 'V1083', '04183', '69581', '71690', '3749', '99604', '48281', '68111', 'E9019', 'E8042', '56986', '8973', '33372', '45387', '79510', 'E8217', '73097', '81307', '44621', '7744', '04186', '78001', '64884', '8470', '9072', '1624', '71105', '9951', 'E9346', '1844', '28800', '7422', '47831', '220', '4386', '42769', '64271', '1512', '95203', '88003', '81313', '53781', 'V4364', '1533', '7767', '01402', '45383', '75329', '75615', 'E0062', '340', '1149', '72871', '81341'}\n",
            "6918 unique codes found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create translation dictionaries\n",
        "index_to_code = {i: code for i, code in enumerate(all_codes)}\n",
        "print(index_to_code)\n",
        "code_to_index = {code: i for i, code in enumerate(all_codes)}\n",
        "print(code_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZW006xMuFt3",
        "outputId": "75b4ad60-6576-487b-8683-e4df38c42ad3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '44289', 1: '0542', 2: '45350', 3: '81383', 4: '76408', 5: '74489', 6: '38905', 7: '07052', 8: 'E8768', 9: '74742', 10: '30491', 11: '20218', 12: '7013', 13: '37820', 14: '41404', 15: '8251', 16: '7859', 17: 'E9687', 18: '72743', 19: 'V580', 20: '73316', 21: '27906', 22: '78799', 23: '75265', 24: '4401', 25: '80182', 26: 'V153', 27: '25030', 28: '20963', 29: '5691', 30: '4440', 31: '8488', 32: '28859', 33: 'E8494', 34: '5272', 35: '9894', 36: '2769', 37: '20903', 38: '71589', 39: 'E8793', 40: '7459', 41: '7012', 42: '4370', 43: 'E8532', 44: '30989', 45: '3492', 46: '80420', 47: '4599', 48: '3221', 49: '3510', 50: '53570', 51: '92810', 52: '5523', 53: '36616', 54: '8371', 55: 'V058', 56: '1516', 57: '76070', 58: '77183', 59: '37741', 60: '3577', 61: '25541', 62: '53190', 63: '01896', 64: '64893', 65: '59010', 66: '4843', 67: 'V1204', 68: '9580', 69: '01504', 70: 'E9470', 71: 'V0259', 72: 'V155', 73: 'E8353', 74: '75261', 75: '1401', 76: '36283', 77: '1965', 78: '9040', 79: '36846', 80: '73002', 81: '99939', 82: '45371', 83: '42781', 84: '53370', 85: '20028', 86: 'V444', 87: '7843', 88: '99769', 89: 'V8745', 90: '64252', 91: '34481', 92: '71530', 93: '71593', 94: '2771', 95: 'V5873', 96: '430', 97: '3337', 98: '78451', 99: '0310', 100: '9065', 101: '7299', 102: 'V6441', 103: '6023', 104: '83920', 105: '83301', 106: '6159', 107: 'E9571', 108: '9054', 109: '7862', 110: '85229', 111: '85406', 112: '44589', 113: '56982', 114: 'V5831', 115: '734', 116: '1410', 117: '5233', 118: 'V420', 119: '1409', 120: '7350', 121: '29600', 122: '9581', 123: '34882', 124: 'E9307', 125: '20288', 126: '9535', 127: '23879', 128: '1913', 129: '20252', 130: '34541', 131: '45374', 132: '4404', 133: '81012', 134: '2156', 135: '86121', 136: '3410', 137: '76711', 138: '2821', 139: '7910', 140: '74912', 141: '2829', 142: '7100', 143: 'E9000', 144: '78961', 145: '25061', 146: 'E8841', 147: '6145', 148: 'V5301', 149: '2140', 150: 'V3101', 151: '30183', 152: '19881', 153: '20382', 154: '2882', 155: '0311', 156: '38600', 157: '3526', 158: '4439', 159: '07799', 160: '1542', 161: '83904', 162: '9553', 163: '6951', 164: '71155', 165: '7530', 166: '28246', 167: '99668', 168: 'V4586', 169: '64843', 170: '37999', 171: 'E8586', 172: '5100', 173: '45620', 174: '2334', 175: '80122', 176: '20070', 177: '5970', 178: '47820', 179: '3308', 180: '7384', 181: '7280', 182: '2250', 183: '2454', 184: '22802', 185: '86113', 186: '34981', 187: '75501', 188: '2449', 189: 'V421', 190: '82382', 191: '64862', 192: '31381', 193: '38900', 194: '2651', 195: 'E9411', 196: '44581', 197: '6938', 198: '2537', 199: '2752', 200: '7613', 201: '74910', 202: '81219', 203: 'V1559', 204: '41020', 205: '46410', 206: '1905', 207: '78061', 208: '0527', 209: '55092', 210: '47400', 211: 'E8700', 212: '78602', 213: '4772', 214: 'E8718', 215: '75470', 216: 'E8654', 217: '80123', 218: '47824', 219: 'V4459', 220: '2309', 221: '1361', 222: '20148', 223: '33510', 224: '13109', 225: '5993', 226: '36251', 227: '78830', 228: '82001', 229: '27652', 230: '71104', 231: 'E9425', 232: 'E8840', 233: 'V851', 234: '81601', 235: '30403', 236: '80032', 237: '38917', 238: '1108', 239: '71680', 240: '6950', 241: 'V192', 242: '87269', 243: '7810', 244: '7660', 245: '1623', 246: '7612', 247: 'E9354', 248: '2979', 249: 'E8624', 250: '7994', 251: 'E8851', 252: '9991', 253: '6926', 254: '7512', 255: '8875', 256: '9301', 257: '48241', 258: '4420', 259: '8363', 260: '80610', 261: '7080', 262: '95901', 263: '5890', 264: '37500', 265: '4871', 266: '04105', 267: '34202', 268: '75889', 269: '6802', 270: '38612', 271: '51634', 272: '24960', 273: '73349', 274: '71840', 275: '7701', 276: '9983', 277: '51901', 278: '99883', 279: '9462', 280: '3688', 281: '8190', 282: '7888', 283: '34670', 284: '5771', 285: '38871', 286: 'E9677', 287: '44771', 288: '81249', 289: '81259', 290: '78869', 291: '990', 292: '37612', 293: 'E8140', 294: '2375', 295: '58389', 296: '3154', 297: '66932', 298: '7048', 299: '7420', 300: 'E9351', 301: '71942', 302: 'E9175', 303: '9260', 304: '2469', 305: 'V5861', 306: '5589', 307: '7220', 308: '37530', 309: '37432', 310: '38000', 311: 'V174', 312: '20260', 313: '7700', 314: 'E9670', 315: '71531', 316: 'V173', 317: '83902', 318: 'E989', 319: '3649', 320: '53270', 321: '3550', 322: '45111', 323: 'V1819', 324: '3319', 325: '53510', 326: '6249', 327: '80001', 328: '04089', 329: '5931', 330: '6011', 331: 'V4589', 332: 'E8141', 333: '82380', 334: '81401', 335: '9047', 336: '5720', 337: '7245', 338: '78031', 339: '70521', 340: '3502', 341: 'E9379', 342: '2360', 343: '7937', 344: '481', 345: '53230', 346: '11519', 347: '64783', 348: '9996', 349: '7336', 350: 'E8613', 351: '86414', 352: 'V1079', 353: 'E8162', 354: '7234', 355: '37555', 356: '81202', 357: '29502', 358: 'E8161', 359: '8870', 360: '95209', 361: '41400', 362: '9050', 363: '74353', 364: '2763', 365: '1270', 366: '86814', 367: '85200', 368: '82342', 369: 'E8180', 370: '9198', 371: 'V065', 372: '9597', 373: 'V425', 374: 'E9805', 375: '7482', 376: '5511', 377: 'E9651', 378: '9104', 379: '6021', 380: '27951', 381: '42099', 382: '6143', 383: '85135', 384: '30749', 385: '37956', 386: '1612', 387: 'V4965', 388: '3580', 389: '1348', 390: '31539', 391: '08240', 392: 'V1241', 393: '75982', 394: '59654', 395: '86345', 396: '44032', 397: '73089', 398: '9010', 399: '5291', 400: '5722', 401: 'V554', 402: 'V1209', 403: 'E9421', 404: '20943', 405: '64811', 406: '29622', 407: '3361', 408: '94321', 409: '4928', 410: '42610', 411: '37993', 412: '81392', 413: 'E9018', 414: '62570', 415: '7538', 416: '2339', 417: '95892', 418: '27949', 419: '30303', 420: '2631', 421: '1508', 422: '71909', 423: 'E9229', 424: '6213', 425: '29634', 426: '20205', 427: '7945', 428: '73732', 429: '37632', 430: '0701', 431: '9851', 432: '7209', 433: '0790', 434: '6144', 435: '2860', 436: 'E9225', 437: '74721', 438: '66624', 439: '412', 440: 'V4573', 441: '72293', 442: '72671', 443: '4847', 444: '85256', 445: '72888', 446: '4801', 447: '85222', 448: 'E8740', 449: '79902', 450: '3091', 451: '80502', 452: 'V272', 453: '5824', 454: 'E9850', 455: '24210', 456: 'E9445', 457: '55012', 458: '36253', 459: '36234', 460: '4660', 461: '43321', 462: '1463', 463: '5839', 464: '9858', 465: '7094', 466: 'V062', 467: '9540', 468: '25081', 469: '75450', 470: '80430', 471: '7754', 472: '7759', 473: '36641', 474: '42613', 475: '5060', 476: '38014', 477: '3599', 478: '72690', 479: '6390', 480: '1363', 481: '0919', 482: '65671', 483: '71687', 484: '3203', 485: '56981', 486: '20023', 487: '5969', 488: '56961', 489: '2758', 490: '2555', 491: '5789', 492: '9953', 493: '4230', 494: '1101', 495: '74362', 496: '7511', 497: '55841', 498: '3729', 499: '64683', 500: 'E9353', 501: '30981', 502: '30523', 503: '1892', 504: 'V5869', 505: '76529', 506: '4411', 507: '30012', 508: '9074', 509: '317', 510: '86331', 511: '81201', 512: '7535', 513: '9052', 514: 'V6443', 515: '5258', 516: '30750', 517: '76076', 518: '3619', 519: '20198', 520: '75263', 521: '59589', 522: '73819', 523: '23876', 524: '30285', 525: '8082', 526: '2978', 527: '30481', 528: '81511', 529: '1418', 530: '11284', 531: '74710', 532: '9108', 533: 'E9382', 534: '5305', 535: '37922', 536: '5300', 537: '5278', 538: '9696', 539: '7466', 540: '90441', 541: '94519', 542: '7683', 543: '5950', 544: '5830', 545: '71844', 546: 'E8767', 547: '4019', 548: '63411', 549: '7756', 550: '7705', 551: '48231', 552: '55320', 553: '17332', 554: '58381', 555: '1177', 556: '38100', 557: '80360', 558: '90287', 559: '74363', 560: '01136', 561: '64801', 562: '90082', 563: '55321', 564: 'E0000', 565: 'E8123', 566: 'V024', 567: '85140', 568: '39899', 569: '7449', 570: '71222', 571: '2132', 572: '0400', 573: 'E8791', 574: 'E8211', 575: '49392', 576: '65821', 577: 'E9871', 578: '7212', 579: '7463', 580: '2689', 581: '72290', 582: '37005', 583: '7617', 584: '2142', 585: 'V152', 586: '5560', 587: '40310', 588: '1983', 589: '31401', 590: 'E8810', 591: 'V4576', 592: '53211', 593: '72972', 594: '80034', 595: '86610', 596: '44329', 597: '04182', 598: '70409', 599: '85215', 600: '7802', 601: 'E9854', 602: '1738', 603: 'E9204', 604: '6171', 605: '37272', 606: '44030', 607: '64203', 608: '86502', 609: '80144', 610: '57461', 611: '8364', 612: '4558', 613: '36801', 614: '53551', 615: '37992', 616: '6264', 617: 'E975', 618: '7355', 619: '85011', 620: '59011', 621: '86342', 622: '7768', 623: '81210', 624: 'V2501', 625: '7673', 626: '2830', 627: '81311', 628: '6162', 629: 'E0073', 630: '4808', 631: 'E8240', 632: '86349', 633: '78606', 634: '85224', 635: '43884', 636: '66401', 637: 'V581', 638: '77211', 639: '4786', 640: 'E9250', 641: '7292', 642: '51900', 643: '35800', 644: '67454', 645: '33384', 646: 'V403', 647: 'E970', 648: '01085', 649: '66921', 650: '5695', 651: '2312', 652: '1941', 653: '20078', 654: '80220', 655: '07070', 656: '2559', 657: '9089', 658: 'V0189', 659: '30015', 660: '36511', 661: '7661', 662: '82130', 663: '6251', 664: 'E927', 665: '73734', 666: '63421', 667: '1968', 668: '262', 669: '0478', 670: '9593', 671: '27650', 672: '76503', 673: '01193', 674: '4272', 675: '41000', 676: '9878', 677: '71516', 678: '3344', 679: '75310', 680: '475', 681: '78601', 682: '75312', 683: '3529', 684: '87202', 685: '6923', 686: '41403', 687: '5735', 688: '20920', 689: '38003', 690: '07819', 691: '5650', 692: 'E8849', 693: '5533', 694: 'E8251', 695: '82392', 696: 'E8145', 697: 'E0010', 698: '1561', 699: '3430', 700: '81231', 701: 'E8121', 702: '1961', 703: '86229', 704: '24901', 705: '7806', 706: 'E8881', 707: '8460', 708: '53521', 709: '29640', 710: 'V071', 711: '53261', 712: '64842', 713: '6809', 714: '3200', 715: '29900', 716: '75614', 717: '1885', 718: '6930', 719: '88001', 720: '8191', 721: '5693', 722: '0709', 723: '80315', 724: '0979', 725: '2122', 726: '80011', 727: '2732', 728: '28311', 729: '87343', 730: '2713', 731: 'V1020', 732: '37410', 733: '2879', 734: '3238', 735: '81613', 736: '71489', 737: '2781', 738: 'V553', 739: '3668', 740: '44282', 741: '1628', 742: '2929', 743: '6238', 744: '78605', 745: 'E8129', 746: '2822', 747: 'V020', 748: '57401', 749: '33391', 750: '3491', 751: '8873', 752: 'V078', 753: 'E8190', 754: '4296', 755: '9664', 756: '0846', 757: '9708', 758: '7626', 759: '41031', 760: '42518', 761: '75313', 762: '29564', 763: '36589', 764: 'E8853', 765: '80125', 766: '1731', 767: '78607', 768: '72887', 769: '99701', 770: '52563', 771: '3343', 772: '55090', 773: '2102', 774: '2330', 775: '58889', 776: '20590', 777: '99563', 778: '1125', 779: 'V1084', 780: 'E9100', 781: '7861', 782: '2154', 783: '9972', 784: '9962', 785: 'E8171', 786: 'E9674', 787: 'V1389', 788: '40591', 789: '1540', 790: '71891', 791: '61171', 792: '99582', 793: 'E8540', 794: '1610', 795: '8830', 796: '61689', 797: '4359', 798: 'E986', 799: 'V5041', 800: '8793', 801: '87341', 802: '2458', 803: '84200', 804: '78940', 805: '36552', 806: '8832', 807: '71831', 808: '38912', 809: '00861', 810: '4259', 811: '03844', 812: '76494', 813: '4294', 814: '4781', 815: '1123', 816: '73396', 817: '9189', 818: '74365', 819: '20957', 820: '9724', 821: '8822', 822: '3083', 823: '2865', 824: '8787', 825: '2362', 826: '76389', 827: '3419', 828: '4519', 829: '05479', 830: '75016', 831: '45375', 832: '7635', 833: '64823', 834: '6910', 835: 'E8584', 836: '29681', 837: '20243', 838: '8602', 839: 'E8880', 840: '7488', 841: 'E9803', 842: '80425', 843: '78630', 844: '2388', 845: '43883', 846: '7762', 847: '47874', 848: '38023', 849: '4200', 850: 'V180', 851: '5644', 852: '66021', 853: '6038', 854: '83109', 855: '1622', 856: '7078', 857: 'E8548', 858: '44284', 859: '30592', 860: '27789', 861: '8076', 862: '58281', 863: '1501', 864: '37611', 865: '2873', 866: 'V653', 867: '8785', 868: '8902', 869: '44021', 870: '3960', 871: 'V568', 872: '99802', 873: '3373', 874: '69510', 875: '9690', 876: '8360', 877: 'V8524', 878: '64313', 879: '7388', 880: '63502', 881: '85239', 882: '72882', 883: '4412', 884: '485', 885: '83921', 886: '36289', 887: '25031', 888: '17322', 889: '40200', 890: '66551', 891: '7840', 892: '4481', 893: 'E954', 894: '66574', 895: '3798', 896: '5219', 897: '75611', 898: '8461', 899: '0329', 900: '9081', 901: '20410', 902: '67414', 903: '7630', 904: '7871', 905: '85201', 906: '67432', 907: '47870', 908: '37482', 909: '99679', 910: '81003', 911: '80432', 912: '67004', 913: '0041', 914: '30541', 915: '4920', 916: '58289', 917: '4260', 918: '5600', 919: '92820', 920: '7510', 921: '7608', 922: '9032', 923: '42689', 924: '76526', 925: '8790', 926: '34400', 927: '71926', 928: '45821', 929: '81610', 930: '7633', 931: '5238', 932: '0545', 933: '34510', 934: '28244', 935: '25063', 936: '7103', 937: '80223', 938: '2419', 939: '3910', 940: '37452', 941: '1105', 942: '5994', 943: '3698', 944: '94504', 945: 'V1006', 946: '9140', 947: '5521', 948: '43853', 949: '80232', 950: '6920', 951: '71895', 952: '35922', 953: '85190', 954: '80426', 955: '7601', 956: '7296', 957: '72610', 958: '60091', 959: '9039', 960: '2518', 961: '86415', 962: '1109', 963: '78659', 964: '4532', 965: '78839', 966: '7776', 967: '6072', 968: '75739', 969: 'E9240', 970: '20401', 971: '5723', 972: 'V1381', 973: 'E8556', 974: '2254', 975: 'V1042', 976: '80165', 977: '78499', 978: '80225', 979: '71856', 980: '72763', 981: 'V620', 982: '72660', 983: '79381', 984: '53010', 985: '0380', 986: '99601', 987: 'V1259', 988: '1455', 989: '9090', 990: '8960', 991: 'V4281', 992: '81250', 993: '7942', 994: 'E8554', 995: '2381', 996: '53110', 997: '8024', 998: '75557', 999: '72761', 1000: '9219', 1001: '71887', 1002: '78838', 1003: '67154', 1004: '7421', 1005: '42830', 1006: '20974', 1007: '3540', 1008: '48801', 1009: '36565', 1010: '79552', 1011: '60490', 1012: '82021', 1013: '8820', 1014: 'E8133', 1015: '7912', 1016: '4878', 1017: '20150', 1018: 'V3001', 1019: '07022', 1020: '33989', 1021: 'V0389', 1022: '2693', 1023: 'V1053', 1024: 'E9303', 1025: '71509', 1026: '56941', 1027: '8977', 1028: '5234', 1029: '65951', 1030: '23770', 1031: '8703', 1032: '95204', 1033: '9694', 1034: 'E8192', 1035: '99562', 1036: '9001', 1037: '7429', 1038: '38010', 1039: '77212', 1040: '44031', 1041: '9152', 1042: '24280', 1043: '77439', 1044: '94325', 1045: '38420', 1046: '7402', 1047: '74560', 1048: '20281', 1049: '0270', 1050: '80106', 1051: '986', 1052: '27903', 1053: '63380', 1054: '3051', 1055: 'E8257', 1056: '51884', 1057: '7260', 1058: '7615', 1059: '98989', 1060: '81301', 1061: '71949', 1062: '3578', 1063: '39891', 1064: '80504', 1065: 'E8110', 1066: '99581', 1067: '72619', 1068: '71784', 1069: '56401', 1070: '5173', 1071: '9243', 1072: '75321', 1073: '9517', 1074: '5888', 1075: '55010', 1076: '56213', 1077: '8748', 1078: '80136', 1079: 'E8318', 1080: '51911', 1081: '393', 1082: '81254', 1083: '72886', 1084: '99812', 1085: '4139', 1086: '37300', 1087: '87200', 1088: '33182', 1089: 'E0032', 1090: 'E9572', 1091: '85316', 1092: '9031', 1093: '86819', 1094: 'E8405', 1095: '30250', 1096: '71855', 1097: '80602', 1098: 'V1072', 1099: '5880', 1100: '80501', 1101: '7825', 1102: '80704', 1103: '3480', 1104: '81200', 1105: 'E9559', 1106: '436', 1107: 'E9352', 1108: '40311', 1109: '64844', 1110: '51283', 1111: '6031', 1112: '64781', 1113: '4910', 1114: 'V171', 1115: '45342', 1116: '5759', 1117: '73315', 1118: '67002', 1119: '0391', 1120: '1714', 1121: '1603', 1122: '3694', 1123: '19882', 1124: '2750', 1125: 'E9401', 1126: '29654', 1127: '33385', 1128: '502', 1129: '7628', 1130: '13101', 1131: '4416', 1132: '3942', 1133: '59080', 1134: '43400', 1135: '6254', 1136: '78906', 1137: '9684', 1138: 'V4502', 1139: '2512', 1140: '7791', 1141: '81600', 1142: '2724', 1143: 'E9800', 1144: '1420', 1145: '3483', 1146: '7864', 1147: '66942', 1148: 'V1061', 1149: 'E9688', 1150: 'E9412', 1151: '1370', 1152: '30570', 1153: '81241', 1154: '8690', 1155: 'E8859', 1156: '5401', 1157: '24980', 1158: '01304', 1159: '5764', 1160: '80025', 1161: '9942', 1162: '2698', 1163: 'V6149', 1164: 'V1202', 1165: 'V1000', 1166: '20722', 1167: '80626', 1168: 'V182', 1169: '1121', 1170: '99569', 1171: '0049', 1172: '566', 1173: '3060', 1174: '4280', 1175: 'V6103', 1176: 'E8241', 1177: '3888', 1178: '3152', 1179: '80621', 1180: '29040', 1181: '37900', 1182: '36847', 1183: '62210', 1184: '8246', 1185: 'V860', 1186: 'E9383', 1187: '59001', 1188: '80151', 1189: '74684', 1190: '55221', 1191: '6178', 1192: '64881', 1193: '5699', 1194: 'V230', 1195: '82003', 1196: '28959', 1197: '3180', 1198: '73007', 1199: '4279', 1200: '1458', 1201: 'E8762', 1202: '7824', 1203: '42979', 1204: '80105', 1205: '28241', 1206: 'E8493', 1207: '4290', 1208: '1110', 1209: '34403', 1210: '6012', 1211: '7461', 1212: '9587', 1213: '6208', 1214: '5768', 1215: '79989', 1216: '2875', 1217: 'V5411', 1218: 'E9399', 1219: 'E8744', 1220: 'E9463', 1221: '9697', 1222: '80230', 1223: '5779', 1224: 'V510', 1225: '99989', 1226: '78051', 1227: '9041', 1228: '7140', 1229: '5161', 1230: '3314', 1231: '37689', 1232: '6827', 1233: '72691', 1234: '7708', 1235: '6110', 1236: '9713', 1237: '3879', 1238: '78059', 1239: '9754', 1240: '4809', 1241: '01205', 1242: '0880', 1243: '94423', 1244: '42842', 1245: '7109', 1246: 'E9358', 1247: '72292', 1248: '29012', 1249: '66811', 1250: '73609', 1251: '6255', 1252: '6829', 1253: '85102', 1254: '80170', 1255: 'V1586', 1256: '20502', 1257: '7891', 1258: '1733', 1259: 'E9588', 1260: '8220', 1261: '5181', 1262: '20312', 1263: '87360', 1264: '7354', 1265: '20282', 1266: '6200', 1267: '8505', 1268: '4144', 1269: '9890', 1270: '5651', 1271: 'V1749', 1272: '4010', 1273: '4512', 1274: '2827', 1275: 'V608', 1276: '94421', 1277: '90002', 1278: '3481', 1279: '9802', 1280: '9181', 1281: '4160', 1282: '79500', 1283: '99982', 1284: '9893', 1285: 'V1641', 1286: '34939', 1287: '78729', 1288: '20018', 1289: '72272', 1290: '3489', 1291: '78060', 1292: 'V4981', 1293: '75251', 1294: '37140', 1295: '86339', 1296: '76521', 1297: '17372', 1298: '7102', 1299: '51909', 1300: '2333', 1301: '79311', 1302: '7687', 1303: '64784', 1304: '95206', 1305: '7248', 1306: 'E8742', 1307: '65701', 1308: '99644', 1309: '2662', 1310: '5821', 1311: '9523', 1312: '42090', 1313: '86800', 1314: '20480', 1315: '9631', 1316: 'V1001', 1317: '45189', 1318: 'E9671', 1319: '57421', 1320: '2890', 1321: '2849', 1322: '7786', 1323: '43850', 1324: '3003', 1325: '33390', 1326: '37800', 1327: '34881', 1328: 'V066', 1329: 'V667', 1330: '44422', 1331: '2731', 1332: 'V4451', 1333: '1468', 1334: '326', 1335: '92310', 1336: '44100', 1337: '99639', 1338: '74193', 1339: '81220', 1340: '2513', 1341: 'V5811', 1342: '30924', 1343: '1611', 1344: '5601', 1345: '4419', 1346: '76493', 1347: '5989', 1348: '9122', 1349: '37501', 1350: '07071', 1351: '3629', 1352: '40492', 1353: '67153', 1354: '8221', 1355: '2738', 1356: '75520', 1357: '2897', 1358: '3509', 1359: '75219', 1360: 'V1043', 1361: '53520', 1362: 'V1059', 1363: '9982', 1364: '9619', 1365: '2376', 1366: '7901', 1367: '53649', 1368: '75616', 1369: 'E9888', 1370: 'E8800', 1371: '27401', 1372: '74349', 1373: '4820', 1374: '27503', 1375: 'V1004', 1376: '20971', 1377: '17341', 1378: '05312', 1379: 'V1012', 1380: '0414', 1381: '2390', 1382: 'E9313', 1383: '03810', 1384: '8780', 1385: '59969', 1386: '65421', 1387: '5109', 1388: '36859', 1389: '80326', 1390: '5528', 1391: '76522', 1392: 'V172', 1393: '95911', 1394: '7428', 1395: '83305', 1396: '78891', 1397: '59972', 1398: '90182', 1399: '70583', 1400: '81100', 1401: '7273', 1402: '9100', 1403: '75526', 1404: '37749', 1405: '2766', 1406: '64841', 1407: '80150', 1408: '87261', 1409: '80506', 1410: '5302', 1411: '4473', 1412: '66541', 1413: '2948', 1414: '56409', 1415: '6205', 1416: '07811', 1417: '1119', 1418: '2777', 1419: '72665', 1420: '25082', 1421: '6979', 1422: '27501', 1423: '29614', 1424: '40391', 1425: '7718', 1426: '7702', 1427: '61800', 1428: '0085', 1429: '55121', 1430: '2144', 1431: 'V113', 1432: '72883', 1433: '20203', 1434: '3963', 1435: '81230', 1436: 'E8705', 1437: '1307', 1438: '60886', 1439: '67404', 1440: '07033', 1441: '9642', 1442: 'E9222', 1443: '40511', 1444: 'V1301', 1445: '43814', 1446: '46450', 1447: '61650', 1448: 'E9600', 1449: '1562', 1450: '261', 1451: '7788', 1452: '75560', 1453: '25062', 1454: '49390', 1455: '25041', 1456: 'E9200', 1457: '80030', 1458: '38321', 1459: '76514', 1460: 'V291', 1461: '71107', 1462: 'V430', 1463: '99641', 1464: '04112', 1465: '53200', 1466: '87374', 1467: '28733', 1468: '99739', 1469: 'E8311', 1470: 'V3100', 1471: '75529', 1472: '5920', 1473: '3363', 1474: '01164', 1475: '72402', 1476: '67412', 1477: '83309', 1478: '7798', 1479: 'E9109', 1480: '29544', 1481: '46431', 1482: '4389', 1483: '1869', 1484: '7906', 1485: '72991', 1486: '00841', 1487: '4822', 1488: '81409', 1489: '34571', 1490: '7822', 1491: '30590', 1492: '90251', 1493: '53084', 1494: '36207', 1495: '73399', 1496: '01803', 1497: '74100', 1498: '41181', 1499: '8795', 1500: '0479', 1501: '73381', 1502: '3530', 1503: '74103', 1504: '71947', 1505: '2521', 1506: '85211', 1507: '2127', 1508: '56081', 1509: '5371', 1510: '71917', 1511: '40211', 1512: '47412', 1513: '8922', 1514: '78842', 1515: '56738', 1516: '9994', 1517: '2767', 1518: '5187', 1519: '7242', 1520: '80071', 1521: '7522', 1522: '72884', 1523: 'E9424', 1524: 'V8544', 1525: '78701', 1526: '7092', 1527: '88102', 1528: '2410', 1529: '68110', 1530: '99677', 1531: 'E8528', 1532: '9043', 1533: '4376', 1534: '72190', 1535: '5164', 1536: '94204', 1537: '99567', 1538: '87342', 1539: '73302', 1540: '92232', 1541: '7333', 1542: '64673', 1543: '4559', 1544: '6868', 1545: '36001', 1546: '25040', 1547: '43490', 1548: '41405', 1549: '1179', 1550: '5960', 1551: '74101', 1552: '4749', 1553: '40390', 1554: '5753', 1555: 'E8233', 1556: '4178', 1557: '25053', 1558: '8058', 1559: '71865', 1560: '60820', 1561: '99649', 1562: '9110', 1563: '6119', 1564: '80332', 1565: '25012', 1566: '3009', 1567: '64133', 1568: '9670', 1569: '38022', 1570: '27709', 1571: '7778', 1572: '86813', 1573: '77213', 1574: '20283', 1575: '85205', 1576: '7689', 1577: '9693', 1578: '2114', 1579: '81010', 1580: 'E9343', 1581: '5570', 1582: '3542', 1583: '85125', 1584: '87201', 1585: '9809', 1586: '2650', 1587: 'E8120', 1588: '25002', 1589: '1535', 1590: '72679', 1591: '71941', 1592: '7536', 1593: '92800', 1594: '3201', 1595: '42292', 1596: '78194', 1597: '60011', 1598: '71666', 1599: '85402', 1600: '9070', 1601: 'V652', 1602: '6009', 1603: '9767', 1604: '2840', 1605: '1891', 1606: '2441', 1607: 'V0382', 1608: 'E8227', 1609: '7830', 1610: '30460', 1611: '83905', 1612: 'E916', 1613: '46451', 1614: '4170', 1615: 'E8582', 1616: '53400', 1617: '81418', 1618: '85245', 1619: '8028', 1620: '1106', 1621: '38915', 1622: '67333', 1623: 'E8131', 1624: '42611', 1625: '99673', 1626: '4289', 1627: '73023', 1628: '2881', 1629: '36510', 1630: '37214', 1631: '31234', 1632: '0529', 1633: '36201', 1634: 'E9342', 1635: 'V1009', 1636: '1952', 1637: '66531', 1638: '0498', 1639: '3575', 1640: '3010', 1641: '0860', 1642: '5763', 1643: '37943', 1644: '80142', 1645: '53081', 1646: 'E9306', 1647: '1734', 1648: '80130', 1649: '6198', 1650: '69012', 1651: '6260', 1652: '71905', 1653: '40301', 1654: '78064', 1655: '92421', 1656: '04189', 1657: '6924', 1658: '28862', 1659: '37730', 1660: '1760', 1661: '28262', 1662: '7916', 1663: '2148', 1664: '37313', 1665: '5226', 1666: '53220', 1667: '66822', 1668: 'V5881', 1669: '34590', 1670: '1990', 1671: '2440', 1672: '51189', 1673: '78079', 1674: '82520', 1675: '8439', 1676: '2396', 1677: 'E9422', 1678: '49320', 1679: '94534', 1680: 'E8189', 1681: 'V0980', 1682: '1912', 1683: 'V4512', 1684: '07031', 1685: '43810', 1686: '64274', 1687: '99676', 1688: 'E8641', 1689: '75981', 1690: 'V1642', 1691: '3438', 1692: '56781', 1693: 'E848', 1694: '57450', 1695: '56944', 1696: '43320', 1697: '82312', 1698: '35921', 1699: '80031', 1700: '67484', 1701: 'E8142', 1702: '99669', 1703: '76382', 1704: '58189', 1705: '86232', 1706: '0859', 1707: '77016', 1708: '03289', 1709: '80601', 1710: '20192', 1711: '81243', 1712: '78723', 1713: '20206', 1714: '73741', 1715: '74740', 1716: '80843', 1717: 'E9289', 1718: 'E9554', 1719: '2113', 1720: '80009', 1721: '27411', 1722: '7919', 1723: '80616', 1724: '28411', 1725: '2392', 1726: '20925', 1727: '56962', 1728: '29573', 1729: '59000', 1730: '25043', 1731: '9556', 1732: '29604', 1733: '37855', 1734: '99659', 1735: 'E0190', 1736: '2872', 1737: '1917', 1738: 'V5812', 1739: 'V1007', 1740: '80306', 1741: '92401', 1742: 'V8801', 1743: '4011', 1744: 'V1869', 1745: '71592', 1746: '36250', 1747: '71103', 1748: '77981', 1749: 'V3401', 1750: '24990', 1751: '96903', 1752: '20300', 1753: '7335', 1754: '8786', 1755: '96502', 1756: '6944', 1757: '6982', 1758: '78603', 1759: '38860', 1760: '80075', 1761: '3315', 1762: '8438', 1763: '7842', 1764: '74686', 1765: '5169', 1766: '6151', 1767: '34620', 1768: 'E9238', 1769: '4549', 1770: '75613', 1771: '2861', 1772: '9992', 1773: '5373', 1774: 'V1049', 1775: '80113', 1776: '7571', 1777: '78071', 1778: '7455', 1779: '23771', 1780: 'E9179', 1781: '2762', 1782: 'V602', 1783: 'E9347', 1784: '74922', 1785: '03811', 1786: '20003', 1787: '66944', 1788: '4423', 1789: 'E9314', 1790: 'V4509', 1791: '7575', 1792: '2101', 1793: '5361', 1794: '3897', 1795: '75989', 1796: '44024', 1797: '7755', 1798: '2652', 1799: '63522', 1800: '4476', 1801: '1808', 1802: '99683', 1803: '2380', 1804: '40400', 1805: '5647', 1806: '73015', 1807: '75482', 1808: '1449', 1809: '5854', 1810: '81509', 1811: '5080', 1812: 'E9455', 1813: '2751', 1814: '6172', 1815: '1602', 1816: '64264', 1817: '7581', 1818: '75839', 1819: 'E9193', 1820: 'E8786', 1821: 'V08', 1822: '56201', 1823: 'E9325', 1824: '64214', 1825: '56731', 1826: '30540', 1827: '32723', 1828: 'E963', 1829: '83979', 1830: '45352', 1831: '20024', 1832: 'V290', 1833: '1969', 1834: '5279', 1835: '86239', 1836: 'V812', 1837: '52462', 1838: '35579', 1839: '80324', 1840: '936', 1841: '36100', 1842: '0931', 1843: '71537', 1844: '64671', 1845: '71198', 1846: '38021', 1847: 'E9319', 1848: '28981', 1849: '1945', 1850: '3071', 1851: '9605', 1852: 'V0981', 1853: '1730', 1854: '73605', 1855: '4548', 1856: '20020', 1857: '30490', 1858: 'V708', 1859: '47833', 1860: 'E8314', 1861: 'E9392', 1862: '42653', 1863: '5551', 1864: '5111', 1865: '99732', 1866: '1453', 1867: '1489', 1868: '5225', 1869: '7028', 1870: 'E9324', 1871: '42989', 1872: '0940', 1873: '74782', 1874: '9171', 1875: '11289', 1876: '48284', 1877: '27788', 1878: '90450', 1879: '99645', 1880: '80164', 1881: '66941', 1882: '84512', 1883: '36012', 1884: '1761', 1885: '42832', 1886: '9055', 1887: '42983', 1888: '30551', 1889: '1977', 1890: 'V1589', 1891: '5303', 1892: '74902', 1893: '2553', 1894: '38830', 1895: '2191', 1896: '75460', 1897: '38611', 1898: '7686', 1899: 'E8850', 1900: '30009', 1901: '3384', 1902: 'V1252', 1903: '9083', 1904: '5831', 1905: 'E8781', 1906: '1743', 1907: 'E857', 1908: '70706', 1909: '99593', 1910: '79589', 1911: '4220', 1912: '41061', 1913: '7783', 1914: 'V5842', 1915: '74732', 1916: '85146', 1917: '9552', 1918: 'V1062', 1919: '0415', 1920: '7243', 1921: '80859', 1922: 'E0031', 1923: '92410', 1924: '01186', 1925: '85105', 1926: '73340', 1927: '7244', 1928: 'V202', 1929: '28522', 1930: '47832', 1931: '36206', 1932: '74429', 1933: '3612', 1934: 'V4966', 1935: '80412', 1936: '37274', 1937: '463', 1938: '3434', 1939: '2121', 1940: '32727', 1941: '85241', 1942: '486', 1943: 'E8589', 1944: '71996', 1945: '51852', 1946: '6806', 1947: '28952', 1948: 'E9682', 1949: '64903', 1950: 'E8860', 1951: '82009', 1952: 'V558', 1953: '4618', 1954: '20969', 1955: 'V053', 1956: 'E9335', 1957: '75169', 1958: '3898', 1959: 'V1060', 1960: '34600', 1961: '4460', 1962: '1231', 1963: '40493', 1964: '04104', 1965: '81109', 1966: '7358', 1967: '60889', 1968: '4821', 1969: '71946', 1970: 'E8714', 1971: '43889', 1972: 'E9262', 1973: '4539', 1974: '34489', 1975: '9330', 1976: 'E8219', 1977: '7313', 1978: 'E9010', 1979: '7424', 1980: '41092', 1981: '34982', 1982: '86513', 1983: '6931', 1984: 'V123', 1985: '78003', 1986: '52512', 1987: '9592', 1988: '0074', 1989: '7821', 1990: '80109', 1991: '33722', 1992: '71430', 1993: '9222', 1994: '95891', 1995: '2150', 1996: 'E9309', 1997: '4770', 1998: '86611', 1999: '86412', 2000: '37775', 2001: '66634', 2002: '226', 2003: '2818', 2004: 'E9108', 2005: '33912', 2006: 'E8845', 2007: '5935', 2008: '80718', 2009: '6209', 2010: '4542', 2011: '74861', 2012: '57512', 2013: 'E9538', 2014: '81612', 2015: '1984', 2016: '56782', 2017: '9779', 2018: '4592', 2019: '41512', 2020: '85216', 2021: '80503', 2022: '65961', 2023: '59389', 2024: '7672', 2025: '95219', 2026: '8240', 2027: '2352', 2028: '74749', 2029: 'V1509', 2030: '30473', 2031: '35981', 2032: '59800', 2033: '28529', 2034: 'E8152', 2035: '65953', 2036: '8054', 2037: '2165', 2038: '99682', 2039: 'E8782', 2040: '04111', 2041: '29212', 2042: 'V4361', 2043: '8691', 2044: '81407', 2045: '587', 2046: '86613', 2047: '7716', 2048: '20011', 2049: '29520', 2050: '74682', 2051: '87412', 2052: '8509', 2053: '1953', 2054: '65423', 2055: '73012', 2056: '6989', 2057: '78821', 2058: '99811', 2059: 'V167', 2060: '88023', 2061: '82030', 2062: '80196', 2063: '41012', 2064: '9557', 2065: '5562', 2066: '00843', 2067: '25000', 2068: '9712', 2069: 'E8788', 2070: '30502', 2071: '80015', 2072: '27669', 2073: '75610', 2074: '4441', 2075: '5128', 2076: '3442', 2077: '92303', 2078: '30302', 2079: '53241', 2080: '05881', 2081: 'V029', 2082: '29980', 2083: '8361', 2084: '82332', 2085: '5531', 2086: '7712', 2087: 'E8846', 2088: 'E9081', 2089: 'E9011', 2090: '0521', 2091: '8920', 2092: '71990', 2093: '29653', 2094: '7707', 2095: '80320', 2096: '9029', 2097: '2116', 2098: '6018', 2099: '24290', 2100: 'E9317', 2101: '63311', 2102: '82539', 2103: '80085', 2104: '1749', 2105: '85225', 2106: '57140', 2107: '7921', 2108: '65651', 2109: '8400', 2110: 'V5862', 2111: '3552', 2112: '2125', 2113: '67012', 2114: '9160', 2115: '05313', 2116: '2329', 2117: '37182', 2118: '66411', 2119: '2230', 2120: 'V489', 2121: '62402', 2122: '5671', 2123: '8411', 2124: '67323', 2125: '7320', 2126: '53100', 2127: '7503', 2128: '71597', 2129: '27730', 2130: '1619', 2131: '1122', 2132: '77089', 2133: '38861', 2134: '71239', 2135: '27941', 2136: '74781', 2137: '83961', 2138: 'V5417', 2139: '7731', 2140: 'V1501', 2141: '3212', 2142: '2331', 2143: '30392', 2144: '44629', 2145: '5071', 2146: '4275', 2147: 'V583', 2148: '5719', 2149: '77083', 2150: '3524', 2151: '57511', 2152: 'E8854', 2153: 'V1649', 2154: '70715', 2155: '3181', 2156: '28983', 2157: '5192', 2158: '8074', 2159: '77589', 2160: '51902', 2161: '7827', 2162: '28989', 2163: '73088', 2164: '90220', 2165: 'V462', 2166: '56987', 2167: 'V4986', 2168: '78609', 2169: '2832', 2170: '6945', 2171: '67434', 2172: '80375', 2173: '42654', 2174: 'E8216', 2175: 'E9360', 2176: '27702', 2177: 'V0252', 2178: '3952', 2179: '48232', 2180: '2755', 2181: '1649', 2182: 'E9991', 2183: 'E9318', 2184: '73009', 2185: '41513', 2186: '7851', 2187: 'V160', 2188: '58881', 2189: '20936', 2190: '86601', 2191: '56400', 2192: '7583', 2193: '4142', 2194: '2859', 2195: '78469', 2196: '76496', 2197: '4295', 2198: '72403', 2199: '99933', 2200: '20270', 2201: '8861', 2202: '78199', 2203: '7383', 2204: '45981', 2205: '56989', 2206: 'E8147', 2207: '88110', 2208: 'V143', 2209: '3330', 2210: '9563', 2211: '6396', 2212: '73670', 2213: '38832', 2214: '20890', 2215: '43885', 2216: '05471', 2217: '7443', 2218: '8471', 2219: '31531', 2220: '7902', 2221: '86413', 2222: '32341', 2223: '9594', 2224: '30553', 2225: '9278', 2226: '71986', 2227: '06641', 2228: '3689', 2229: '67402', 2230: '80161', 2231: '30751', 2232: '30150', 2233: '62130', 2234: '2722', 2235: '20191', 2236: '71913', 2237: '8020', 2238: '9248', 2239: '7235', 2240: '9561', 2241: '9245', 2242: 'V5422', 2243: '74869', 2244: 'V4578', 2245: '8700', 2246: '73017', 2247: '7818', 2248: 'V4588', 2249: '69282', 2250: '1550', 2251: '7249', 2252: '4299', 2253: '88100', 2254: 'V5309', 2255: '2982', 2256: '78639', 2257: '8440', 2258: 'V1250', 2259: '5206', 2260: '2870', 2261: '7785', 2262: '41001', 2263: 'E9804', 2264: '1729', 2265: '30583', 2266: 'V1364', 2267: '1129', 2268: '8711', 2269: '75453', 2270: 'E0080', 2271: '38872', 2272: '88111', 2273: 'V331', 2274: '64233', 2275: '75315', 2276: '80475', 2277: '9162', 2278: '1548', 2279: '44409', 2280: '65963', 2281: '9711', 2282: 'E8669', 2283: '5762', 2284: '66554', 2285: 'E8888', 2286: '59581', 2287: '3080', 2288: 'V270', 2289: '2528', 2290: '36320', 2291: '85240', 2292: '96501', 2293: '3334', 2294: '2811', 2295: '77214', 2296: '9196', 2297: '84209', 2298: '30571', 2299: '78009', 2300: '5750', 2301: '03285', 2302: '86321', 2303: '83114', 2304: '4536', 2305: '1642', 2306: '92320', 2307: '1373', 2308: '6175', 2309: '37739', 2310: '79579', 2311: '30420', 2312: 'V8531', 2313: '5538', 2314: '7062', 2315: '36107', 2316: '37273', 2317: 'E0299', 2318: '4541', 2319: '56960', 2320: '7717', 2321: '86409', 2322: '20285', 2323: '7750', 2324: '45382', 2325: 'V5841', 2326: '4387', 2327: 'V8536', 2328: '9191', 2329: '78262', 2330: '29524', 2331: '41412', 2332: '96509', 2333: '325', 2334: '71904', 2335: '73689', 2336: '47829', 2337: '81406', 2338: '49322', 2339: '7962', 2340: '7176', 2341: '46400', 2342: '42971', 2343: '7093', 2344: '83200', 2345: 'V5865', 2346: '07032', 2347: '0840', 2348: '7083', 2349: '6268', 2350: 'V170', 2351: '78862', 2352: '6253', 2353: '2700', 2354: '34693', 2355: '6952', 2356: '99832', 2357: '2704', 2358: '03819', 2359: '71235', 2360: '36315', 2361: '73630', 2362: '83501', 2363: '5168', 2364: '3094', 2365: '71590', 2366: '92311', 2367: '00862', 2368: '36274', 2369: '52101', 2370: '23691', 2371: '20972', 2372: '29632', 2373: '8794', 2374: '53170', 2375: '42291', 2376: '30410', 2377: 'V1507', 2378: '37312', 2379: '9558', 2380: '82129', 2381: 'E8497', 2382: '9068', 2383: '55129', 2384: '0092', 2385: 'V6285', 2386: '3379', 2387: 'V4987', 2388: '76492', 2389: '65583', 2390: '5756', 2391: '36101', 2392: '1515', 2393: '29411', 2394: '78052', 2395: '3962', 2396: '9998', 2397: '6140', 2398: 'E9380', 2399: '2137', 2400: '83503', 2401: '30089', 2402: '80322', 2403: '47822', 2404: '5734', 2405: '9912', 2406: '38908', 2407: '78054', 2408: '2539', 2409: '4480', 2410: '64421', 2411: 'E0026', 2412: '6820', 2413: '36000', 2414: '2839', 2415: '42652', 2416: '71180', 2417: '7089', 2418: '37881', 2419: '95215', 2420: '56721', 2421: '74729', 2422: 'E9294', 2423: '71950', 2424: '1560', 2425: '5799', 2426: 'E8830', 2427: '85175', 2428: '6214', 2429: '0360', 2430: '40501', 2431: '04110', 2432: '82133', 2433: '4417', 2434: 'V4989', 2435: '90289', 2436: '75431', 2437: 'V422', 2438: '72709', 2439: '4531', 2440: '78951', 2441: '7224', 2442: '7753', 2443: '79319', 2444: '7523', 2445: '36910', 2446: '25080', 2447: '9661', 2448: '75262', 2449: 'V0971', 2450: 'E8542', 2451: '3814', 2452: '29532', 2453: '29689', 2454: '80175', 2455: '9120', 2456: '79402', 2457: '70713', 2458: '29624', 2459: '7291', 2460: '632', 2461: 'E9359', 2462: '7981', 2463: '42732', 2464: '7271', 2465: '9093', 2466: '048', 2467: 'E8178', 2468: '1513', 2469: 'E9386', 2470: '725', 2471: 'V162', 2472: '8940', 2473: '64254', 2474: '07953', 2475: '52333', 2476: '33729', 2477: '03849', 2478: '3693', 2479: '69279', 2480: '87371', 2481: '0071', 2482: '3968', 2483: '6261', 2484: '5997', 2485: '35801', 2486: '73312', 2487: 'V7651', 2488: '30422', 2489: '2866', 2490: '7595', 2491: 'E9659', 2492: '2411', 2493: 'V624', 2494: '9473', 2495: 'V1203', 2496: '24950', 2497: '72705', 2498: '9142', 2499: '80311', 2500: '7709', 2501: '01894', 2502: '83104', 2503: '38921', 2504: 'E9060', 2505: '57451', 2506: '6202', 2507: '1641', 2508: '20047', 2509: '92231', 2510: '72230', 2511: '9161', 2512: 'E9298', 2513: 'E8784', 2514: '81514', 2515: '29421', 2516: '47811', 2517: '8027', 2518: '5285', 2519: '0093', 2520: 'E9064', 2521: '64813', 2522: '25050', 2523: '78552', 2524: '9060', 2525: '74920', 2526: '1529', 2527: '7792', 2528: '1450', 2529: '4730', 2530: '4371', 2531: '6988', 2532: '87411', 2533: '37420', 2534: '83101', 2535: 'V5426', 2536: '67324', 2537: '90223', 2538: '25639', 2539: '4918', 2540: '43811', 2541: '2442', 2542: '37842', 2543: '51289', 2544: '2710', 2545: 'E9378', 2546: '3949', 2547: '2761', 2548: '79439', 2549: 'E8502', 2550: '48242', 2551: '3019', 2552: '1910', 2553: '80362', 2554: '0048', 2555: '5119', 2556: 'V6405', 2557: 'V468', 2558: '37203', 2559: '37143', 2560: 'V0482', 2561: '9729', 2562: '44481', 2563: '37630', 2564: '38903', 2565: '97081', 2566: '66111', 2567: '3202', 2568: '0088', 2569: '4320', 2570: '83650', 2571: '52579', 2572: '34540', 2573: '25092', 2574: '99830', 2575: '86393', 2576: '71835', 2577: '37990', 2578: '66511', 2579: '36019', 2580: '7585', 2581: '64863', 2582: '71911', 2583: '71594', 2584: '83901', 2585: '0849', 2586: '7808', 2587: '43813', 2588: '52100', 2589: '6181', 2590: '37923', 2591: 'E8212', 2592: '90254', 2593: 'V789', 2594: '2776', 2595: 'E9391', 2596: '6861', 2597: '85226', 2598: '74602', 2599: '7760', 2600: '9165', 2601: '5770', 2602: '82032', 2603: '83969', 2604: 'V1003', 2605: '7596', 2606: '79953', 2607: '36520', 2608: '36800', 2609: '45184', 2610: '94214', 2611: '75539', 2612: '8209', 2613: '01880', 2614: '80423', 2615: '9562', 2616: '81519', 2617: '3545', 2618: '27802', 2619: '2552', 2620: '90141', 2621: '44389', 2622: 'V016', 2623: '82529', 2624: '55001', 2625: 'V427', 2626: '0463', 2627: 'E0061', 2628: '37311', 2629: 'V453', 2630: '1809', 2631: '5235', 2632: '34281', 2633: 'V9081', 2634: '6398', 2635: '81333', 2636: '3831', 2637: '74441', 2638: '81331', 2639: 'V9103', 2640: '45621', 2641: '6266', 2642: '2842', 2643: '94108', 2644: '5131', 2645: '1390', 2646: '7325', 2647: '9224', 2648: '45829', 2649: '7854', 2650: 'E8619', 2651: '80660', 2652: '5778', 2653: '38015', 2654: '7597', 2655: 'V1081', 2656: '9390', 2657: '6082', 2658: 'V6111', 2659: '4479', 2660: '0902', 2661: '9238', 2662: '6071', 2663: '51853', 2664: '4958', 2665: '5852', 2666: 'E0009', 2667: '9952', 2668: '79415', 2669: '51633', 2670: '96909', 2671: '7933', 2672: '29522', 2673: '20302', 2674: '75510', 2675: '57491', 2676: '76416', 2677: 'V537', 2678: '64294', 2679: 'V601', 2680: '78449', 2681: '28249', 2682: '71930', 2683: '5264', 2684: '43300', 2685: '23872', 2686: '8744', 2687: '4957', 2688: '2581', 2689: 'V2652', 2690: '9698', 2691: '1830', 2692: '7218', 2693: '7960', 2694: '9695', 2695: '20973', 2696: '3158', 2697: '43840', 2698: 'E9241', 2699: '7318', 2700: '78341', 2701: '77182', 2702: '8742', 2703: '6146', 2704: '2270', 2705: 'E9444', 2706: '7621', 2707: '28310', 2708: '85189', 2709: '53171', 2710: '37710', 2711: '90301', 2712: '6180', 2713: '7015', 2714: '25073', 2715: '2874', 2716: '64241', 2717: '3439', 2718: '73008', 2719: '8370', 2720: '71907', 2721: '37941', 2722: '05412', 2723: '70701', 2724: '53240', 2725: '7010', 2726: '25071', 2727: '23875', 2728: '85209', 2729: '34680', 2730: '71296', 2731: '44281', 2732: '6028', 2733: 'E9570', 2734: '82320', 2735: '5752', 2736: '57410', 2737: '80132', 2738: '7908', 2739: 'V5302', 2740: '042', 2741: '82120', 2742: '71966', 2743: 'E9209', 2744: '3659', 2745: '80162', 2746: '64833', 2747: '4540', 2748: 'E9315', 2749: '4489', 2750: '4554', 2751: '64103', 2752: '69515', 2753: '61610', 2754: 'V1309', 2755: '28319', 2756: '80224', 2757: 'V5339', 2758: '80422', 2759: 'E8637', 2760: '4219', 2761: '74401', 2762: '59971', 2763: 'E8552', 2764: '81602', 2765: '75617', 2766: '20001', 2767: '83402', 2768: '7462', 2769: '37240', 2770: '78491', 2771: '74763', 2772: '72291', 2773: '71191', 2774: 'E9889', 2775: 'V4575', 2776: '6170', 2777: '82111', 2778: '4561', 2779: '67311', 2780: '5820', 2781: '75564', 2782: '80609', 2783: '9986', 2784: '64931', 2785: '85246', 2786: '7386', 2787: '33920', 2788: '83202', 2789: '5847', 2790: '6953', 2791: '2511', 2792: '81611', 2793: '1962', 2794: '25090', 2795: '40490', 2796: 'V4985', 2797: 'V9010', 2798: '460', 2799: '9725', 2800: '28264', 2801: '5968', 2802: '9899', 2803: '4269', 2804: 'V169', 2805: '8930', 2806: '01215', 2807: '85400', 2808: '5721', 2809: '01805', 2810: 'V6129', 2811: '71236', 2812: '23871', 2813: '20008', 2814: '7852', 2815: '24991', 2816: '75563', 2817: '78072', 2818: '1228', 2819: '1120', 2820: '71432', 2821: '73342', 2822: '2182', 2823: '25060', 2824: '69512', 2825: '44322', 2826: '9012', 2827: '99971', 2828: '64292', 2829: '8961', 2830: '8247', 2831: '27739', 2832: '90181', 2833: 'V168', 2834: '78934', 2835: '81203', 2836: '57142', 2837: '7816', 2838: '2753', 2839: '1715', 2840: '8472', 2841: '2869', 2842: '81405', 2843: '92321', 2844: '37942', 2845: 'V850', 2846: '8064', 2847: '8631', 2848: '1717', 2849: '80312', 2850: '2541', 2851: '6965', 2852: '99589', 2853: '8080', 2854: '11505', 2855: '1919', 2856: '4613', 2857: '7993', 2858: '0309', 2859: '00845', 2860: '4612', 2861: '57480', 2862: '60090', 2863: '7222', 2864: '5569', 2865: 'E8249', 2866: '52109', 2867: '3885', 2868: '3970', 2869: '9961', 2870: '80180', 2871: '1981', 2872: '7572', 2873: '75683', 2874: '30443', 2875: '7797', 2876: '36844', 2877: '6169', 2878: '2143', 2879: '30580', 2880: '6954', 2881: '78097', 2882: '1124', 2883: '36281', 2884: '74423', 2885: '5859', 2886: '86402', 2887: '90442', 2888: '1911', 2889: '65404', 2890: '72781', 2891: '25042', 2892: '7905', 2893: 'V596', 2894: '7746', 2895: '33720', 2896: '71916', 2897: '43822', 2898: '29420', 2899: '47411', 2900: '66632', 2901: '39890', 2902: '73629', 2903: '4748', 2904: '1273', 2905: '41022', 2906: '2889', 2907: '9228', 2908: 'V4282', 2909: '30472', 2910: '80222', 2911: '92411', 2912: '8840', 2913: 'V1201', 2914: '8676', 2915: '29010', 2916: '86501', 2917: 'E8702', 2918: '8921', 2919: '5811', 2920: '36362', 2921: '80485', 2922: '9051', 2923: 'E976', 2924: '20071', 2925: '53530', 2926: 'E8012', 2927: '7932', 2928: '71108', 2929: '86222', 2930: '86100', 2931: '2251', 2932: '937', 2933: '28982', 2934: '64403', 2935: 'V1041', 2936: '4465', 2937: 'V7281', 2938: '3959', 2939: '677', 2940: '64101', 2941: '4258', 2942: '85242', 2943: '36204', 2944: '99859', 2945: 'V1029', 2946: 'E8415', 2947: '4353', 2948: '42290', 2949: '59659', 2950: '7484', 2951: '8404', 2952: '27787', 2953: '83906', 2954: 'E9452', 2955: 'E9283', 2956: '38300', 2957: 'V3200', 2958: '24291', 2959: 'E0076', 2960: '8068', 2961: '80706', 2962: '52800', 2963: 'V8401', 2964: '56722', 2965: '71696', 2966: '07049', 2967: '74769', 2968: '4148', 2969: '27801', 2970: 'V8539', 2971: '5529', 2972: '5646', 2973: '75550', 2974: '1608', 2975: '2800', 2976: 'E8780', 2977: '30752', 2978: 'E9356', 2979: 'V5332', 2980: 'E9102', 2981: '32720', 2982: 'E0030', 2983: '48289', 2984: 'E9387', 2985: '87349', 2986: '20917', 2987: '34404', 2988: '1578', 2989: '59689', 2990: 'V5482', 2991: '7934', 2992: '4241', 2993: '8083', 2994: '5640', 2995: '92701', 2996: '1703', 2997: '37034', 2998: '37431', 2999: 'V5415', 3000: '9582', 3001: '7566', 3002: '86500', 3003: 'E9322', 3004: '7481', 3005: '4233', 3006: 'E8250', 3007: '81351', 3008: '43852', 3009: '4941', 3010: '44421', 3011: '49122', 3012: '30723', 3013: '55202', 3014: '28732', 3015: 'V641', 3016: '61882', 3017: 'E8529', 3018: '71846', 3019: '04119', 3020: '20287', 3021: '64204', 3022: '2532', 3023: '11599', 3024: '2387', 3025: '36840', 3026: '4881', 3027: '94800', 3028: '44102', 3029: '3519', 3030: '99609', 3031: '3239', 3032: '2851', 3033: '3431', 3034: '4659', 3035: '3559', 3036: '99662', 3037: '53340', 3038: '42833', 3039: '5566', 3040: '65221', 3041: '1160', 3042: '99702', 3043: '74343', 3044: '48282', 3045: '85305', 3046: '7236', 3047: '29042', 3048: '34710', 3049: '1452', 3050: '9999', 3051: '71845', 3052: '30471', 3053: '92612', 3054: '85403', 3055: '9895', 3056: '0539', 3057: '30522', 3058: '4264', 3059: 'V1090', 3060: '2163', 3061: '85219', 3062: '68102', 3063: '3013', 3064: '33523', 3065: '3061', 3066: '29620', 3067: '9514', 3068: '9212', 3069: '75566', 3070: '34889', 3071: '23877', 3072: '2598', 3073: '01895', 3074: '4378', 3075: '87364', 3076: '53013', 3077: '5761', 3078: '64782', 3079: '5952', 3080: '75162', 3081: 'V3201', 3082: '8677', 3083: '74741', 3084: '65613', 3085: '4239', 3086: '7793', 3087: '3229', 3088: '4610', 3089: '4243', 3090: '9500', 3091: 'E9329', 3092: '3969', 3093: 'E9308', 3094: '20975', 3095: '25013', 3096: 'E8785', 3097: '7452', 3098: '64511', 3099: '04184', 3100: '30742', 3101: 'E9650', 3102: '36563', 3103: '28412', 3104: '30552', 3105: 'E8058', 3106: '7704', 3107: '501', 3108: '7812', 3109: '30759', 3110: '541', 3111: '01890', 3112: '03842', 3113: '99799', 3114: 'E9208', 3115: '43310', 3116: '31230', 3117: '70712', 3118: '86384', 3119: '8243', 3120: '490', 3121: '53441', 3122: '30789', 3123: '30461', 3124: '33721', 3125: '28803', 3126: '25010', 3127: '71842', 3128: '1300', 3129: '38039', 3130: '3571', 3131: '7099', 3132: 'E8052', 3133: 'E8191', 3134: '52489', 3135: '78343', 3136: '9744', 3137: '6111', 3138: '5846', 3139: 'V644', 3140: '96561', 3141: '79009', 3142: '25072', 3143: '7197', 3144: '29631', 3145: '4778', 3146: '1431', 3147: '82535', 3148: '99984', 3149: '47830', 3150: '81512', 3151: '99741', 3152: '20212', 3153: 'E8663', 3154: '2823', 3155: '81510', 3156: 'V671', 3157: '33701', 3158: 'E8252', 3159: '86419', 3160: '37601', 3161: '52340', 3162: '4210', 3163: '8900', 3164: '6850', 3165: '30520', 3166: '0412', 3167: '2462', 3168: '1880', 3169: '72740', 3170: '86112', 3171: '8770', 3172: '87363', 3173: '46411', 3174: '82020', 3175: '7480', 3176: 'E8543', 3177: '47825', 3178: '9663', 3179: 'E9408', 3180: '49302', 3181: '3108', 3182: '38201', 3183: '99665', 3184: '45381', 3185: '81502', 3186: '94234', 3187: '71945', 3188: '7359', 3189: '5193', 3190: 'E9419', 3191: '53401', 3192: '81381', 3193: '61179', 3194: 'E8122', 3195: '1173', 3196: '4560', 3197: '4271', 3198: '8600', 3199: '40291', 3200: '1589', 3201: 'E9583', 3202: '075', 3203: '37871', 3204: '87330', 3205: '85131', 3206: 'V643', 3207: 'V452', 3208: '5670', 3209: '83908', 3210: '7467', 3211: '4570', 3212: '7600', 3213: '80172', 3214: '0499', 3215: '29383', 3216: '3069', 3217: '37850', 3218: '1510', 3219: '34581', 3220: '1613', 3221: '80853', 3222: '80708', 3223: '30000', 3224: '53021', 3225: 'E8278', 3226: '73301', 3227: '42091', 3228: '8418', 3229: '94840', 3230: '75559', 3231: '1309', 3232: '9132', 3233: '8504', 3234: '5565', 3235: 'E9413', 3236: '32361', 3237: '76072', 3238: '4739', 3239: '1726', 3240: 'V068', 3241: '6040', 3242: '3574', 3243: '9735', 3244: '61172', 3245: '3380', 3246: '3236', 3247: '78443', 3248: 'E8150', 3249: '40290', 3250: '8713', 3251: 'V298', 3252: '0340', 3253: '7795', 3254: '2382', 3255: '81305', 3256: '79092', 3257: '1174', 3258: '45351', 3259: '73020', 3260: '4292', 3261: 'E8384', 3262: '76497', 3263: '77084', 3264: '38918', 3265: '1588', 3266: '88121', 3267: '30591', 3268: '5758', 3269: '36217', 3270: '5692', 3271: '86804', 3272: '1720', 3273: '6201', 3274: '79509', 3275: '4329', 3276: '1618', 3277: '11283', 3278: '74689', 3279: '2972', 3280: '81353', 3281: '80145', 3282: '80228', 3283: 'E9395', 3284: '8745', 3285: '36960', 3286: '2510', 3287: '8448', 3288: 'V103', 3289: '80466', 3290: '71120', 3291: 'V037', 3292: '94203', 3293: '9881', 3294: 'E8794', 3295: '3241', 3296: '4422', 3297: '1571', 3298: '25091', 3299: '81344', 3300: '99813', 3301: '68100', 3302: '67482', 3303: '94850', 3304: '85223', 3305: '27911', 3306: '9515', 3307: '2812', 3308: '2692', 3309: '72669', 3310: 'E8261', 3311: 'E8655', 3312: '81332', 3313: 'V4569', 3314: '24900', 3315: '7936', 3316: 'E9601', 3317: '81400', 3318: '86509', 3319: '2538', 3320: '86512', 3321: '32726', 3322: '60782', 3323: '4430', 3324: '86802', 3325: '32719', 3326: 'V1505', 3327: '72981', 3328: '1440', 3329: '64261', 3330: '3331', 3331: 'V555', 3332: 'V1504', 3333: '42789', 3334: '71849', 3335: '56881', 3336: '74722', 3337: '34210', 3338: '75433', 3339: '8728', 3340: '01190', 3341: '75019', 3342: '9604', 3343: '7765', 3344: '56985', 3345: '29289', 3346: '73671', 3347: '73313', 3348: '4538', 3349: 'E9190', 3350: '2536', 3351: 'V4574', 3352: '20490', 3353: '60000', 3354: '33900', 3355: '1522', 3356: '80301', 3357: '37950', 3358: '51636', 3359: '85185', 3360: 'E9499', 3361: '75269', 3362: '7541', 3363: '20310', 3364: 'E8228', 3365: '99889', 3366: 'E8381', 3367: '95207', 3368: '9164', 3369: '36816', 3370: '71965', 3371: '82523', 3372: '8672', 3373: '81242', 3374: 'V550', 3375: '41032', 3376: 'E9509', 3377: '81002', 3378: '20042', 3379: '9588', 3380: 'V063', 3381: '30573', 3382: 'E9132', 3383: '4350', 3384: '3543', 3385: '85212', 3386: '30001', 3387: '82123', 3388: '40201', 3389: '64933', 3390: '46619', 3391: '83500', 3392: '33520', 3393: '8871', 3394: '8245', 3395: '6141', 3396: '27509', 3397: '4838', 3398: '6270', 3399: '80703', 3400: '25003', 3401: '4449', 3402: '76508', 3403: '82131', 3404: '78863', 3405: '7020', 3406: 'V426', 3407: '64793', 3408: '3079', 3409: '94532', 3410: '01505', 3411: '77989', 3412: 'V222', 3413: '52550', 3414: '3099', 3415: '5269', 3416: '86381', 3417: '2853', 3418: '5282', 3419: '72283', 3420: '8851', 3421: '99651', 3422: 'V8389', 3423: '8089', 3424: '68101', 3425: '5582', 3426: '4231', 3427: '6010', 3428: '8791', 3429: 'V5867', 3430: '64861', 3431: '85182', 3432: '51181', 3433: '1820', 3434: '73712', 3435: '2967', 3436: '7620', 3437: '8738', 3438: '83302', 3439: '56210', 3440: '4162', 3441: 'E9290', 3442: '52540', 3443: '27901', 3444: '86503', 3445: '8406', 3446: '71106', 3447: 'V4283', 3448: '81342', 3449: '33511', 3450: '5298', 3451: '44772', 3452: '32372', 3453: '53011', 3454: '9599', 3455: '86103', 3456: '179', 3457: '3159', 3458: '47871', 3459: '62989', 3460: '05371', 3461: '28860', 3462: '70721', 3463: '99654', 3464: 'V023', 3465: '2588', 3466: '61801', 3467: 'V4501', 3468: '30561', 3469: '9210', 3470: 'V135', 3471: '33828', 3472: '73022', 3473: '65571', 3474: '36211', 3475: '71580', 3476: '47879', 3477: '77581', 3478: '37854', 3479: '8761', 3480: '1330', 3481: '20190', 3482: '3369', 3483: '76510', 3484: '22809', 3485: '6273', 3486: '70909', 3487: '3490', 3488: '36573', 3489: '57411', 3490: '99932', 3491: '65451', 3492: '9747', 3493: '66612', 3494: '2117', 3495: '36813', 3496: '92300', 3497: '3234', 3498: 'E8792', 3499: '80625', 3500: '75567', 3501: '64664', 3502: '53160', 3503: 'V4031', 3504: '96979', 3505: '33522', 3506: '42650', 3507: '1639', 3508: '29572', 3509: '9509', 3510: '1320', 3511: '78340', 3512: '53541', 3513: '80605', 3514: '71616', 3515: '2535', 3516: '37851', 3517: '99833', 3518: '7475', 3519: '66534', 3520: '7238', 3521: 'E9291', 3522: '73739', 3523: '1916', 3524: '80470', 3525: '7580', 3526: '3482', 3527: '92821', 3528: '5199', 3529: '74320', 3530: '27800', 3531: '45181', 3532: '9710', 3533: '8081', 3534: '0530', 3535: '32737', 3536: '1724', 3537: 'V4963', 3538: '85109', 3539: '1640', 3540: '64234', 3541: '83400', 3542: '75511', 3543: '86803', 3544: '78057', 3545: '3332', 3546: '81419', 3547: '2639', 3548: '78702', 3549: '81412', 3550: '2253', 3551: '79400', 3552: '76402', 3553: '59381', 3554: '78441', 3555: '1700', 3556: '30122', 3557: '5642', 3558: '80600', 3559: '7763', 3560: '5296', 3561: '9569', 3562: '7757', 3563: '4720', 3564: '1978', 3565: '1539', 3566: 'V4971', 3567: '65444', 3568: '78703', 3569: 'E9457', 3570: '80473', 3571: '8408', 3572: 'V1069', 3573: '99685', 3574: '7720', 3575: '5959', 3576: '80082', 3577: '4377', 3578: '76381', 3579: '73004', 3580: '74609', 3581: '29680', 3582: '85203', 3583: '1906', 3584: '1763', 3585: '8750', 3586: 'E9466', 3587: '82534', 3588: '34830', 3589: '80366', 3590: '1982', 3591: '769', 3592: '2459', 3593: '9471', 3594: '2828', 3595: 'E9323', 3596: '316', 3597: '29284', 3598: 'E9051', 3599: '7804', 3600: '66914', 3601: '0793', 3602: '99590', 3603: '80604', 3604: '07030', 3605: '6392', 3606: '99680', 3607: '99527', 3608: '7631', 3609: '3592', 3610: '2306', 3611: '99681', 3612: '44321', 3613: '5563', 3614: '85106', 3615: 'V1508', 3616: '36811', 3617: '5738', 3618: '88000', 3619: '51284', 3620: '83300', 3621: '81001', 3622: '2913', 3623: '2863', 3624: '4581', 3625: '72971', 3626: '45376', 3627: 'V714', 3628: '86355', 3629: '76409', 3630: '99663', 3631: '3593', 3632: '6212', 3633: '76495', 3634: 'E9053', 3635: 'E8182', 3636: 'E9809', 3637: 'E9173', 3638: '7454', 3639: '7930', 3640: '28731', 3641: '33399', 3642: '6279', 3643: '99643', 3644: '23873', 3645: '7211', 3646: '3383', 3647: '78760', 3648: '7876', 3649: '44101', 3650: '2721', 3651: 'E9292', 3652: '99689', 3653: '37024', 3654: '65251', 3655: '34211', 3656: '70714', 3657: '3589', 3658: '94100', 3659: '71481', 3660: '3522', 3661: '78939', 3662: 'V4571', 3663: '1570', 3664: '3068', 3665: '72742', 3666: '73311', 3667: 'E888', 3668: '53989', 3669: '42761', 3670: '37700', 3671: '0540', 3672: '2900', 3673: 'E8580', 3674: 'V8533', 3675: '9989', 3676: '99561', 3677: 'E977', 3678: '8052', 3679: '7835', 3680: 'E9438', 3681: '86354', 3682: '80235', 3683: '4733', 3684: '27900', 3685: '8407', 3686: '9971', 3687: '73001', 3688: '2813', 3689: '9671', 3690: '34290', 3691: '6274', 3692: '85144', 3693: '96972', 3694: '7741', 3695: '75430', 3696: '07020', 3697: '41189', 3698: 'V1811', 3699: '76075', 3700: 'E9177', 3701: '72930', 3702: '2930', 3703: '2682', 3704: 'E8500', 3705: '36043', 3706: '30483', 3707: '81212', 3708: '9063', 3709: '2910', 3710: '1531', 3711: '76528', 3712: 'V1087', 3713: '20929', 3714: '36403', 3715: '5194', 3716: '36105', 3717: '7920', 3718: '30151', 3719: '5996', 3720: '6278', 3721: '05472', 3722: '78053', 3723: '4179', 3724: '36203', 3725: 'E912', 3726: '5275', 3727: '4785', 3728: 'V6542', 3729: '56949', 3730: '80013', 3731: '72210', 3732: '7517', 3733: '9239', 3734: '80023', 3735: '2373', 3736: '47834', 3737: '85231', 3738: '08881', 3739: '72211', 3740: '25093', 3741: '82524', 3742: '43311', 3743: '1537', 3744: '85194', 3745: '5933', 3746: '76523', 3747: '7833', 3748: '8053', 3749: '88013', 3750: '3321', 3751: '01485', 3752: '52181', 3753: 'V5481', 3754: '118', 3755: '2699', 3756: '72612', 3757: '2371', 3758: '78831', 3759: 'V4962', 3760: '4168', 3761: '9973', 3762: '5262', 3763: '71536', 3764: '3549', 3765: '79959', 3766: '7563', 3767: '92400', 3768: '70401', 3769: '29384', 3770: '3739', 3771: '55120', 3772: '34501', 3773: '8604', 3774: '5198', 3775: '78837', 3776: 'E8550', 3777: '80135', 3778: '7599', 3779: '9980', 3780: '2880', 3781: '85221', 3782: 'V1541', 3783: '4358', 3784: '41419', 3785: '99941', 3786: '32742', 3787: '6089', 3788: '53311', 3789: '45372', 3790: '7230', 3791: '33812', 3792: '5293', 3793: '9009', 3794: '6984', 3795: '95914', 3796: '86353', 3797: 'V560', 3798: '20290', 3799: '56030', 3800: '2989', 3801: '61189', 3802: '56212', 3803: '1971', 3804: '1764', 3805: '42760', 3806: '2728', 3807: '1490', 3808: '34441', 3809: '1980', 3810: '8502', 3811: '1574', 3812: '77181', 3813: '3824', 3814: '48283', 3815: '71944', 3816: '20411', 3817: 'E8600', 3818: '30391', 3819: '20048', 3820: '2155', 3821: '6210', 3822: '9654', 3823: '76404', 3824: '08882', 3825: '3081', 3826: '8749', 3827: '69289', 3828: '04103', 3829: '77985', 3830: 'E9063', 3831: '2760', 3832: '88112', 3833: '83103', 3834: '7508', 3835: 'V5863', 3836: '95912', 3837: '36233', 3838: '85142', 3839: '5982', 3840: '9033', 3841: '71912', 3842: '7991', 3843: 'V8538', 3844: '7969', 3845: '938', 3846: '9221', 3847: '76505', 3848: '6960', 3849: '7141', 3850: 'E9068', 3851: '79401', 3852: '77081', 3853: '5277', 3854: '34460', 3855: '452', 3856: '86514', 3857: '74330', 3858: '6918', 3859: '81408', 3860: '9513', 3861: '4870', 3862: 'V4614', 3863: '75029', 3864: '7915', 3865: '05320', 3866: 'E8170', 3867: 'E9174', 3868: '73341', 3869: '66614', 3870: '53201', 3871: '80036', 3872: '71101', 3873: '1630', 3874: '73681', 3875: '33119', 3876: '1881', 3877: '60781', 3878: 'V1502', 3879: 'E8801', 3880: '34431', 3881: '3576', 3882: '1479', 3883: '76501', 3884: '37940', 3885: '20022', 3886: '53082', 3887: '30503', 3888: '1511', 3889: '40300', 3890: '80100', 3891: '3568', 3892: '9879', 3893: '65661', 3894: '85184', 3895: '7850', 3896: '80021', 3897: '80114', 3898: '42840', 3899: '1308', 3900: '9351', 3901: '78864', 3902: '1960', 3903: 'V8525', 3904: '94323', 3905: '9993', 3906: '86809', 3907: '63491', 3908: 'E8551', 3909: '1848', 3910: '9766', 3911: '3569', 3912: '41071', 3913: '94320', 3914: '1874', 3915: '64852', 3916: '53087', 3917: 'V4587', 3918: '52310', 3919: '7931', 3920: '542', 3921: '76499', 3922: '56789', 3923: '27402', 3924: '9975', 3925: '94536', 3926: '7714', 3927: '07983', 3928: '78791', 3929: '36205', 3930: '1920', 3931: '43819', 3932: 'E956', 3933: '79399', 3934: '9683', 3935: '59960', 3936: '25021', 3937: '2734', 3938: 'E9345', 3939: 'V1582', 3940: '7811', 3941: '1991', 3942: 'E9288', 3943: '78960', 3944: '80111', 3945: '99580', 3946: '74190', 3947: '99791', 3948: 'V5883', 3949: '81209', 3950: '7286', 3951: '9600', 3952: 'E8498', 3953: '25208', 3954: '4919', 3955: '38401', 3956: 'V145', 3957: 'E8282', 3958: '5280', 3959: '4373', 3960: '75461', 3961: '85183', 3962: '5949', 3963: '5733', 3964: '24200', 3965: '2181', 3966: '78033', 3967: 'V1050', 3968: 'V5419', 3969: '59582', 3970: '57481', 3971: '80024', 3972: '80120', 3973: '7483', 3974: '11281', 3975: '25022', 3976: '29580', 3977: 'E9201', 3978: '7202', 3979: '30011', 3980: '72633', 3981: '42741', 3982: '2554', 3983: '8731', 3984: 'E9274', 3985: '73319', 3986: '2759', 3987: '66331', 3988: '99686', 3989: '86600', 3990: '29650', 3991: '4321', 3992: '45385', 3993: '9948', 3994: '78550', 3995: 'E9689', 3996: '87352', 3997: '71940', 3998: '9911', 3999: '28866', 4000: 'E9420', 4001: 'V045', 4002: '77210', 4003: '76079', 4004: '7730', 4005: 'V1542', 4006: '53642', 4007: '56984', 4008: 'E8799', 4009: '71533', 4010: '71698', 4011: '45340', 4012: '20021', 4013: '29633', 4014: 'E9501', 4015: '83651', 4016: 'V1585', 4017: '34690', 4018: '78093', 4019: '6184', 4020: '90234', 4021: '1707', 4022: '3335', 4023: '74923', 4024: '86120', 4025: '63320', 4026: 'E8343', 4027: '5951', 4028: '85186', 4029: '7790', 4030: '73013', 4031: 'V189', 4032: '74683', 4033: '99565', 4034: '37200', 4035: '9061', 4036: '03840', 4037: 'E8499', 4038: '3209', 4039: 'V110', 4040: '71695', 4041: '2808', 4042: '1713', 4043: 'E8341', 4044: '42843', 4045: '8171', 4046: '9048', 4047: '40599', 4048: '81391', 4049: '7533', 4050: 'V8532', 4051: '48239', 4052: '80171', 4053: '20050', 4054: '41042', 4055: '1748', 4056: '72673', 4057: '5369', 4058: '56031', 4059: '99709', 4060: '8479', 4061: '7324', 4062: '3699', 4063: '27482', 4064: '71195', 4065: '4169', 4066: '71828', 4067: '70705', 4068: '5289', 4069: '82330', 4070: '20060', 4071: 'V5427', 4072: '4242', 4073: 'V8530', 4074: '2564', 4075: '20211', 4076: '75289', 4077: '5265', 4078: '4530', 4079: '37633', 4080: '34551', 4081: '82019', 4082: '1500', 4083: '74359', 4084: '9630', 4085: '57510', 4086: '4409', 4087: '34931', 4088: '2774', 4089: '1638', 4090: '5769', 4091: '2841', 4092: 'V148', 4093: '07951', 4094: '2718', 4095: '82100', 4096: '36522', 4097: 'V8534', 4098: '57400', 4099: '29570', 4100: '20010', 4101: '43812', 4102: '75612', 4103: '41002', 4104: 'E8811', 4105: '37810', 4106: '7671', 4107: 'E0060', 4108: '55300', 4109: '32381', 4110: '65921', 4111: '2727', 4112: '23989', 4113: '80849', 4114: 'V451', 4115: '30531', 4116: '5712', 4117: '80115', 4118: '5643', 4119: '49391', 4120: 'V4984', 4121: '1572', 4122: '6168', 4123: '0091', 4124: '2871', 4125: '7676', 4126: '46611', 4127: 'E8136', 4128: 'E0162', 4129: '37701', 4130: '44502', 4131: '74711', 4132: '449', 4133: '5934', 4134: 'E8169', 4135: '37952', 4136: '5991', 4137: '7904', 4138: '44620', 4139: '36284', 4140: '7570', 4141: 'V655', 4142: '53140', 4143: '9595', 4144: '34401', 4145: '76491', 4146: '2894', 4147: '34550', 4148: '62211', 4149: 'V4961', 4150: '2452', 4151: '76407', 4152: '2669', 4153: '4251', 4154: '80226', 4155: 'V51', 4156: '94127', 4157: '5362', 4158: '99801', 4159: '1765', 4160: 'V090', 4161: '5259', 4162: 'V5391', 4163: 'E0069', 4164: '5602', 4165: '56983', 4166: '30521', 4167: '28984', 4168: 'E8193', 4169: '1504', 4170: '3572', 4171: '72271', 4172: '81251', 4173: '3129', 4174: '1609', 4175: '1429', 4176: '6084', 4177: '52108', 4178: '78066', 4179: '71598', 4180: 'E9479', 4181: '75513', 4182: 'E8232', 4183: '20400', 4184: '9064', 4185: '2273', 4186: '27549', 4187: '1520', 4188: '36841', 4189: '1488', 4190: '69010', 4191: '7722', 4192: '37652', 4193: 'E9320', 4194: '23773', 4195: '9472', 4196: '2794', 4197: '7784', 4198: '31400', 4199: '80238', 4200: '78002', 4201: '1272', 4202: '55220', 4203: '5409', 4204: '96500', 4205: '3210', 4206: '43821', 4207: 'V0253', 4208: '6079', 4209: '05319', 4210: '5966', 4211: '36252', 4212: '92801', 4213: '74600', 4214: '5728', 4215: 'E851', 4216: 'V1044', 4217: '0419', 4218: 'E8130', 4219: '5713', 4220: '20800', 4221: 'V721', 4222: '45910', 4223: '3669', 4224: '99675', 4225: '77012', 4226: '87354', 4227: '1921', 4228: '8702', 4229: '52342', 4230: '74789', 4231: 'E8852', 4232: '64661', 4233: 'V6141', 4234: '28419', 4235: 'V1529', 4236: '5400', 4237: 'V5849', 4238: '05314', 4239: '41040', 4240: '80070', 4241: '80000', 4242: '94224', 4243: '64824', 4244: '7068', 4245: '28730', 4246: '1469', 4247: '28850', 4248: '2271', 4249: '5941', 4250: '75323', 4251: '71843', 4252: '78905', 4253: '66001', 4254: '81318', 4255: '99772', 4256: 'V556', 4257: 'V0481', 4258: '9760', 4259: '53784', 4260: '9984', 4261: '62981', 4262: '70711', 4263: '70901', 4264: '28749', 4265: '1884', 4266: '0239', 4267: '1736', 4268: '3551', 4269: '4380', 4270: '86411', 4271: '5513', 4272: '8679', 4273: '3029', 4274: '72270', 4275: '3541', 4276: '9995', 4277: 'V872', 4278: '7947', 4279: 'V486', 4280: 'E9385', 4281: '71226', 4282: '1728', 4283: '5283', 4284: '53783', 4285: '43380', 4286: '51919', 4287: '73027', 4288: '86130', 4289: '80631', 4290: '30747', 4291: '1481', 4292: '80116', 4293: '1209', 4294: '33381', 4295: '80709', 4296: '0549', 4297: '30029', 4298: '7282', 4299: '86110', 4300: '99657', 4301: '75619', 4302: '04149', 4303: '32724', 4304: '51881', 4305: 'V640', 4306: '3453', 4307: '30401', 4308: 'V4983', 4309: 'V8541', 4310: '44103', 4311: 'V146', 4312: 'E8798', 4313: '70725', 4314: '9678', 4315: '80126', 4316: '4848', 4317: '29540', 4318: '2357', 4319: '36372', 4320: '53291', 4321: '7537', 4322: 'E8761', 4323: 'E8769', 4324: '82532', 4325: '2448', 4326: '82381', 4327: '80416', 4328: '99640', 4329: '2301', 4330: '73345', 4331: '57470', 4332: '9585', 4333: '0785', 4334: '44023', 4335: '38922', 4336: '71897', 4337: '86131', 4338: '80436', 4339: 'V5423', 4340: '99700', 4341: '7992', 4342: '6256', 4343: '29644', 4344: '78907', 4345: 'V454', 4346: '43411', 4347: '9679', 4348: '30431', 4349: '79099', 4350: '3759', 4351: '53490', 4352: '95214', 4353: '27542', 4354: '85314', 4355: '5304', 4356: '4471', 4357: '64943', 4358: '3899', 4359: 'E8146', 4360: '99931', 4361: '3682', 4362: '32351', 4363: '78930', 4364: 'V698', 4365: '30411', 4366: '9180', 4367: '96901', 4368: 'V1242', 4369: '78959', 4370: 'V074', 4371: '7935', 4372: '3561', 4373: '7610', 4374: '33189', 4375: 'V163', 4376: '76527', 4377: '6289', 4378: '6825', 4379: '76525', 4380: '24221', 4381: '72141', 4382: '7881', 4383: 'V0489', 4384: '36231', 4385: '36400', 4386: '1739', 4387: '5679', 4388: '7711', 4389: '90229', 4390: '85404', 4391: '07889', 4392: '37289', 4393: '7469', 4394: '78065', 4395: '1509', 4396: '4780', 4397: '34212', 4398: 'V5844', 4399: '4784', 4400: '8701', 4401: '80603', 4402: '5121', 4403: '80624', 4404: '7504', 4405: '3445', 4406: '1620', 4407: 'V017', 4408: '79380', 4409: '73029', 4410: '43881', 4411: '64131', 4412: '00847', 4413: '7586', 4414: '84503', 4415: '6981', 4416: 'V1086', 4417: '78909', 4418: '2534', 4419: '94209', 4420: '80508', 4421: 'V4973', 4422: '30021', 4423: 'E966', 4424: '4553', 4425: '7131', 4426: '90226', 4427: 'E8668', 4428: '30014', 4429: '9685', 4430: '36970', 4431: '1898', 4432: '83401', 4433: '41511', 4434: 'V8821', 4435: '3562', 4436: 'V448', 4437: '86511', 4438: '80711', 4439: '217', 4440: '9011', 4441: '5940', 4442: 'E9138', 4443: '95893', 4444: '80234', 4445: '6235', 4446: '73679', 4447: '8901', 4448: '45379', 4449: '6116', 4450: '67403', 4451: '7564', 4452: '2778', 4453: '7473', 4454: 'V4450', 4455: '85145', 4456: '7869', 4457: '88120', 4458: '40411', 4459: 'V4585', 4460: '8500', 4461: '57420', 4462: 'E0089', 4463: '80622', 4464: '64892', 4465: '43882', 4466: '5793', 4467: '7505', 4468: '78321', 4469: '37489', 4470: '66131', 4471: 'E0020', 4472: 'V1651', 4473: 'E911', 4474: '59370', 4475: 'E9242', 4476: 'E8716', 4477: '86350', 4478: '87321', 4479: 'V600', 4480: '8912', 4481: '79022', 4482: 'V8381', 4483: '3418', 4484: '04671', 4485: '920', 4486: 'V8522', 4487: 'V091', 4488: 'V1279', 4489: '9082', 4490: 'E9270', 4491: '29625', 4492: '5849', 4493: '7246', 4494: 'V6109', 4495: 'V716', 4496: '41519', 4497: '64663', 4498: 'E8698', 4499: '20240', 4500: '1985', 4501: '4372', 4502: '79093', 4503: '72142', 4504: '72879', 4505: '1538', 4506: '2733', 4507: 'E9301', 4508: '83813', 4509: '9102', 4510: '34831', 4511: '2867', 4512: '52511', 4513: '75160', 4514: '29990', 4515: '37909', 4516: '42490', 4517: '05443', 4518: '2572', 4519: '5780', 4520: '23772', 4521: '5253', 4522: '1329', 4523: '85232', 4524: '7279', 4525: '76511', 4526: '27403', 4527: '42820', 4528: 'E9205', 4529: '53410', 4530: '71902', 4531: '4467', 4532: '6262', 4533: '86383', 4534: '2337', 4535: '82531', 4536: '20000', 4537: '30430', 4538: '3342', 4539: '64814', 4540: 'V1553', 4541: '92811', 4542: '2394', 4543: '9341', 4544: '7828', 4545: '7070', 4546: '30390', 4547: '64243', 4548: '80321', 4549: '64821', 4550: '51631', 4551: '8441', 4552: '1759', 4553: '80410', 4554: '53501', 4555: '73733', 4556: '2166', 4557: '29211', 4558: 'V400', 4559: '2298', 4560: '70900', 4561: 'E9390', 4562: '2779', 4563: '56889', 4564: '2379', 4565: '2589', 4566: '9726', 4567: '76403', 4568: '9583', 4569: 'E8041', 4570: '1471', 4571: '8712', 4572: '9220', 4573: '0388', 4574: '9941', 4575: '01354', 4576: '37000', 4577: '80141', 4578: '41010', 4579: '99779', 4580: '3789', 4581: '37443', 4582: '0844', 4583: '80060', 4584: 'V0261', 4585: '22801', 4586: '36901', 4587: '1629', 4588: '1840', 4589: 'E0161', 4590: 'E8199', 4591: '1536', 4592: '28950', 4593: '8792', 4594: '41411', 4595: '72989', 4596: '72999', 4597: '81501', 4598: '64251', 4599: '27540', 4600: '75521', 4601: '71691', 4602: 'E9293', 4603: '9957', 4604: '04100', 4605: '92720', 4606: 'V0991', 4607: '81503', 4608: '85150', 4609: '67202', 4610: '82101', 4611: '75319', 4612: '1521', 4613: '86801', 4614: '85302', 4615: '1568', 4616: '80124', 4617: '07054', 4618: '29651', 4619: '30928', 4620: '1768', 4621: '1970', 4622: '5790', 4623: '32382', 4624: '30402', 4625: '9046', 4626: '90242', 4627: '7284', 4628: '25542', 4629: '2210', 4630: '5439', 4631: 'E0008', 4632: '7101', 4633: '74685', 4634: '1419', 4635: 'V301', 4636: '33902', 4637: '05440', 4638: '1882', 4639: '40413', 4640: '9053', 4641: '5184', 4642: '66582', 4643: '73620', 4644: '0413', 4645: 'V1551', 4646: '56729', 4647: 'E9330', 4648: '5363', 4649: '2384', 4650: '83311', 4651: '86320', 4652: '2824', 4653: '5988', 4654: 'E9198', 4655: 'V5489', 4656: '8241', 4657: 'E9364', 4658: '0051', 4659: 'E9478', 4660: '3222', 4661: '81244', 4662: '70709', 4663: '9701', 4664: '64653', 4665: '1914', 4666: '34839', 4667: '79094', 4668: 'E0064', 4669: 'E0138', 4670: 'V1253', 4671: '7726', 4672: '79671', 4673: '85100', 4674: '63552', 4675: '44020', 4676: '34500', 4677: '71847', 4678: '20213', 4679: '4111', 4680: '41401', 4681: '80705', 4682: '7622', 4683: '1832', 4684: '32089', 4685: 'E9453', 4686: '86405', 4687: '24230', 4688: 'E9426', 4689: '78050', 4690: '75249', 4691: '3518', 4692: '2112', 4693: '24201', 4694: '683', 4695: 'V181', 4696: '6929', 4697: '5561', 4698: '8261', 4699: '1118', 4700: '28489', 4701: '53085', 4702: '38869', 4703: '37924', 4704: '64904', 4705: '53020', 4706: 'V1022', 4707: '73300', 4708: '2515', 4709: '85405', 4710: '8281', 4711: '87264', 4712: '76518', 4713: '6948', 4714: '69018', 4715: '41041', 4716: '79001', 4717: '65801', 4718: '42981', 4719: '81411', 4720: '0279', 4721: '75651', 4722: '2971', 4723: '1464', 4724: 'E8717', 4725: '81382', 4726: '4431', 4727: '75733', 4728: '34402', 4729: '41072', 4730: '42293', 4731: '3971', 4732: '27953', 4733: '75671', 4734: '24981', 4735: '2690', 4736: '01405', 4737: '85310', 4738: '82132', 4739: '52410', 4740: '7540', 4741: '73600', 4742: '34430', 4743: '49301', 4744: '76513', 4745: '9149', 4746: '515', 4747: '3829', 4748: '74900', 4749: '2920', 4750: '90453', 4751: '7866', 4752: '71213', 4753: '64201', 4754: '7366', 4755: '88020', 4756: '9173', 4757: '99602', 4758: '36232', 4759: '81101', 4760: '70400', 4761: '45119', 4762: 'E8196', 4763: '44283', 4764: '78904', 4765: '37275', 4766: '7623', 4767: 'V551', 4768: '5710', 4769: '24220', 4770: '25801', 4771: '7539', 4772: 'E8790', 4773: '05821', 4774: '3017', 4775: '86102', 4776: 'V1272', 4777: '78651', 4778: '36202', 4779: '83303', 4780: '72751', 4781: '9518', 4782: '8025', 4783: '79923', 4784: '1460', 4785: '2749', 4786: '64303', 4787: '71848', 4788: 'V140', 4789: '95200', 4790: '6192', 4791: '9680', 4792: '32725', 4793: '3370', 4794: '86400', 4795: '99693', 4796: '64224', 4797: '67401', 4798: '29282', 4799: '28240', 4800: 'V5331', 4801: '29560', 4802: '0039', 4803: '87344', 4804: '38610', 4805: '0030', 4806: '4710', 4807: '4466', 4808: '6823', 4809: '43391', 4810: '2540', 4811: '7513', 4812: '53101', 4813: 'V6284', 4814: '2819', 4815: 'E8258', 4816: '60001', 4817: '7589', 4818: '71694', 4819: '1648', 4820: '4552', 4821: 'E8583', 4822: '28521', 4823: '79029', 4824: '5260', 4825: '38870', 4826: '78947', 4827: 'E9460', 4828: 'V5399', 4829: '82331', 4830: '80620', 4831: '33394', 4832: '31239', 4833: '71693', 4834: '49381', 4835: '9092', 4836: '1915', 4837: '82521', 4838: '90221', 4839: '9273', 4840: '28801', 4841: '37751', 4842: 'E9507', 4843: 'E9581', 4844: '2409', 4845: '6149', 4846: '71943', 4847: '3076', 4848: '7831', 4849: 'E9801', 4850: '70700', 4851: '70710', 4852: '1976', 4853: '71821', 4854: '4414', 4855: '0319', 4856: '07989', 4857: '6218', 4858: '07059', 4859: '3310', 4860: '30530', 4861: '0383', 4862: '2939', 4863: '8760', 4864: '1712', 4865: '71596', 4866: 'V1088', 4867: '05410', 4868: '94126', 4869: '3182', 4870: '2189', 4871: '99592', 4872: 'V1506', 4873: '7872', 4874: '4510', 4875: '83903', 4876: '7845', 4877: '78865', 4878: 'V707', 4879: '7863', 4880: '2858', 4881: '20037', 4882: '99603', 4883: 'E964', 4884: '42651', 4885: '4478', 4886: '2706', 4887: 'E9389', 4888: '7961', 4889: '6269', 4890: '6259', 4891: '03641', 4892: '87320', 4893: '36241', 4894: '7949', 4895: '94522', 4896: '79539', 4897: '80476', 4898: '49120', 4899: 'V6289', 4900: '55100', 4901: '41090', 4902: 'E8238', 4903: 'E9295', 4904: '7423', 4905: 'E8842', 4906: '28951', 4907: '25032', 4908: '7079', 4909: '70724', 4910: '3961', 4911: '74761', 4912: '83411', 4913: '8678', 4914: '87353', 4915: '75555', 4916: '37515', 4917: '30501', 4918: '64803', 4919: '80702', 4920: '2765', 4921: 'V0251', 4922: '7265', 4923: '7030', 4924: '36523', 4925: '75481', 4926: '34292', 4927: '37230', 4928: 'E9398', 4929: '5641', 4930: '78604', 4931: '99678', 4932: '8603', 4933: '3090', 4934: '86401', 4935: '1140', 4936: '9618', 4937: '53251', 4938: 'E8694', 4939: '3594', 4940: '80710', 4941: '87361', 4942: '83942', 4943: '7761', 4944: '57441', 4945: '87351', 4946: 'E9192', 4947: '29410', 4948: '8670', 4949: 'V175', 4950: '79389', 4951: '2899', 4952: '7105', 4953: '9598', 4954: 'V6104', 4955: '7592', 4956: '6080', 4957: '75531', 4958: '9094', 4959: '71595', 4960: '59382', 4961: '30016', 4962: '0470', 4963: '30560', 4964: '7627', 4965: '36470', 4966: '57431', 4967: '99591', 4968: 'V561', 4969: '34701', 4970: '6248', 4971: '80341', 4972: '41011', 4973: '3360', 4974: '4263', 4975: '6179', 4976: '78039', 4977: '9035', 4978: '36812', 4979: '80505', 4980: '0338', 4981: 'V4382', 4982: '41091', 4983: 'V642', 4984: 'V1271', 4985: '2302', 4986: '9614', 4987: 'E9451', 4988: '78704', 4989: '76383', 4990: '48249', 4991: '7217', 4992: '07044', 4993: '32721', 4994: 'V052', 4995: '2558', 4996: '8449', 4997: '1974', 4998: '81404', 4999: '1987', 5000: '2118', 5001: 'V3000', 5002: '53130', 5003: '4472', 5004: '71237', 5005: '8821', 5006: '1888', 5007: '20402', 5008: '7515', 5009: 'V632', 5010: '43401', 5011: '71535', 5012: '75689', 5013: '52469', 5014: '86504', 5015: '85304', 5016: 'V610', 5017: '5609', 5018: '34201', 5019: '86101', 5020: 'E887', 5021: '4238', 5022: '80076', 5023: '0382', 5024: '75569', 5025: '7011', 5026: '8783', 5027: '74681', 5028: '72813', 5029: '01325', 5030: '82121', 5031: '85220', 5032: '2548', 5033: '80102', 5034: 'V040', 5035: '80606', 5036: '42491', 5037: '51889', 5038: '27541', 5039: '42822', 5040: '36275', 5041: '38582', 5042: '44381', 5043: '70704', 5044: '2308', 5045: '5368', 5046: '75317', 5047: '5172', 5048: '035', 5049: '88122', 5050: '80701', 5051: 'V1021', 5052: '7678', 5053: '2729', 5054: '7964', 5055: '75432', 5056: 'V8521', 5057: '73729', 5058: 'V6442', 5059: '42841', 5060: '36040', 5061: '00863', 5062: '0490', 5063: '2911', 5064: '7706', 5065: '7873', 5066: '81314', 5067: 'V442', 5068: '5739', 5069: '64934', 5070: '9658', 5071: '64682', 5072: '56489', 5073: '0090', 5074: '74361', 5075: '80061', 5076: 'V138', 5077: '7801', 5078: '71958', 5079: '68601', 5080: '66622', 5081: '60785', 5082: '76517', 5083: '2354', 5084: '94401', 5085: '30543', 5086: '6185', 5087: '53450', 5088: '78652', 5089: '35781', 5090: '72762', 5091: '87365', 5092: '43491', 5093: '71615', 5094: 'E8187', 5095: '38910', 5096: '76502', 5097: '3004', 5098: '34700', 5099: '538', 5100: '94425', 5101: '7593', 5102: 'V6107', 5103: '04102', 5104: '7907', 5105: 'V239', 5106: '65971', 5107: '9627', 5108: '80101', 5109: '8409', 5110: '49300', 5111: '3499', 5112: 'E9447', 5113: '87340', 5114: '4293', 5115: '71901', 5116: '57430', 5117: '5518', 5118: '8073', 5119: '73672', 5120: 'E918', 5121: 'E969', 5122: '6101', 5123: '9393', 5124: '80020', 5125: '78899', 5126: '37852', 5127: '64293', 5128: '43331', 5129: '7636', 5130: '8300', 5131: '88029', 5132: 'V0179', 5133: '78829', 5134: '80121', 5135: '4262', 5136: '7892', 5137: '73719', 5138: '80160', 5139: '75500', 5140: 'E9441', 5141: '7779', 5142: '5855', 5143: '57490', 5144: '1950', 5145: '25052', 5146: '3320', 5147: '90241', 5148: '34282', 5149: '20005', 5150: '9916', 5151: '59655', 5152: '2980', 5153: '6190', 5154: '53500', 5155: '1369', 5156: '1922', 5157: '5244', 5158: '5772', 5159: '9733', 5160: '80152', 5161: 'V8545', 5162: '1519', 5163: '22804', 5164: '2592', 5165: '7602', 5166: '78820', 5167: '6160', 5168: '99674', 5169: 'E8160', 5170: '4551', 5171: '56723', 5172: '9348', 5173: 'E8231', 5174: '9752', 5175: '71230', 5176: '75983', 5177: '77431', 5178: '69589', 5179: 'V4284', 5180: '6182', 5181: '5716', 5182: '0218', 5183: '3839', 5184: '51883', 5185: '30593', 5186: '2825', 5187: '37991', 5188: '8621', 5189: 'E9305', 5190: '9352', 5191: '2167', 5192: '25070', 5193: '8716', 5194: '5122', 5195: 'V6282', 5196: '9531', 5197: '65261', 5198: '20280', 5199: '59984', 5200: '33379', 5201: '1963', 5202: '7534', 5203: '5921', 5204: '56032', 5205: '55000', 5206: 'E9170', 5207: '85196', 5208: '52330', 5209: '71515', 5210: '80110', 5211: '27651', 5212: '6173', 5213: '71534', 5214: '99688', 5215: 'V1581', 5216: '78931', 5217: 'V6110', 5218: '29389', 5219: '80400', 5220: '9964', 5221: '28802', 5222: '3950', 5223: '5307', 5224: '81252', 5225: '6073', 5226: '81393', 5227: '29189', 5228: '36002', 5229: '90089', 5230: '53440', 5231: '47819', 5232: '78551', 5233: '0839', 5234: 'V311', 5235: '9722', 5236: 'E9361', 5237: '13102', 5238: 'V5410', 5239: '80607', 5240: '64891', 5241: '38200', 5242: '7703', 5243: '29621', 5244: '80707', 5245: '78724', 5246: '53210', 5247: '27400', 5248: '90253', 5249: 'E9310', 5250: '81603', 5251: '90003', 5252: '72939', 5253: 'E8581', 5254: '33811', 5255: '28809', 5256: 'V434', 5257: '9080', 5258: '72782', 5259: '99670', 5260: '52400', 5261: '71216', 5262: '86351', 5263: '72789', 5264: '0031', 5265: '73024', 5266: '4841', 5267: '37210', 5268: '2550', 5269: 'V292', 5270: '00869', 5271: 'V638', 5272: '95909', 5273: '20158', 5274: '78721', 5275: 'E8789', 5276: '2252', 5277: '4464', 5278: '01330', 5279: '2149', 5280: '4270', 5281: '99585', 5282: '4253', 5283: '96971', 5284: 'V1255', 5285: '83661', 5286: '37921', 5287: '66602', 5288: '78442', 5289: '9524', 5290: '9528', 5291: '34989', 5292: '42682', 5293: '5964', 5294: 'E9203', 5295: '25512', 5296: '86329', 5297: '78833', 5298: '3101', 5299: '63512', 5300: 'E8708', 5301: '9720', 5302: '75679', 5303: '096', 5304: '80131', 5305: '78834', 5306: '3573', 5307: '7820', 5308: '80080', 5309: '3588', 5310: '42821', 5311: 'E9580', 5312: '80629', 5313: '8710', 5314: '4149', 5315: '8021', 5316: '9578', 5317: 'V1351', 5318: '75479', 5319: '29574', 5320: '5963', 5321: '6824', 5322: '87359', 5323: 'E8132', 5324: '30002', 5325: '32082', 5326: 'E8555', 5327: '3093', 5328: 'V703', 5329: '37702', 5330: 'V142', 5331: '9019', 5332: '5290', 5333: '80239', 5334: '7051', 5335: '94232', 5336: '30500', 5337: '20080', 5338: '37886', 5339: 'V5416', 5340: '2638', 5341: '2180', 5342: '591', 5343: '80623', 5344: '6962', 5345: 'V4611', 5346: '243', 5347: '53540', 5348: '500', 5349: '81102', 5350: '04082', 5351: '80639', 5352: '45183', 5353: '52689', 5354: 'V1302', 5355: '0578', 5356: '7213', 5357: 'E9821', 5358: 'V4972', 5359: '4846', 5360: 'V1011', 5361: '5881', 5362: '1528', 5363: '78841', 5364: '0794', 5365: '81322', 5366: '81402', 5367: '5781', 5368: '78902', 5369: '3109', 5370: '74561', 5371: '38589', 5372: '8029', 5373: '5810', 5374: '20412', 5375: '30441', 5376: '4240', 5377: '6826', 5378: '2893', 5379: '29630', 5380: '4571', 5381: '8797', 5382: '76719', 5383: '78096', 5384: 'V166', 5385: '86344', 5386: '3501', 5387: '5539', 5388: '70722', 5389: 'V2651', 5390: '78722', 5391: '7237', 5392: '2370', 5393: '5990', 5394: '7385', 5395: '73730', 5396: '53390', 5397: '44022', 5398: '30480', 5399: '73382', 5400: '53640', 5401: '71102', 5402: 'V8535', 5403: '9340', 5404: '2126', 5405: '1643', 5406: '30120', 5407: '76406', 5408: 'V1091', 5409: '20510', 5410: '9596', 5411: '82139', 5412: '82002', 5413: 'V4363', 5414: '8072', 5415: '3349', 5416: 'E8508', 5417: 'E030', 5418: '81352', 5419: 'V443', 5420: '1973', 5421: '7587', 5422: 'E9194', 5423: '0362', 5424: 'E8843', 5425: '4588', 5426: '1579', 5427: '92721', 5428: '20040', 5429: '4267', 5430: '20500', 5431: '76498', 5432: 'E9338', 5433: '72741', 5434: '60789', 5435: '9249', 5436: 'V1052', 5437: '586', 5438: '70219', 5439: '33392', 5440: '3432', 5441: '86132', 5442: '3082', 5443: '60783', 5444: '05829', 5445: '4550', 5446: '7764', 5447: '80012', 5448: '30181', 5449: '36189', 5450: '8469', 5451: 'E0011', 5452: '83801', 5453: '9309', 5454: '45989', 5455: '7289', 5456: '7226', 5457: '20380', 5458: '9056', 5459: '0970', 5460: '70723', 5461: '1625', 5462: '44324', 5463: '29285', 5464: '1211', 5465: '29575', 5466: 'E9300', 5467: 'V1002', 5468: '71109', 5469: '7929', 5470: '6851', 5471: '75522', 5472: '8798', 5473: '2891', 5474: '5306', 5475: '53191', 5476: '29699', 5477: '4110', 5478: '60010', 5479: '7732', 5480: '99851', 5481: 'E0070', 5482: 'V271', 5483: '7104', 5484: '41081', 5485: '99529', 5486: '43381', 5487: '2809', 5488: '7841', 5489: '36512', 5490: '27803', 5491: '5754', 5492: 'V1652', 5493: 'V1552', 5494: 'V6142', 5495: '27952', 5496: '99731', 5497: '20033', 5498: '53111', 5499: 'E0039', 5500: '99749', 5501: '6019', 5502: '4254', 5503: '81354', 5504: '8280', 5505: '33821', 5506: '80421', 5507: 'E8839', 5508: '3368', 5509: '90222', 5510: '9099', 5511: '81513', 5512: '44323', 5513: '86389', 5514: '4533', 5515: '27731', 5516: '86404', 5517: '30550', 5518: '87402', 5519: '3558', 5520: '9114', 5521: 'V8811', 5522: '53341', 5523: 'E8002', 5524: '44029', 5525: '2325', 5526: '73026', 5527: '3371', 5528: 'V1643', 5529: 'E8156', 5530: 'V604', 5531: '30300', 5532: '53019', 5533: '73019', 5534: '2218', 5535: 'E8764', 5536: '80500', 5537: '94420', 5538: '41021', 5539: '7380', 5540: '7885', 5541: '83652', 5542: '7018', 5543: 'V183', 5544: '2561', 5545: '80221', 5546: '1737', 5547: 'E9589', 5548: '61804', 5549: '53141', 5550: '1498', 5551: '3581', 5552: 'V1588', 5553: '7520', 5554: '64244', 5555: '86810', 5556: '86352', 5557: '4802', 5558: '0971', 5559: '83805', 5560: '65641', 5561: 'V1365', 5562: '35971', 5563: '3249', 5564: '72401', 5565: '3014', 5566: 'V8523', 5567: '3336', 5568: '5995', 5569: '7990', 5570: 'V1085', 5571: '25200', 5572: 'V1046', 5573: 'E9654', 5574: '5239', 5575: '53511', 5576: 'V4975', 5577: 'E8501', 5578: '4719', 5579: '7775', 5580: '81504', 5581: '9229', 5582: '2931', 5583: '45341', 5584: '71970', 5585: '38842', 5586: '09152', 5587: 'V0262', 5588: '71233', 5589: '60883', 5590: '29043', 5591: '8671', 5592: '72252', 5593: '1716', 5594: '36544', 5595: '80176', 5596: 'V440', 5597: '36815', 5598: '57460', 5599: '45384', 5600: '79901', 5601: '5932', 5602: '5715', 5603: '2110', 5604: '9034', 5605: '76077', 5606: '9392', 5607: 'V502', 5608: 'E8710', 5609: '0543', 5610: '43820', 5611: 'V109', 5612: '3918', 5613: '23874', 5614: '1478', 5615: '7946', 5616: '4731', 5617: '80376', 5618: '85141', 5619: 'E9328', 5620: '7231', 5621: '28861', 5622: '74783', 5623: 'V0254', 5624: 'E8126', 5625: '5853', 5626: '72992', 5627: '5822', 5628: '7108', 5629: '63401', 5630: 'V1741', 5631: '72700', 5632: '7019', 5633: '135', 5634: '81321', 5635: '88101', 5636: '0312', 5637: 'E8704', 5638: '70720', 5639: '2104', 5640: '01300', 5641: 'V8542', 5642: '9610', 5643: '36570', 5644: '9116', 5645: '56882', 5646: '3671', 5647: '5070', 5648: '74687', 5649: '4732', 5650: '87373', 5651: '4611', 5652: '83660', 5653: '74442', 5654: '7069', 5655: '1580', 5656: '85103', 5657: '99653', 5658: '25083', 5659: '3449', 5660: '56969', 5661: 'V431', 5662: '9950', 5663: '6203', 5664: 'V4960', 5665: 'V552', 5666: '3220', 5667: '9839', 5668: 'E9357', 5669: '42731', 5670: '1551', 5671: 'V9089', 5672: '4718', 5673: '9551', 5674: '17342', 5675: '64231', 5676: '04101', 5677: '56942', 5678: '52809', 5679: '75320', 5680: '71238', 5681: '92301', 5682: '4408', 5683: '1975', 5684: '65414', 5685: '2300', 5686: 'E8496', 5687: 'E9178', 5688: 'V5413', 5689: '1723', 5690: '0416', 5691: 'E9505', 5692: '72706', 5693: '63592', 5694: 'V8709', 5695: 'E8138', 5696: '73390', 5697: '5804', 5698: '78099', 5699: '1514', 5700: 'E9410', 5701: 'V461', 5702: '5160', 5703: '92619', 5704: 'E8348', 5705: '80316', 5706: '64111', 5707: '2862', 5708: '79021', 5709: '71918', 5710: 'E9394', 5711: '7142', 5712: '49382', 5713: '75311', 5714: '6002', 5715: '2130', 5716: '20951', 5717: '4291', 5718: '78650', 5719: '25051', 5720: '73005', 5721: 'V4289', 5722: '85204', 5723: '5564', 5724: '96905', 5725: '1893', 5726: '03843', 5727: 'E8796', 5728: '30781', 5729: '72273', 5730: '43410', 5731: '9213', 5732: '1986', 5733: '85306', 5734: '8170', 5735: '8075', 5736: 'V4581', 5737: 'V672', 5738: '4413', 5739: '2123', 5740: '27700', 5741: '57149', 5742: '75322', 5743: '78937', 5744: '30400', 5745: '3348', 5746: '88019', 5747: '75026', 5748: '29690', 5749: '72704', 5750: '2848', 5751: '1503', 5752: '0272', 5753: '9691', 5754: '4590', 5755: '4619', 5756: '2571', 5757: '78900', 5758: 'E9500', 5759: '3411', 5760: '27950', 5761: '4421', 5762: 'V122', 5763: '64864', 5764: '5672', 5765: '4470', 5766: '267', 5767: '76401', 5768: '4257', 5769: '07998', 5770: '73309', 5771: '76515', 5772: '1918', 5773: '9584', 5774: '20301', 5775: '99631', 5776: '75314', 5777: '76524', 5778: 'V433', 5779: '20780', 5780: '34511', 5781: 'V6406', 5782: '4779', 5783: '5829', 5784: '9510', 5785: '3341', 5786: '2391', 5787: '75732', 5788: '08249', 5789: '80662', 5790: '20030', 5791: '86394', 5792: '2903', 5793: '4940', 5794: '17331', 5795: '5581', 5796: 'E8062', 5797: '1505', 5798: '9038', 5799: '3100', 5800: '00581', 5801: '9172', 5802: '71500', 5803: '73018', 5804: '28863', 5805: '6271', 5806: '76504', 5807: '77430', 5808: '1448', 5809: 'V151', 5810: '73393', 5811: '20930', 5812: '0980', 5813: '4829', 5814: '70707', 5815: '37430', 5816: '03841', 5817: '30113', 5818: '2531', 5819: '31289', 5820: 'E899', 5821: '64804', 5822: '9331', 5823: 'E8844', 5824: '73006', 5825: '7799', 5826: '6142', 5827: '41402', 5828: '9013', 5829: '34440', 5830: '99881', 5831: '3207', 5832: '29530', 5833: '9947', 5834: '3674', 5835: '2358', 5836: '56089', 5837: '3595', 5838: 'E8703', 5839: '5718', 5840: '6272', 5841: '36210', 5842: '53150', 5843: '9755', 5844: '75489', 5845: '45377', 5846: 'E9430', 5847: 'E8151', 5848: '80851', 5849: '7441', 5850: 'V161', 5851: '4475', 5852: '9579', 5853: 'E9370', 5854: '4959', 5855: '99652', 5856: '1104', 5857: '25033', 5858: 'V118', 5859: '84500', 5860: '2720', 5861: '35782', 5862: '2395', 5863: 'V4582', 5864: '86812', 5865: '932', 5866: '5678', 5867: '72251', 5868: '8248', 5869: '3089', 5870: '0417', 5871: '3393', 5872: '99771', 5873: '29512', 5874: '2356', 5875: '5130', 5876: '20600', 5877: '6164', 5878: '42612', 5879: '1838', 5880: '5370', 5881: '06642', 5882: '80006', 5883: '4400', 5884: '00804', 5885: 'V5843', 5886: '53641', 5887: '7500', 5888: '72191', 5889: '85301', 5890: '9571', 5891: '7796', 5892: 'V1261', 5893: '5730', 5894: '138', 5895: '04185', 5896: 'E9299', 5897: 'V1859', 5898: '90081', 5899: '8244', 5900: 'E8709', 5901: '72400', 5902: '71697', 5903: '1966', 5904: '73710', 5905: '0318', 5906: '0993', 5907: '1972', 5908: '53531', 5909: '4281', 5910: '4351', 5911: '9020', 5912: 'V4577', 5913: '1103', 5914: '78459', 5915: 'V694', 5916: '7948', 5917: '74510', 5918: '80146', 5919: '04109', 5920: '25023', 5921: '53089', 5922: '5981', 5923: '44382', 5924: '5118', 5925: '30301', 5926: '4555', 5927: '82110', 5928: '99831', 5929: '56200', 5930: '1400', 5931: '4375', 5932: 'E8127', 5933: '2739', 5934: '4556', 5935: '4352', 5936: '1883', 5937: 'V8812', 5938: '42823', 5939: '8860', 5940: '20511', 5941: '2386', 5942: '1727', 5943: '82525', 5944: '4572', 5945: '0778', 5946: '76516', 5947: '7295', 5948: '2281', 5949: 'E0139', 5950: '5552', 5951: '7135', 5952: '3555', 5953: 'E9334', 5954: '94524', 5955: '8601', 5956: 'V626', 5957: '8088', 5958: '2141', 5959: '6805', 5960: '5568', 5961: '45182', 5962: '431', 5963: '36441', 5964: 'V1047', 5965: '48230', 5966: '6258', 5967: '67014', 5968: 'V061', 5969: '7836', 5970: '99564', 5971: '95899', 5972: '35571', 5973: 'V141', 5974: '52406', 5975: '43830', 5976: '36900', 5977: 'E8200', 5978: '29381', 5979: '72982', 5980: '64761', 5981: '33819', 5982: '2820', 5983: '36003', 5984: '78843', 5985: '20891', 5986: '3484', 5987: '56039', 5988: 'E8609', 5989: '5088', 5990: '9529', 5991: '6118', 5992: '69551', 5993: 'V1251', 5994: '2397', 5995: '88002', 5996: '3804', 5997: '86340', 5998: '42511', 5999: '51851', 6000: '7809', 6001: '99661', 6002: '34570', 6003: '1725', 6004: '1800', 6005: '4830', 6006: 'E9673', 6007: '1398', 6008: 'E8603', 6009: '2864', 6010: 'V1659', 6011: '83900', 6012: '0398', 6013: '5224', 6014: '7464', 6015: '3708', 6016: '9832', 6017: '77088', 6018: 'E8188', 6019: 'V195', 6020: '80435', 6021: '2730', 6022: '30563', 6023: '69514', 6024: '3911', 6025: 'E9433', 6026: '8972', 6027: '86403', 6028: '64831', 6029: '4255', 6030: '9539', 6031: '1532', 6032: '2355', 6033: '64623', 6034: '8782', 6035: '71903', 6036: '37556', 6037: '64894', 6038: '78055', 6039: '1943', 6040: '36500', 6041: '05413', 6042: '65441', 6043: '35989', 6044: '40210', 6045: '6232', 6046: 'E8735', 6047: '78600', 6048: '99762', 6049: '0059', 6050: '5191', 6051: '34409', 6052: '51882', 6053: '82301', 6054: '3940', 6055: '33818', 6056: '74762', 6057: '3523', 6058: '4474', 6059: '7294', 6060: '6221', 6061: '7200', 6062: '70719', 6063: '3709', 6064: 'E9585', 6065: 'V549', 6066: '8799', 6067: '80026', 6068: '7847', 6069: 'E8795', 6070: '82522', 6071: '5163', 6072: '41410', 6073: '3452', 6074: '83100', 6075: '700', 6076: '3951', 6077: '78261', 6078: '4150', 6079: '6391', 6080: 'E915', 6081: '2773', 6082: '90140', 6083: 'V1082', 6084: '3300', 6085: '73711', 6086: '30440', 6087: '37882', 6088: '6983', 6089: '74511', 6090: '73722', 6091: '29562', 6092: '92309', 6093: '8442', 6094: '0796', 6095: '73028', 6096: '20278', 6097: '2115', 6098: '8629', 6099: '8056', 6100: 'E856', 6101: 'E8797', 6102: 'E8588', 6103: '7837', 6104: '25202', 6105: '77189', 6106: '5186', 6107: '71906', 6108: '1175', 6109: '7041', 6110: '1208', 6111: '7634', 6112: 'E8163', 6113: '07053', 6114: '3301', 6115: '80372', 6116: '69550', 6117: '71888', 6118: '2707', 6119: '36646', 6120: '20979', 6121: 'E9506', 6122: '9913', 6123: '59371', 6124: '1414', 6125: '71650', 6126: '5760', 6127: '9778', 6128: '7590', 6129: '7913', 6130: '8911', 6131: '46430', 6132: '2798', 6133: '185', 6134: '76384', 6135: '1461', 6136: '28804', 6137: '25001', 6138: '8055', 6139: 'E9211', 6140: '5680', 6141: 'V8543', 6142: '2884', 6143: 'E8809', 6144: '53550', 6145: '90142', 6146: '7450', 6147: '5800', 6148: '56971', 6149: '1735', 6150: 'E8495', 6151: '20201', 6152: 'V4572', 6153: 'V2541', 6154: '1251', 6155: '9617', 6156: '66604', 6157: '7611', 6158: '64263', 6159: 'V1071', 6160: '81221', 6161: '5374', 6162: 'E8748', 6163: '75010', 6164: '85121', 6165: '63412', 6166: '28243', 6167: '85180', 6168: 'V741', 6169: '29623', 6170: '90233', 6171: '7138', 6172: '70703', 6173: '87350', 6174: '514', 6175: 'V536', 6176: '1958', 6177: 'E9393', 6178: '85210', 6179: '2810', 6180: '95919', 6181: '59651', 6182: '311', 6183: '1745', 6184: '1100', 6185: '2443', 6186: 'E8585', 6187: '64913', 6188: '92420', 6189: '47821', 6190: 'E9620', 6191: '2725', 6192: '9721', 6193: '6963', 6194: '3591', 6195: '80841', 6196: '85401', 6197: '72664', 6198: '82310', 6199: '30493', 6200: '4582', 6201: '07810', 6202: 'E9579', 6203: '71699', 6204: '78901', 6205: 'V1005', 6206: '01194', 6207: '0949', 6208: '66561', 6209: '76506', 6210: '53012', 6211: '7742', 6212: '9623', 6213: '99800', 6214: '96569', 6215: '684', 6216: '9570', 6217: '5798', 6218: '44773', 6219: '32362', 6220: '4131', 6221: '80441', 6222: 'E9249', 6223: '5755', 6224: '7963', 6225: 'E882', 6226: 'E9530', 6227: '7817', 6228: '76519', 6229: '73025', 6230: '01123', 6231: '53560', 6232: 'V5049', 6233: 'E9340', 6234: '7771', 6235: '47875', 6236: '8065', 6237: '37887', 6238: '77018', 6239: 'V618', 6240: '78062', 6241: '86343', 6242: '5559', 6243: 'V8409', 6244: 'E8918', 6245: '0520', 6246: '52510', 6247: '3007', 6248: '53561', 6249: '1541', 6250: '30022', 6251: '74259', 6252: 'V1851', 6253: '7582', 6254: '24911', 6255: '2630', 6256: '8242', 6257: '66524', 6258: '5120', 6259: '79409', 6260: 'V1051', 6261: '7728', 6262: '2726', 6263: '28260', 6264: '8730', 6265: '87323', 6266: '60784', 6267: '82122', 6268: '48240', 6269: '7458', 6270: '80350', 6271: '6869', 6272: '7625', 6273: 'V289', 6274: '33709', 6275: '74601', 6276: '2888', 6277: 'E9504', 6278: '99642', 6279: '29382', 6280: '3485', 6281: '1569', 6282: '5848', 6283: '53782', 6284: '7868', 6285: '7789', 6286: '6959', 6287: '53250', 6288: '3941', 6289: '8751', 6290: 'V1089', 6291: 'E8587', 6292: '30562', 6293: '3590', 6294: '5550', 6295: '319', 6296: '9042', 6297: '86510', 6298: '09181', 6299: '82322', 6300: 'V6549', 6301: '78903', 6302: '55200', 6303: '57471', 6304: '20002', 6305: '58181', 6306: '5967', 6307: 'V625', 6308: 'V463', 6309: '42831', 6310: '55011', 6311: '585', 6312: '29590', 6313: '4589', 6314: '81323', 6315: '4738', 6316: '58089', 6317: '9190', 6318: '7038', 6319: '9949', 6320: '75440', 6321: '3862', 6322: '1430', 6323: '59970', 6324: '8872', 6325: '81110', 6326: '71591', 6327: '7823', 6328: '80174', 6329: '34280', 6330: '79551', 6331: '37955', 6332: '1951', 6333: 'V861', 6334: 'E9502', 6335: '73314', 6336: '34461', 6337: '8605', 6338: '36230', 6339: '81303', 6340: '85181', 6341: '86122', 6342: '80229', 6343: 'V198', 6344: '34591', 6345: '5942', 6346: '7542', 6347: '2111', 6348: '82022', 6349: '7955', 6350: '0392', 6351: '2984', 6352: '77982', 6353: '9753', 6354: '79431', 6355: '94332', 6356: '34200', 6357: '5832', 6358: '3441', 6359: 'V4561', 6360: 'V9039', 6361: '2912', 6362: '7887', 6363: '7210', 6364: '99684', 6365: '7766', 6366: '83907', 6367: '72632', 6368: '64914', 6369: 'E9363', 6370: '73329', 6371: '462', 6372: '9095', 6373: '9125', 6374: '79095', 6375: '95202', 6376: '6822', 6377: '58081', 6378: 'V1369', 6379: '35789', 6380: '8208', 6381: '78094', 6382: '1599', 6383: '83313', 6384: '33822', 6385: '7310', 6386: '07999', 6387: 'V503', 6388: '82390', 6389: '2164', 6390: '3548', 6391: '74860', 6392: '56481', 6393: '99687', 6394: '4578', 6395: '19889', 6396: '05329', 6397: '5081', 6398: '9067', 6399: '76512', 6400: '2353', 6401: '25011', 6402: '86612', 6403: '59373', 6404: '33829', 6405: '81103', 6406: '6940', 6407: '1769', 6408: '60499', 6409: '5185', 6410: '0491', 6411: '90225', 6412: '56202', 6413: '2702', 6414: '9831', 6415: '5909', 6416: '27502', 6417: '53789', 6418: '7758', 6419: '2367', 6420: '20220', 6421: '04590', 6422: '72409', 6423: '36106', 6424: 'V252', 6425: '7685', 6426: '84509', 6427: '7670', 6428: '40401', 6429: '496', 6430: '7262', 6431: '8249', 6432: '3313', 6433: '82302', 6434: '3240', 6435: 'E8538', 6436: '80112', 6437: '7721', 6438: '73016', 6439: '3488', 6440: '7149', 6441: '20961', 6442: '27489', 6443: '5845', 6444: '64681', 6445: '2119', 6446: 'E0071', 6447: 'E9348', 6448: '2594', 6449: '8260', 6450: '7514', 6451: '43301', 6452: '85173', 6453: '5078', 6454: '3556', 6455: '9800', 6456: '4171', 6457: '72885', 6458: 'V1584', 6459: 'V8741', 6460: '80227', 6461: 'E8706', 6462: '390', 6463: '37603', 6464: '7856', 6465: '80507', 6466: '2169', 6467: '2599', 6468: '07041', 6469: '2120', 6470: '9233', 6471: '43831', 6472: '78932', 6473: 'V111', 6474: '4568', 6475: '2764', 6476: '5856', 6477: '23331', 6478: 'V4365', 6479: '43330', 6480: '75530', 6481: '1732', 6482: '6150', 6483: 'E9496', 6484: '5809', 6485: '49121', 6486: '73810', 6487: '64621', 6488: '72889', 6489: '85300', 6490: '3732', 6491: 'E8260', 6492: '1702', 6493: '1598', 6494: '29181', 6495: '86603', 6496: '37189', 6497: '67204', 6498: '90210', 6499: '20501', 6500: '20208', 6501: '36221', 6502: '85202', 6503: '5812', 6504: '79955', 6505: 'V446', 6506: '94335', 6507: '5183', 6508: '73743', 6509: '2530', 6510: '36213', 6511: '85101', 6512: 'E9278', 6513: '72703', 6514: '8796', 6515: '80842', 6516: '5082', 6517: '25201', 6518: '0389', 6519: '64121', 6520: '3769', 6521: 'E8490', 6522: '6039', 6523: 'V5412', 6524: 'V4976', 6525: '8630', 6526: '65411', 6527: '80016', 6528: '90001', 6529: '79929', 6530: '78492', 6531: '53300', 6532: '7061', 6533: 'E0291', 6534: '8831', 6535: '193', 6536: '9773', 6537: '81240', 6538: '77087', 6539: '41082', 6540: '80002', 6541: '71815', 6542: 'E9331', 6543: 'E8504', 6544: '37520', 6545: '72990', 6546: 'V0950', 6547: '1543', 6548: '99809', 6549: '6000', 6550: '7624', 6551: '80119', 6552: '470', 6553: '07051', 6554: '40403', 6555: '53290', 6556: '85162', 6557: '11590', 6558: '83809', 6559: '2519', 6560: '85206', 6561: '9075', 6562: '9635', 6563: 'E8908', 6564: '4789', 6565: '8250', 6566: 'V1254', 6567: '7724', 6568: '2768', 6569: 'E9503', 6570: '9534', 6571: '99666', 6572: '7560', 6573: '81302', 6574: '34580', 6575: '59789', 6576: '7944', 6577: '1573', 6578: '4580', 6579: '9252', 6580: '3102', 6581: 'V5864', 6582: '1710', 6583: '5939', 6584: '82321', 6585: 'V4579', 6586: 'E9196', 6587: '05379', 6588: '29041', 6589: 'V011', 6590: '25020', 6591: '82300', 6592: '78832', 6593: '7895', 6594: '6191', 6595: '20210', 6596: '2153', 6597: '5571', 6598: '45389', 6599: '2468', 6600: 'E8242', 6601: '2949', 6602: 'E9550', 6603: '34442', 6604: '83209', 6605: 'V449', 6606: '86602', 6607: '80700', 6608: '1534', 6609: '5178', 6610: 'E959', 6611: 'V441', 6612: 'V5401', 6613: 'V5866', 6614: '92710', 6615: '4374', 6616: '05419', 6617: '7662', 6618: '72611', 6619: '99583', 6620: '7874', 6621: '2793', 6622: '7529', 6623: '2788', 6624: '78933', 6625: '5583', 6626: '33382', 6627: 'V4511', 6628: '2898', 6629: 'E9304', 6630: '38302', 6631: '63451', 6632: '99672', 6633: '9112', 6634: 'E9344', 6635: 'E9220', 6636: '28652', 6637: 'V469', 6638: 'V8489', 6639: '37635', 6640: '0912', 6641: '44489', 6642: 'E8210', 6643: '3340', 6644: '2533', 6645: '7088', 6646: '67451', 6647: '75502', 6648: '7132', 6649: '66911', 6650: '03812', 6651: '8704', 6652: '30393', 6653: '80336', 6654: '9559', 6655: 'E9443', 6656: '9308', 6657: '5902', 6658: '40491', 6659: 'V7219', 6660: '7618', 6661: '99667', 6662: 'E9678', 6663: '8708', 6664: '70702', 6665: '20530', 6666: '30470', 6667: '5579', 6668: '9211', 6669: '72973', 6670: '33183', 6671: '9828', 6672: '7098', 6673: '64883', 6674: '66934', 6675: '7321', 6676: '80022', 6677: '9086', 6678: '2799', 6679: 'E9248', 6680: '55091', 6681: '28851', 6682: '71985', 6683: 'E8541', 6684: '78559', 6685: 'V188', 6686: '24281', 6687: '99586', 6688: '6188', 6689: 'E8783', 6690: '5819', 6691: 'E9384', 6692: '1467', 6693: '9920', 6694: '37716', 6695: '32713', 6696: '76621', 6697: 'V0990', 6698: '20152', 6699: '37853', 6700: '30423', 6701: 'E8230', 6702: '3062', 6703: '86399', 6704: '99594', 6705: '9349', 6706: 'E8503', 6707: '1889', 6708: '9651', 6709: '1718', 6710: '53120', 6711: '68609', 6712: '20200', 6713: '3869', 6714: '5731', 6715: '52460', 6716: '5834', 6717: '53430', 6718: '0038', 6719: '27661', 6720: '2883', 6721: '80236', 6722: '9974', 6723: '7283', 6724: '80325', 6725: '5851', 6726: '99671', 6727: '53260', 6728: '64944', 6729: '5273', 6730: '72672', 6731: '75881', 6732: '1552', 6733: '5711', 6734: '6828', 6735: '45386', 6736: '52331', 6737: '34691', 6738: '74512', 6739: '4379', 6740: '55093', 6741: '7485', 6742: '0579', 6743: 'E8298', 6744: '1744', 6745: '570', 6746: '95208', 6747: '9530', 6748: '37239', 6749: '94128', 6750: '5690', 6751: '34120', 6752: '38619', 6753: '8970', 6754: 'E8902', 6755: '1530', 6756: 'E9397', 6757: '78720', 6758: '7509', 6759: '7774', 6760: '2520', 6761: '1890', 6762: 'V854', 6763: '64822', 6764: '80010', 6765: '0780', 6766: 'E9679', 6767: '55229', 6768: '6821', 6769: '56211', 6770: '9170', 6771: '81403', 6772: '11285', 6773: '99656', 6774: 'V435', 6775: '8850', 6776: '81343', 6777: '605', 6778: '37733', 6779: '8620', 6780: '3536', 6781: '5180', 6782: '1940', 6783: '81500', 6784: '3570', 6785: '95205', 6786: '7780', 6787: '8362', 6788: 'V0381', 6789: '80608', 6790: '7323', 6791: '99664', 6792: '30421', 6793: '38630', 6794: '0700', 6795: '74569', 6796: '5379', 6797: '55329', 6798: '29281', 6799: '73320', 6800: 'E8248', 6801: '20512', 6802: '87322', 6803: '52801', 6804: '49321', 6805: '85309', 6806: '7773', 6807: 'V1352', 6808: 'E8889', 6809: '0622', 6810: '81000', 6811: '3553', 6812: '57440', 6813: '81308', 6814: '90451', 6815: '3643', 6816: 'E9001', 6817: '6183', 6818: 'V660', 6819: '7470', 6820: '2740', 6821: '4266', 6822: '86330', 6823: 'V293', 6824: '8026', 6825: '20921', 6826: '88012', 6827: 'E8197', 6828: '76405', 6829: '41051', 6830: '74720', 6831: '260', 6832: '2940', 6833: '1518', 6834: '5724', 6835: 'V8537', 6836: 'V0739', 6837: '99647', 6838: 'E8181', 6839: '65101', 6840: '05311', 6841: '78461', 6842: '6970', 6843: '34291', 6844: '8910', 6845: '63572', 6846: 'E9429', 6847: '69513', 6848: '7813', 6849: '7569', 6850: '6961', 6851: '5929', 6852: '29592', 6853: '5961', 6854: '05889', 6855: '5309', 6856: '25510', 6857: '77082', 6858: '5200', 6859: '9130', 6860: '5110', 6861: '38906', 6862: '42742', 6863: '4232', 6864: 'V1083', 6865: '04183', 6866: '69581', 6867: '71690', 6868: '3749', 6869: '99604', 6870: '48281', 6871: '68111', 6872: 'E9019', 6873: 'E8042', 6874: '56986', 6875: '8973', 6876: '33372', 6877: '45387', 6878: '79510', 6879: 'E8217', 6880: '73097', 6881: '81307', 6882: '44621', 6883: '7744', 6884: '04186', 6885: '78001', 6886: '64884', 6887: '8470', 6888: '9072', 6889: '1624', 6890: '71105', 6891: '9951', 6892: 'E9346', 6893: '1844', 6894: '28800', 6895: '7422', 6896: '47831', 6897: '220', 6898: '4386', 6899: '42769', 6900: '64271', 6901: '1512', 6902: '95203', 6903: '88003', 6904: '81313', 6905: '53781', 6906: 'V4364', 6907: '1533', 6908: '7767', 6909: '01402', 6910: '45383', 6911: '75329', 6912: '75615', 6913: 'E0062', 6914: '340', 6915: '1149', 6916: '72871', 6917: '81341'}\n",
            "{'44289': 0, '0542': 1, '45350': 2, '81383': 3, '76408': 4, '74489': 5, '38905': 6, '07052': 7, 'E8768': 8, '74742': 9, '30491': 10, '20218': 11, '7013': 12, '37820': 13, '41404': 14, '8251': 15, '7859': 16, 'E9687': 17, '72743': 18, 'V580': 19, '73316': 20, '27906': 21, '78799': 22, '75265': 23, '4401': 24, '80182': 25, 'V153': 26, '25030': 27, '20963': 28, '5691': 29, '4440': 30, '8488': 31, '28859': 32, 'E8494': 33, '5272': 34, '9894': 35, '2769': 36, '20903': 37, '71589': 38, 'E8793': 39, '7459': 40, '7012': 41, '4370': 42, 'E8532': 43, '30989': 44, '3492': 45, '80420': 46, '4599': 47, '3221': 48, '3510': 49, '53570': 50, '92810': 51, '5523': 52, '36616': 53, '8371': 54, 'V058': 55, '1516': 56, '76070': 57, '77183': 58, '37741': 59, '3577': 60, '25541': 61, '53190': 62, '01896': 63, '64893': 64, '59010': 65, '4843': 66, 'V1204': 67, '9580': 68, '01504': 69, 'E9470': 70, 'V0259': 71, 'V155': 72, 'E8353': 73, '75261': 74, '1401': 75, '36283': 76, '1965': 77, '9040': 78, '36846': 79, '73002': 80, '99939': 81, '45371': 82, '42781': 83, '53370': 84, '20028': 85, 'V444': 86, '7843': 87, '99769': 88, 'V8745': 89, '64252': 90, '34481': 91, '71530': 92, '71593': 93, '2771': 94, 'V5873': 95, '430': 96, '3337': 97, '78451': 98, '0310': 99, '9065': 100, '7299': 101, 'V6441': 102, '6023': 103, '83920': 104, '83301': 105, '6159': 106, 'E9571': 107, '9054': 108, '7862': 109, '85229': 110, '85406': 111, '44589': 112, '56982': 113, 'V5831': 114, '734': 115, '1410': 116, '5233': 117, 'V420': 118, '1409': 119, '7350': 120, '29600': 121, '9581': 122, '34882': 123, 'E9307': 124, '20288': 125, '9535': 126, '23879': 127, '1913': 128, '20252': 129, '34541': 130, '45374': 131, '4404': 132, '81012': 133, '2156': 134, '86121': 135, '3410': 136, '76711': 137, '2821': 138, '7910': 139, '74912': 140, '2829': 141, '7100': 142, 'E9000': 143, '78961': 144, '25061': 145, 'E8841': 146, '6145': 147, 'V5301': 148, '2140': 149, 'V3101': 150, '30183': 151, '19881': 152, '20382': 153, '2882': 154, '0311': 155, '38600': 156, '3526': 157, '4439': 158, '07799': 159, '1542': 160, '83904': 161, '9553': 162, '6951': 163, '71155': 164, '7530': 165, '28246': 166, '99668': 167, 'V4586': 168, '64843': 169, '37999': 170, 'E8586': 171, '5100': 172, '45620': 173, '2334': 174, '80122': 175, '20070': 176, '5970': 177, '47820': 178, '3308': 179, '7384': 180, '7280': 181, '2250': 182, '2454': 183, '22802': 184, '86113': 185, '34981': 186, '75501': 187, '2449': 188, 'V421': 189, '82382': 190, '64862': 191, '31381': 192, '38900': 193, '2651': 194, 'E9411': 195, '44581': 196, '6938': 197, '2537': 198, '2752': 199, '7613': 200, '74910': 201, '81219': 202, 'V1559': 203, '41020': 204, '46410': 205, '1905': 206, '78061': 207, '0527': 208, '55092': 209, '47400': 210, 'E8700': 211, '78602': 212, '4772': 213, 'E8718': 214, '75470': 215, 'E8654': 216, '80123': 217, '47824': 218, 'V4459': 219, '2309': 220, '1361': 221, '20148': 222, '33510': 223, '13109': 224, '5993': 225, '36251': 226, '78830': 227, '82001': 228, '27652': 229, '71104': 230, 'E9425': 231, 'E8840': 232, 'V851': 233, '81601': 234, '30403': 235, '80032': 236, '38917': 237, '1108': 238, '71680': 239, '6950': 240, 'V192': 241, '87269': 242, '7810': 243, '7660': 244, '1623': 245, '7612': 246, 'E9354': 247, '2979': 248, 'E8624': 249, '7994': 250, 'E8851': 251, '9991': 252, '6926': 253, '7512': 254, '8875': 255, '9301': 256, '48241': 257, '4420': 258, '8363': 259, '80610': 260, '7080': 261, '95901': 262, '5890': 263, '37500': 264, '4871': 265, '04105': 266, '34202': 267, '75889': 268, '6802': 269, '38612': 270, '51634': 271, '24960': 272, '73349': 273, '71840': 274, '7701': 275, '9983': 276, '51901': 277, '99883': 278, '9462': 279, '3688': 280, '8190': 281, '7888': 282, '34670': 283, '5771': 284, '38871': 285, 'E9677': 286, '44771': 287, '81249': 288, '81259': 289, '78869': 290, '990': 291, '37612': 292, 'E8140': 293, '2375': 294, '58389': 295, '3154': 296, '66932': 297, '7048': 298, '7420': 299, 'E9351': 300, '71942': 301, 'E9175': 302, '9260': 303, '2469': 304, 'V5861': 305, '5589': 306, '7220': 307, '37530': 308, '37432': 309, '38000': 310, 'V174': 311, '20260': 312, '7700': 313, 'E9670': 314, '71531': 315, 'V173': 316, '83902': 317, 'E989': 318, '3649': 319, '53270': 320, '3550': 321, '45111': 322, 'V1819': 323, '3319': 324, '53510': 325, '6249': 326, '80001': 327, '04089': 328, '5931': 329, '6011': 330, 'V4589': 331, 'E8141': 332, '82380': 333, '81401': 334, '9047': 335, '5720': 336, '7245': 337, '78031': 338, '70521': 339, '3502': 340, 'E9379': 341, '2360': 342, '7937': 343, '481': 344, '53230': 345, '11519': 346, '64783': 347, '9996': 348, '7336': 349, 'E8613': 350, '86414': 351, 'V1079': 352, 'E8162': 353, '7234': 354, '37555': 355, '81202': 356, '29502': 357, 'E8161': 358, '8870': 359, '95209': 360, '41400': 361, '9050': 362, '74353': 363, '2763': 364, '1270': 365, '86814': 366, '85200': 367, '82342': 368, 'E8180': 369, '9198': 370, 'V065': 371, '9597': 372, 'V425': 373, 'E9805': 374, '7482': 375, '5511': 376, 'E9651': 377, '9104': 378, '6021': 379, '27951': 380, '42099': 381, '6143': 382, '85135': 383, '30749': 384, '37956': 385, '1612': 386, 'V4965': 387, '3580': 388, '1348': 389, '31539': 390, '08240': 391, 'V1241': 392, '75982': 393, '59654': 394, '86345': 395, '44032': 396, '73089': 397, '9010': 398, '5291': 399, '5722': 400, 'V554': 401, 'V1209': 402, 'E9421': 403, '20943': 404, '64811': 405, '29622': 406, '3361': 407, '94321': 408, '4928': 409, '42610': 410, '37993': 411, '81392': 412, 'E9018': 413, '62570': 414, '7538': 415, '2339': 416, '95892': 417, '27949': 418, '30303': 419, '2631': 420, '1508': 421, '71909': 422, 'E9229': 423, '6213': 424, '29634': 425, '20205': 426, '7945': 427, '73732': 428, '37632': 429, '0701': 430, '9851': 431, '7209': 432, '0790': 433, '6144': 434, '2860': 435, 'E9225': 436, '74721': 437, '66624': 438, '412': 439, 'V4573': 440, '72293': 441, '72671': 442, '4847': 443, '85256': 444, '72888': 445, '4801': 446, '85222': 447, 'E8740': 448, '79902': 449, '3091': 450, '80502': 451, 'V272': 452, '5824': 453, 'E9850': 454, '24210': 455, 'E9445': 456, '55012': 457, '36253': 458, '36234': 459, '4660': 460, '43321': 461, '1463': 462, '5839': 463, '9858': 464, '7094': 465, 'V062': 466, '9540': 467, '25081': 468, '75450': 469, '80430': 470, '7754': 471, '7759': 472, '36641': 473, '42613': 474, '5060': 475, '38014': 476, '3599': 477, '72690': 478, '6390': 479, '1363': 480, '0919': 481, '65671': 482, '71687': 483, '3203': 484, '56981': 485, '20023': 486, '5969': 487, '56961': 488, '2758': 489, '2555': 490, '5789': 491, '9953': 492, '4230': 493, '1101': 494, '74362': 495, '7511': 496, '55841': 497, '3729': 498, '64683': 499, 'E9353': 500, '30981': 501, '30523': 502, '1892': 503, 'V5869': 504, '76529': 505, '4411': 506, '30012': 507, '9074': 508, '317': 509, '86331': 510, '81201': 511, '7535': 512, '9052': 513, 'V6443': 514, '5258': 515, '30750': 516, '76076': 517, '3619': 518, '20198': 519, '75263': 520, '59589': 521, '73819': 522, '23876': 523, '30285': 524, '8082': 525, '2978': 526, '30481': 527, '81511': 528, '1418': 529, '11284': 530, '74710': 531, '9108': 532, 'E9382': 533, '5305': 534, '37922': 535, '5300': 536, '5278': 537, '9696': 538, '7466': 539, '90441': 540, '94519': 541, '7683': 542, '5950': 543, '5830': 544, '71844': 545, 'E8767': 546, '4019': 547, '63411': 548, '7756': 549, '7705': 550, '48231': 551, '55320': 552, '17332': 553, '58381': 554, '1177': 555, '38100': 556, '80360': 557, '90287': 558, '74363': 559, '01136': 560, '64801': 561, '90082': 562, '55321': 563, 'E0000': 564, 'E8123': 565, 'V024': 566, '85140': 567, '39899': 568, '7449': 569, '71222': 570, '2132': 571, '0400': 572, 'E8791': 573, 'E8211': 574, '49392': 575, '65821': 576, 'E9871': 577, '7212': 578, '7463': 579, '2689': 580, '72290': 581, '37005': 582, '7617': 583, '2142': 584, 'V152': 585, '5560': 586, '40310': 587, '1983': 588, '31401': 589, 'E8810': 590, 'V4576': 591, '53211': 592, '72972': 593, '80034': 594, '86610': 595, '44329': 596, '04182': 597, '70409': 598, '85215': 599, '7802': 600, 'E9854': 601, '1738': 602, 'E9204': 603, '6171': 604, '37272': 605, '44030': 606, '64203': 607, '86502': 608, '80144': 609, '57461': 610, '8364': 611, '4558': 612, '36801': 613, '53551': 614, '37992': 615, '6264': 616, 'E975': 617, '7355': 618, '85011': 619, '59011': 620, '86342': 621, '7768': 622, '81210': 623, 'V2501': 624, '7673': 625, '2830': 626, '81311': 627, '6162': 628, 'E0073': 629, '4808': 630, 'E8240': 631, '86349': 632, '78606': 633, '85224': 634, '43884': 635, '66401': 636, 'V581': 637, '77211': 638, '4786': 639, 'E9250': 640, '7292': 641, '51900': 642, '35800': 643, '67454': 644, '33384': 645, 'V403': 646, 'E970': 647, '01085': 648, '66921': 649, '5695': 650, '2312': 651, '1941': 652, '20078': 653, '80220': 654, '07070': 655, '2559': 656, '9089': 657, 'V0189': 658, '30015': 659, '36511': 660, '7661': 661, '82130': 662, '6251': 663, 'E927': 664, '73734': 665, '63421': 666, '1968': 667, '262': 668, '0478': 669, '9593': 670, '27650': 671, '76503': 672, '01193': 673, '4272': 674, '41000': 675, '9878': 676, '71516': 677, '3344': 678, '75310': 679, '475': 680, '78601': 681, '75312': 682, '3529': 683, '87202': 684, '6923': 685, '41403': 686, '5735': 687, '20920': 688, '38003': 689, '07819': 690, '5650': 691, 'E8849': 692, '5533': 693, 'E8251': 694, '82392': 695, 'E8145': 696, 'E0010': 697, '1561': 698, '3430': 699, '81231': 700, 'E8121': 701, '1961': 702, '86229': 703, '24901': 704, '7806': 705, 'E8881': 706, '8460': 707, '53521': 708, '29640': 709, 'V071': 710, '53261': 711, '64842': 712, '6809': 713, '3200': 714, '29900': 715, '75614': 716, '1885': 717, '6930': 718, '88001': 719, '8191': 720, '5693': 721, '0709': 722, '80315': 723, '0979': 724, '2122': 725, '80011': 726, '2732': 727, '28311': 728, '87343': 729, '2713': 730, 'V1020': 731, '37410': 732, '2879': 733, '3238': 734, '81613': 735, '71489': 736, '2781': 737, 'V553': 738, '3668': 739, '44282': 740, '1628': 741, '2929': 742, '6238': 743, '78605': 744, 'E8129': 745, '2822': 746, 'V020': 747, '57401': 748, '33391': 749, '3491': 750, '8873': 751, 'V078': 752, 'E8190': 753, '4296': 754, '9664': 755, '0846': 756, '9708': 757, '7626': 758, '41031': 759, '42518': 760, '75313': 761, '29564': 762, '36589': 763, 'E8853': 764, '80125': 765, '1731': 766, '78607': 767, '72887': 768, '99701': 769, '52563': 770, '3343': 771, '55090': 772, '2102': 773, '2330': 774, '58889': 775, '20590': 776, '99563': 777, '1125': 778, 'V1084': 779, 'E9100': 780, '7861': 781, '2154': 782, '9972': 783, '9962': 784, 'E8171': 785, 'E9674': 786, 'V1389': 787, '40591': 788, '1540': 789, '71891': 790, '61171': 791, '99582': 792, 'E8540': 793, '1610': 794, '8830': 795, '61689': 796, '4359': 797, 'E986': 798, 'V5041': 799, '8793': 800, '87341': 801, '2458': 802, '84200': 803, '78940': 804, '36552': 805, '8832': 806, '71831': 807, '38912': 808, '00861': 809, '4259': 810, '03844': 811, '76494': 812, '4294': 813, '4781': 814, '1123': 815, '73396': 816, '9189': 817, '74365': 818, '20957': 819, '9724': 820, '8822': 821, '3083': 822, '2865': 823, '8787': 824, '2362': 825, '76389': 826, '3419': 827, '4519': 828, '05479': 829, '75016': 830, '45375': 831, '7635': 832, '64823': 833, '6910': 834, 'E8584': 835, '29681': 836, '20243': 837, '8602': 838, 'E8880': 839, '7488': 840, 'E9803': 841, '80425': 842, '78630': 843, '2388': 844, '43883': 845, '7762': 846, '47874': 847, '38023': 848, '4200': 849, 'V180': 850, '5644': 851, '66021': 852, '6038': 853, '83109': 854, '1622': 855, '7078': 856, 'E8548': 857, '44284': 858, '30592': 859, '27789': 860, '8076': 861, '58281': 862, '1501': 863, '37611': 864, '2873': 865, 'V653': 866, '8785': 867, '8902': 868, '44021': 869, '3960': 870, 'V568': 871, '99802': 872, '3373': 873, '69510': 874, '9690': 875, '8360': 876, 'V8524': 877, '64313': 878, '7388': 879, '63502': 880, '85239': 881, '72882': 882, '4412': 883, '485': 884, '83921': 885, '36289': 886, '25031': 887, '17322': 888, '40200': 889, '66551': 890, '7840': 891, '4481': 892, 'E954': 893, '66574': 894, '3798': 895, '5219': 896, '75611': 897, '8461': 898, '0329': 899, '9081': 900, '20410': 901, '67414': 902, '7630': 903, '7871': 904, '85201': 905, '67432': 906, '47870': 907, '37482': 908, '99679': 909, '81003': 910, '80432': 911, '67004': 912, '0041': 913, '30541': 914, '4920': 915, '58289': 916, '4260': 917, '5600': 918, '92820': 919, '7510': 920, '7608': 921, '9032': 922, '42689': 923, '76526': 924, '8790': 925, '34400': 926, '71926': 927, '45821': 928, '81610': 929, '7633': 930, '5238': 931, '0545': 932, '34510': 933, '28244': 934, '25063': 935, '7103': 936, '80223': 937, '2419': 938, '3910': 939, '37452': 940, '1105': 941, '5994': 942, '3698': 943, '94504': 944, 'V1006': 945, '9140': 946, '5521': 947, '43853': 948, '80232': 949, '6920': 950, '71895': 951, '35922': 952, '85190': 953, '80426': 954, '7601': 955, '7296': 956, '72610': 957, '60091': 958, '9039': 959, '2518': 960, '86415': 961, '1109': 962, '78659': 963, '4532': 964, '78839': 965, '7776': 966, '6072': 967, '75739': 968, 'E9240': 969, '20401': 970, '5723': 971, 'V1381': 972, 'E8556': 973, '2254': 974, 'V1042': 975, '80165': 976, '78499': 977, '80225': 978, '71856': 979, '72763': 980, 'V620': 981, '72660': 982, '79381': 983, '53010': 984, '0380': 985, '99601': 986, 'V1259': 987, '1455': 988, '9090': 989, '8960': 990, 'V4281': 991, '81250': 992, '7942': 993, 'E8554': 994, '2381': 995, '53110': 996, '8024': 997, '75557': 998, '72761': 999, '9219': 1000, '71887': 1001, '78838': 1002, '67154': 1003, '7421': 1004, '42830': 1005, '20974': 1006, '3540': 1007, '48801': 1008, '36565': 1009, '79552': 1010, '60490': 1011, '82021': 1012, '8820': 1013, 'E8133': 1014, '7912': 1015, '4878': 1016, '20150': 1017, 'V3001': 1018, '07022': 1019, '33989': 1020, 'V0389': 1021, '2693': 1022, 'V1053': 1023, 'E9303': 1024, '71509': 1025, '56941': 1026, '8977': 1027, '5234': 1028, '65951': 1029, '23770': 1030, '8703': 1031, '95204': 1032, '9694': 1033, 'E8192': 1034, '99562': 1035, '9001': 1036, '7429': 1037, '38010': 1038, '77212': 1039, '44031': 1040, '9152': 1041, '24280': 1042, '77439': 1043, '94325': 1044, '38420': 1045, '7402': 1046, '74560': 1047, '20281': 1048, '0270': 1049, '80106': 1050, '986': 1051, '27903': 1052, '63380': 1053, '3051': 1054, 'E8257': 1055, '51884': 1056, '7260': 1057, '7615': 1058, '98989': 1059, '81301': 1060, '71949': 1061, '3578': 1062, '39891': 1063, '80504': 1064, 'E8110': 1065, '99581': 1066, '72619': 1067, '71784': 1068, '56401': 1069, '5173': 1070, '9243': 1071, '75321': 1072, '9517': 1073, '5888': 1074, '55010': 1075, '56213': 1076, '8748': 1077, '80136': 1078, 'E8318': 1079, '51911': 1080, '393': 1081, '81254': 1082, '72886': 1083, '99812': 1084, '4139': 1085, '37300': 1086, '87200': 1087, '33182': 1088, 'E0032': 1089, 'E9572': 1090, '85316': 1091, '9031': 1092, '86819': 1093, 'E8405': 1094, '30250': 1095, '71855': 1096, '80602': 1097, 'V1072': 1098, '5880': 1099, '80501': 1100, '7825': 1101, '80704': 1102, '3480': 1103, '81200': 1104, 'E9559': 1105, '436': 1106, 'E9352': 1107, '40311': 1108, '64844': 1109, '51283': 1110, '6031': 1111, '64781': 1112, '4910': 1113, 'V171': 1114, '45342': 1115, '5759': 1116, '73315': 1117, '67002': 1118, '0391': 1119, '1714': 1120, '1603': 1121, '3694': 1122, '19882': 1123, '2750': 1124, 'E9401': 1125, '29654': 1126, '33385': 1127, '502': 1128, '7628': 1129, '13101': 1130, '4416': 1131, '3942': 1132, '59080': 1133, '43400': 1134, '6254': 1135, '78906': 1136, '9684': 1137, 'V4502': 1138, '2512': 1139, '7791': 1140, '81600': 1141, '2724': 1142, 'E9800': 1143, '1420': 1144, '3483': 1145, '7864': 1146, '66942': 1147, 'V1061': 1148, 'E9688': 1149, 'E9412': 1150, '1370': 1151, '30570': 1152, '81241': 1153, '8690': 1154, 'E8859': 1155, '5401': 1156, '24980': 1157, '01304': 1158, '5764': 1159, '80025': 1160, '9942': 1161, '2698': 1162, 'V6149': 1163, 'V1202': 1164, 'V1000': 1165, '20722': 1166, '80626': 1167, 'V182': 1168, '1121': 1169, '99569': 1170, '0049': 1171, '566': 1172, '3060': 1173, '4280': 1174, 'V6103': 1175, 'E8241': 1176, '3888': 1177, '3152': 1178, '80621': 1179, '29040': 1180, '37900': 1181, '36847': 1182, '62210': 1183, '8246': 1184, 'V860': 1185, 'E9383': 1186, '59001': 1187, '80151': 1188, '74684': 1189, '55221': 1190, '6178': 1191, '64881': 1192, '5699': 1193, 'V230': 1194, '82003': 1195, '28959': 1196, '3180': 1197, '73007': 1198, '4279': 1199, '1458': 1200, 'E8762': 1201, '7824': 1202, '42979': 1203, '80105': 1204, '28241': 1205, 'E8493': 1206, '4290': 1207, '1110': 1208, '34403': 1209, '6012': 1210, '7461': 1211, '9587': 1212, '6208': 1213, '5768': 1214, '79989': 1215, '2875': 1216, 'V5411': 1217, 'E9399': 1218, 'E8744': 1219, 'E9463': 1220, '9697': 1221, '80230': 1222, '5779': 1223, 'V510': 1224, '99989': 1225, '78051': 1226, '9041': 1227, '7140': 1228, '5161': 1229, '3314': 1230, '37689': 1231, '6827': 1232, '72691': 1233, '7708': 1234, '6110': 1235, '9713': 1236, '3879': 1237, '78059': 1238, '9754': 1239, '4809': 1240, '01205': 1241, '0880': 1242, '94423': 1243, '42842': 1244, '7109': 1245, 'E9358': 1246, '72292': 1247, '29012': 1248, '66811': 1249, '73609': 1250, '6255': 1251, '6829': 1252, '85102': 1253, '80170': 1254, 'V1586': 1255, '20502': 1256, '7891': 1257, '1733': 1258, 'E9588': 1259, '8220': 1260, '5181': 1261, '20312': 1262, '87360': 1263, '7354': 1264, '20282': 1265, '6200': 1266, '8505': 1267, '4144': 1268, '9890': 1269, '5651': 1270, 'V1749': 1271, '4010': 1272, '4512': 1273, '2827': 1274, 'V608': 1275, '94421': 1276, '90002': 1277, '3481': 1278, '9802': 1279, '9181': 1280, '4160': 1281, '79500': 1282, '99982': 1283, '9893': 1284, 'V1641': 1285, '34939': 1286, '78729': 1287, '20018': 1288, '72272': 1289, '3489': 1290, '78060': 1291, 'V4981': 1292, '75251': 1293, '37140': 1294, '86339': 1295, '76521': 1296, '17372': 1297, '7102': 1298, '51909': 1299, '2333': 1300, '79311': 1301, '7687': 1302, '64784': 1303, '95206': 1304, '7248': 1305, 'E8742': 1306, '65701': 1307, '99644': 1308, '2662': 1309, '5821': 1310, '9523': 1311, '42090': 1312, '86800': 1313, '20480': 1314, '9631': 1315, 'V1001': 1316, '45189': 1317, 'E9671': 1318, '57421': 1319, '2890': 1320, '2849': 1321, '7786': 1322, '43850': 1323, '3003': 1324, '33390': 1325, '37800': 1326, '34881': 1327, 'V066': 1328, 'V667': 1329, '44422': 1330, '2731': 1331, 'V4451': 1332, '1468': 1333, '326': 1334, '92310': 1335, '44100': 1336, '99639': 1337, '74193': 1338, '81220': 1339, '2513': 1340, 'V5811': 1341, '30924': 1342, '1611': 1343, '5601': 1344, '4419': 1345, '76493': 1346, '5989': 1347, '9122': 1348, '37501': 1349, '07071': 1350, '3629': 1351, '40492': 1352, '67153': 1353, '8221': 1354, '2738': 1355, '75520': 1356, '2897': 1357, '3509': 1358, '75219': 1359, 'V1043': 1360, '53520': 1361, 'V1059': 1362, '9982': 1363, '9619': 1364, '2376': 1365, '7901': 1366, '53649': 1367, '75616': 1368, 'E9888': 1369, 'E8800': 1370, '27401': 1371, '74349': 1372, '4820': 1373, '27503': 1374, 'V1004': 1375, '20971': 1376, '17341': 1377, '05312': 1378, 'V1012': 1379, '0414': 1380, '2390': 1381, 'E9313': 1382, '03810': 1383, '8780': 1384, '59969': 1385, '65421': 1386, '5109': 1387, '36859': 1388, '80326': 1389, '5528': 1390, '76522': 1391, 'V172': 1392, '95911': 1393, '7428': 1394, '83305': 1395, '78891': 1396, '59972': 1397, '90182': 1398, '70583': 1399, '81100': 1400, '7273': 1401, '9100': 1402, '75526': 1403, '37749': 1404, '2766': 1405, '64841': 1406, '80150': 1407, '87261': 1408, '80506': 1409, '5302': 1410, '4473': 1411, '66541': 1412, '2948': 1413, '56409': 1414, '6205': 1415, '07811': 1416, '1119': 1417, '2777': 1418, '72665': 1419, '25082': 1420, '6979': 1421, '27501': 1422, '29614': 1423, '40391': 1424, '7718': 1425, '7702': 1426, '61800': 1427, '0085': 1428, '55121': 1429, '2144': 1430, 'V113': 1431, '72883': 1432, '20203': 1433, '3963': 1434, '81230': 1435, 'E8705': 1436, '1307': 1437, '60886': 1438, '67404': 1439, '07033': 1440, '9642': 1441, 'E9222': 1442, '40511': 1443, 'V1301': 1444, '43814': 1445, '46450': 1446, '61650': 1447, 'E9600': 1448, '1562': 1449, '261': 1450, '7788': 1451, '75560': 1452, '25062': 1453, '49390': 1454, '25041': 1455, 'E9200': 1456, '80030': 1457, '38321': 1458, '76514': 1459, 'V291': 1460, '71107': 1461, 'V430': 1462, '99641': 1463, '04112': 1464, '53200': 1465, '87374': 1466, '28733': 1467, '99739': 1468, 'E8311': 1469, 'V3100': 1470, '75529': 1471, '5920': 1472, '3363': 1473, '01164': 1474, '72402': 1475, '67412': 1476, '83309': 1477, '7798': 1478, 'E9109': 1479, '29544': 1480, '46431': 1481, '4389': 1482, '1869': 1483, '7906': 1484, '72991': 1485, '00841': 1486, '4822': 1487, '81409': 1488, '34571': 1489, '7822': 1490, '30590': 1491, '90251': 1492, '53084': 1493, '36207': 1494, '73399': 1495, '01803': 1496, '74100': 1497, '41181': 1498, '8795': 1499, '0479': 1500, '73381': 1501, '3530': 1502, '74103': 1503, '71947': 1504, '2521': 1505, '85211': 1506, '2127': 1507, '56081': 1508, '5371': 1509, '71917': 1510, '40211': 1511, '47412': 1512, '8922': 1513, '78842': 1514, '56738': 1515, '9994': 1516, '2767': 1517, '5187': 1518, '7242': 1519, '80071': 1520, '7522': 1521, '72884': 1522, 'E9424': 1523, 'V8544': 1524, '78701': 1525, '7092': 1526, '88102': 1527, '2410': 1528, '68110': 1529, '99677': 1530, 'E8528': 1531, '9043': 1532, '4376': 1533, '72190': 1534, '5164': 1535, '94204': 1536, '99567': 1537, '87342': 1538, '73302': 1539, '92232': 1540, '7333': 1541, '64673': 1542, '4559': 1543, '6868': 1544, '36001': 1545, '25040': 1546, '43490': 1547, '41405': 1548, '1179': 1549, '5960': 1550, '74101': 1551, '4749': 1552, '40390': 1553, '5753': 1554, 'E8233': 1555, '4178': 1556, '25053': 1557, '8058': 1558, '71865': 1559, '60820': 1560, '99649': 1561, '9110': 1562, '6119': 1563, '80332': 1564, '25012': 1565, '3009': 1566, '64133': 1567, '9670': 1568, '38022': 1569, '27709': 1570, '7778': 1571, '86813': 1572, '77213': 1573, '20283': 1574, '85205': 1575, '7689': 1576, '9693': 1577, '2114': 1578, '81010': 1579, 'E9343': 1580, '5570': 1581, '3542': 1582, '85125': 1583, '87201': 1584, '9809': 1585, '2650': 1586, 'E8120': 1587, '25002': 1588, '1535': 1589, '72679': 1590, '71941': 1591, '7536': 1592, '92800': 1593, '3201': 1594, '42292': 1595, '78194': 1596, '60011': 1597, '71666': 1598, '85402': 1599, '9070': 1600, 'V652': 1601, '6009': 1602, '9767': 1603, '2840': 1604, '1891': 1605, '2441': 1606, 'V0382': 1607, 'E8227': 1608, '7830': 1609, '30460': 1610, '83905': 1611, 'E916': 1612, '46451': 1613, '4170': 1614, 'E8582': 1615, '53400': 1616, '81418': 1617, '85245': 1618, '8028': 1619, '1106': 1620, '38915': 1621, '67333': 1622, 'E8131': 1623, '42611': 1624, '99673': 1625, '4289': 1626, '73023': 1627, '2881': 1628, '36510': 1629, '37214': 1630, '31234': 1631, '0529': 1632, '36201': 1633, 'E9342': 1634, 'V1009': 1635, '1952': 1636, '66531': 1637, '0498': 1638, '3575': 1639, '3010': 1640, '0860': 1641, '5763': 1642, '37943': 1643, '80142': 1644, '53081': 1645, 'E9306': 1646, '1734': 1647, '80130': 1648, '6198': 1649, '69012': 1650, '6260': 1651, '71905': 1652, '40301': 1653, '78064': 1654, '92421': 1655, '04189': 1656, '6924': 1657, '28862': 1658, '37730': 1659, '1760': 1660, '28262': 1661, '7916': 1662, '2148': 1663, '37313': 1664, '5226': 1665, '53220': 1666, '66822': 1667, 'V5881': 1668, '34590': 1669, '1990': 1670, '2440': 1671, '51189': 1672, '78079': 1673, '82520': 1674, '8439': 1675, '2396': 1676, 'E9422': 1677, '49320': 1678, '94534': 1679, 'E8189': 1680, 'V0980': 1681, '1912': 1682, 'V4512': 1683, '07031': 1684, '43810': 1685, '64274': 1686, '99676': 1687, 'E8641': 1688, '75981': 1689, 'V1642': 1690, '3438': 1691, '56781': 1692, 'E848': 1693, '57450': 1694, '56944': 1695, '43320': 1696, '82312': 1697, '35921': 1698, '80031': 1699, '67484': 1700, 'E8142': 1701, '99669': 1702, '76382': 1703, '58189': 1704, '86232': 1705, '0859': 1706, '77016': 1707, '03289': 1708, '80601': 1709, '20192': 1710, '81243': 1711, '78723': 1712, '20206': 1713, '73741': 1714, '74740': 1715, '80843': 1716, 'E9289': 1717, 'E9554': 1718, '2113': 1719, '80009': 1720, '27411': 1721, '7919': 1722, '80616': 1723, '28411': 1724, '2392': 1725, '20925': 1726, '56962': 1727, '29573': 1728, '59000': 1729, '25043': 1730, '9556': 1731, '29604': 1732, '37855': 1733, '99659': 1734, 'E0190': 1735, '2872': 1736, '1917': 1737, 'V5812': 1738, 'V1007': 1739, '80306': 1740, '92401': 1741, 'V8801': 1742, '4011': 1743, 'V1869': 1744, '71592': 1745, '36250': 1746, '71103': 1747, '77981': 1748, 'V3401': 1749, '24990': 1750, '96903': 1751, '20300': 1752, '7335': 1753, '8786': 1754, '96502': 1755, '6944': 1756, '6982': 1757, '78603': 1758, '38860': 1759, '80075': 1760, '3315': 1761, '8438': 1762, '7842': 1763, '74686': 1764, '5169': 1765, '6151': 1766, '34620': 1767, 'E9238': 1768, '4549': 1769, '75613': 1770, '2861': 1771, '9992': 1772, '5373': 1773, 'V1049': 1774, '80113': 1775, '7571': 1776, '78071': 1777, '7455': 1778, '23771': 1779, 'E9179': 1780, '2762': 1781, 'V602': 1782, 'E9347': 1783, '74922': 1784, '03811': 1785, '20003': 1786, '66944': 1787, '4423': 1788, 'E9314': 1789, 'V4509': 1790, '7575': 1791, '2101': 1792, '5361': 1793, '3897': 1794, '75989': 1795, '44024': 1796, '7755': 1797, '2652': 1798, '63522': 1799, '4476': 1800, '1808': 1801, '99683': 1802, '2380': 1803, '40400': 1804, '5647': 1805, '73015': 1806, '75482': 1807, '1449': 1808, '5854': 1809, '81509': 1810, '5080': 1811, 'E9455': 1812, '2751': 1813, '6172': 1814, '1602': 1815, '64264': 1816, '7581': 1817, '75839': 1818, 'E9193': 1819, 'E8786': 1820, 'V08': 1821, '56201': 1822, 'E9325': 1823, '64214': 1824, '56731': 1825, '30540': 1826, '32723': 1827, 'E963': 1828, '83979': 1829, '45352': 1830, '20024': 1831, 'V290': 1832, '1969': 1833, '5279': 1834, '86239': 1835, 'V812': 1836, '52462': 1837, '35579': 1838, '80324': 1839, '936': 1840, '36100': 1841, '0931': 1842, '71537': 1843, '64671': 1844, '71198': 1845, '38021': 1846, 'E9319': 1847, '28981': 1848, '1945': 1849, '3071': 1850, '9605': 1851, 'V0981': 1852, '1730': 1853, '73605': 1854, '4548': 1855, '20020': 1856, '30490': 1857, 'V708': 1858, '47833': 1859, 'E8314': 1860, 'E9392': 1861, '42653': 1862, '5551': 1863, '5111': 1864, '99732': 1865, '1453': 1866, '1489': 1867, '5225': 1868, '7028': 1869, 'E9324': 1870, '42989': 1871, '0940': 1872, '74782': 1873, '9171': 1874, '11289': 1875, '48284': 1876, '27788': 1877, '90450': 1878, '99645': 1879, '80164': 1880, '66941': 1881, '84512': 1882, '36012': 1883, '1761': 1884, '42832': 1885, '9055': 1886, '42983': 1887, '30551': 1888, '1977': 1889, 'V1589': 1890, '5303': 1891, '74902': 1892, '2553': 1893, '38830': 1894, '2191': 1895, '75460': 1896, '38611': 1897, '7686': 1898, 'E8850': 1899, '30009': 1900, '3384': 1901, 'V1252': 1902, '9083': 1903, '5831': 1904, 'E8781': 1905, '1743': 1906, 'E857': 1907, '70706': 1908, '99593': 1909, '79589': 1910, '4220': 1911, '41061': 1912, '7783': 1913, 'V5842': 1914, '74732': 1915, '85146': 1916, '9552': 1917, 'V1062': 1918, '0415': 1919, '7243': 1920, '80859': 1921, 'E0031': 1922, '92410': 1923, '01186': 1924, '85105': 1925, '73340': 1926, '7244': 1927, 'V202': 1928, '28522': 1929, '47832': 1930, '36206': 1931, '74429': 1932, '3612': 1933, 'V4966': 1934, '80412': 1935, '37274': 1936, '463': 1937, '3434': 1938, '2121': 1939, '32727': 1940, '85241': 1941, '486': 1942, 'E8589': 1943, '71996': 1944, '51852': 1945, '6806': 1946, '28952': 1947, 'E9682': 1948, '64903': 1949, 'E8860': 1950, '82009': 1951, 'V558': 1952, '4618': 1953, '20969': 1954, 'V053': 1955, 'E9335': 1956, '75169': 1957, '3898': 1958, 'V1060': 1959, '34600': 1960, '4460': 1961, '1231': 1962, '40493': 1963, '04104': 1964, '81109': 1965, '7358': 1966, '60889': 1967, '4821': 1968, '71946': 1969, 'E8714': 1970, '43889': 1971, 'E9262': 1972, '4539': 1973, '34489': 1974, '9330': 1975, 'E8219': 1976, '7313': 1977, 'E9010': 1978, '7424': 1979, '41092': 1980, '34982': 1981, '86513': 1982, '6931': 1983, 'V123': 1984, '78003': 1985, '52512': 1986, '9592': 1987, '0074': 1988, '7821': 1989, '80109': 1990, '33722': 1991, '71430': 1992, '9222': 1993, '95891': 1994, '2150': 1995, 'E9309': 1996, '4770': 1997, '86611': 1998, '86412': 1999, '37775': 2000, '66634': 2001, '226': 2002, '2818': 2003, 'E9108': 2004, '33912': 2005, 'E8845': 2006, '5935': 2007, '80718': 2008, '6209': 2009, '4542': 2010, '74861': 2011, '57512': 2012, 'E9538': 2013, '81612': 2014, '1984': 2015, '56782': 2016, '9779': 2017, '4592': 2018, '41512': 2019, '85216': 2020, '80503': 2021, '65961': 2022, '59389': 2023, '7672': 2024, '95219': 2025, '8240': 2026, '2352': 2027, '74749': 2028, 'V1509': 2029, '30473': 2030, '35981': 2031, '59800': 2032, '28529': 2033, 'E8152': 2034, '65953': 2035, '8054': 2036, '2165': 2037, '99682': 2038, 'E8782': 2039, '04111': 2040, '29212': 2041, 'V4361': 2042, '8691': 2043, '81407': 2044, '587': 2045, '86613': 2046, '7716': 2047, '20011': 2048, '29520': 2049, '74682': 2050, '87412': 2051, '8509': 2052, '1953': 2053, '65423': 2054, '73012': 2055, '6989': 2056, '78821': 2057, '99811': 2058, 'V167': 2059, '88023': 2060, '82030': 2061, '80196': 2062, '41012': 2063, '9557': 2064, '5562': 2065, '00843': 2066, '25000': 2067, '9712': 2068, 'E8788': 2069, '30502': 2070, '80015': 2071, '27669': 2072, '75610': 2073, '4441': 2074, '5128': 2075, '3442': 2076, '92303': 2077, '30302': 2078, '53241': 2079, '05881': 2080, 'V029': 2081, '29980': 2082, '8361': 2083, '82332': 2084, '5531': 2085, '7712': 2086, 'E8846': 2087, 'E9081': 2088, 'E9011': 2089, '0521': 2090, '8920': 2091, '71990': 2092, '29653': 2093, '7707': 2094, '80320': 2095, '9029': 2096, '2116': 2097, '6018': 2098, '24290': 2099, 'E9317': 2100, '63311': 2101, '82539': 2102, '80085': 2103, '1749': 2104, '85225': 2105, '57140': 2106, '7921': 2107, '65651': 2108, '8400': 2109, 'V5862': 2110, '3552': 2111, '2125': 2112, '67012': 2113, '9160': 2114, '05313': 2115, '2329': 2116, '37182': 2117, '66411': 2118, '2230': 2119, 'V489': 2120, '62402': 2121, '5671': 2122, '8411': 2123, '67323': 2124, '7320': 2125, '53100': 2126, '7503': 2127, '71597': 2128, '27730': 2129, '1619': 2130, '1122': 2131, '77089': 2132, '38861': 2133, '71239': 2134, '27941': 2135, '74781': 2136, '83961': 2137, 'V5417': 2138, '7731': 2139, 'V1501': 2140, '3212': 2141, '2331': 2142, '30392': 2143, '44629': 2144, '5071': 2145, '4275': 2146, 'V583': 2147, '5719': 2148, '77083': 2149, '3524': 2150, '57511': 2151, 'E8854': 2152, 'V1649': 2153, '70715': 2154, '3181': 2155, '28983': 2156, '5192': 2157, '8074': 2158, '77589': 2159, '51902': 2160, '7827': 2161, '28989': 2162, '73088': 2163, '90220': 2164, 'V462': 2165, '56987': 2166, 'V4986': 2167, '78609': 2168, '2832': 2169, '6945': 2170, '67434': 2171, '80375': 2172, '42654': 2173, 'E8216': 2174, 'E9360': 2175, '27702': 2176, 'V0252': 2177, '3952': 2178, '48232': 2179, '2755': 2180, '1649': 2181, 'E9991': 2182, 'E9318': 2183, '73009': 2184, '41513': 2185, '7851': 2186, 'V160': 2187, '58881': 2188, '20936': 2189, '86601': 2190, '56400': 2191, '7583': 2192, '4142': 2193, '2859': 2194, '78469': 2195, '76496': 2196, '4295': 2197, '72403': 2198, '99933': 2199, '20270': 2200, '8861': 2201, '78199': 2202, '7383': 2203, '45981': 2204, '56989': 2205, 'E8147': 2206, '88110': 2207, 'V143': 2208, '3330': 2209, '9563': 2210, '6396': 2211, '73670': 2212, '38832': 2213, '20890': 2214, '43885': 2215, '05471': 2216, '7443': 2217, '8471': 2218, '31531': 2219, '7902': 2220, '86413': 2221, '32341': 2222, '9594': 2223, '30553': 2224, '9278': 2225, '71986': 2226, '06641': 2227, '3689': 2228, '67402': 2229, '80161': 2230, '30751': 2231, '30150': 2232, '62130': 2233, '2722': 2234, '20191': 2235, '71913': 2236, '8020': 2237, '9248': 2238, '7235': 2239, '9561': 2240, '9245': 2241, 'V5422': 2242, '74869': 2243, 'V4578': 2244, '8700': 2245, '73017': 2246, '7818': 2247, 'V4588': 2248, '69282': 2249, '1550': 2250, '7249': 2251, '4299': 2252, '88100': 2253, 'V5309': 2254, '2982': 2255, '78639': 2256, '8440': 2257, 'V1250': 2258, '5206': 2259, '2870': 2260, '7785': 2261, '41001': 2262, 'E9804': 2263, '1729': 2264, '30583': 2265, 'V1364': 2266, '1129': 2267, '8711': 2268, '75453': 2269, 'E0080': 2270, '38872': 2271, '88111': 2272, 'V331': 2273, '64233': 2274, '75315': 2275, '80475': 2276, '9162': 2277, '1548': 2278, '44409': 2279, '65963': 2280, '9711': 2281, 'E8669': 2282, '5762': 2283, '66554': 2284, 'E8888': 2285, '59581': 2286, '3080': 2287, 'V270': 2288, '2528': 2289, '36320': 2290, '85240': 2291, '96501': 2292, '3334': 2293, '2811': 2294, '77214': 2295, '9196': 2296, '84209': 2297, '30571': 2298, '78009': 2299, '5750': 2300, '03285': 2301, '86321': 2302, '83114': 2303, '4536': 2304, '1642': 2305, '92320': 2306, '1373': 2307, '6175': 2308, '37739': 2309, '79579': 2310, '30420': 2311, 'V8531': 2312, '5538': 2313, '7062': 2314, '36107': 2315, '37273': 2316, 'E0299': 2317, '4541': 2318, '56960': 2319, '7717': 2320, '86409': 2321, '20285': 2322, '7750': 2323, '45382': 2324, 'V5841': 2325, '4387': 2326, 'V8536': 2327, '9191': 2328, '78262': 2329, '29524': 2330, '41412': 2331, '96509': 2332, '325': 2333, '71904': 2334, '73689': 2335, '47829': 2336, '81406': 2337, '49322': 2338, '7962': 2339, '7176': 2340, '46400': 2341, '42971': 2342, '7093': 2343, '83200': 2344, 'V5865': 2345, '07032': 2346, '0840': 2347, '7083': 2348, '6268': 2349, 'V170': 2350, '78862': 2351, '6253': 2352, '2700': 2353, '34693': 2354, '6952': 2355, '99832': 2356, '2704': 2357, '03819': 2358, '71235': 2359, '36315': 2360, '73630': 2361, '83501': 2362, '5168': 2363, '3094': 2364, '71590': 2365, '92311': 2366, '00862': 2367, '36274': 2368, '52101': 2369, '23691': 2370, '20972': 2371, '29632': 2372, '8794': 2373, '53170': 2374, '42291': 2375, '30410': 2376, 'V1507': 2377, '37312': 2378, '9558': 2379, '82129': 2380, 'E8497': 2381, '9068': 2382, '55129': 2383, '0092': 2384, 'V6285': 2385, '3379': 2386, 'V4987': 2387, '76492': 2388, '65583': 2389, '5756': 2390, '36101': 2391, '1515': 2392, '29411': 2393, '78052': 2394, '3962': 2395, '9998': 2396, '6140': 2397, 'E9380': 2398, '2137': 2399, '83503': 2400, '30089': 2401, '80322': 2402, '47822': 2403, '5734': 2404, '9912': 2405, '38908': 2406, '78054': 2407, '2539': 2408, '4480': 2409, '64421': 2410, 'E0026': 2411, '6820': 2412, '36000': 2413, '2839': 2414, '42652': 2415, '71180': 2416, '7089': 2417, '37881': 2418, '95215': 2419, '56721': 2420, '74729': 2421, 'E9294': 2422, '71950': 2423, '1560': 2424, '5799': 2425, 'E8830': 2426, '85175': 2427, '6214': 2428, '0360': 2429, '40501': 2430, '04110': 2431, '82133': 2432, '4417': 2433, 'V4989': 2434, '90289': 2435, '75431': 2436, 'V422': 2437, '72709': 2438, '4531': 2439, '78951': 2440, '7224': 2441, '7753': 2442, '79319': 2443, '7523': 2444, '36910': 2445, '25080': 2446, '9661': 2447, '75262': 2448, 'V0971': 2449, 'E8542': 2450, '3814': 2451, '29532': 2452, '29689': 2453, '80175': 2454, '9120': 2455, '79402': 2456, '70713': 2457, '29624': 2458, '7291': 2459, '632': 2460, 'E9359': 2461, '7981': 2462, '42732': 2463, '7271': 2464, '9093': 2465, '048': 2466, 'E8178': 2467, '1513': 2468, 'E9386': 2469, '725': 2470, 'V162': 2471, '8940': 2472, '64254': 2473, '07953': 2474, '52333': 2475, '33729': 2476, '03849': 2477, '3693': 2478, '69279': 2479, '87371': 2480, '0071': 2481, '3968': 2482, '6261': 2483, '5997': 2484, '35801': 2485, '73312': 2486, 'V7651': 2487, '30422': 2488, '2866': 2489, '7595': 2490, 'E9659': 2491, '2411': 2492, 'V624': 2493, '9473': 2494, 'V1203': 2495, '24950': 2496, '72705': 2497, '9142': 2498, '80311': 2499, '7709': 2500, '01894': 2501, '83104': 2502, '38921': 2503, 'E9060': 2504, '57451': 2505, '6202': 2506, '1641': 2507, '20047': 2508, '92231': 2509, '72230': 2510, '9161': 2511, 'E9298': 2512, 'E8784': 2513, '81514': 2514, '29421': 2515, '47811': 2516, '8027': 2517, '5285': 2518, '0093': 2519, 'E9064': 2520, '64813': 2521, '25050': 2522, '78552': 2523, '9060': 2524, '74920': 2525, '1529': 2526, '7792': 2527, '1450': 2528, '4730': 2529, '4371': 2530, '6988': 2531, '87411': 2532, '37420': 2533, '83101': 2534, 'V5426': 2535, '67324': 2536, '90223': 2537, '25639': 2538, '4918': 2539, '43811': 2540, '2442': 2541, '37842': 2542, '51289': 2543, '2710': 2544, 'E9378': 2545, '3949': 2546, '2761': 2547, '79439': 2548, 'E8502': 2549, '48242': 2550, '3019': 2551, '1910': 2552, '80362': 2553, '0048': 2554, '5119': 2555, 'V6405': 2556, 'V468': 2557, '37203': 2558, '37143': 2559, 'V0482': 2560, '9729': 2561, '44481': 2562, '37630': 2563, '38903': 2564, '97081': 2565, '66111': 2566, '3202': 2567, '0088': 2568, '4320': 2569, '83650': 2570, '52579': 2571, '34540': 2572, '25092': 2573, '99830': 2574, '86393': 2575, '71835': 2576, '37990': 2577, '66511': 2578, '36019': 2579, '7585': 2580, '64863': 2581, '71911': 2582, '71594': 2583, '83901': 2584, '0849': 2585, '7808': 2586, '43813': 2587, '52100': 2588, '6181': 2589, '37923': 2590, 'E8212': 2591, '90254': 2592, 'V789': 2593, '2776': 2594, 'E9391': 2595, '6861': 2596, '85226': 2597, '74602': 2598, '7760': 2599, '9165': 2600, '5770': 2601, '82032': 2602, '83969': 2603, 'V1003': 2604, '7596': 2605, '79953': 2606, '36520': 2607, '36800': 2608, '45184': 2609, '94214': 2610, '75539': 2611, '8209': 2612, '01880': 2613, '80423': 2614, '9562': 2615, '81519': 2616, '3545': 2617, '27802': 2618, '2552': 2619, '90141': 2620, '44389': 2621, 'V016': 2622, '82529': 2623, '55001': 2624, 'V427': 2625, '0463': 2626, 'E0061': 2627, '37311': 2628, 'V453': 2629, '1809': 2630, '5235': 2631, '34281': 2632, 'V9081': 2633, '6398': 2634, '81333': 2635, '3831': 2636, '74441': 2637, '81331': 2638, 'V9103': 2639, '45621': 2640, '6266': 2641, '2842': 2642, '94108': 2643, '5131': 2644, '1390': 2645, '7325': 2646, '9224': 2647, '45829': 2648, '7854': 2649, 'E8619': 2650, '80660': 2651, '5778': 2652, '38015': 2653, '7597': 2654, 'V1081': 2655, '9390': 2656, '6082': 2657, 'V6111': 2658, '4479': 2659, '0902': 2660, '9238': 2661, '6071': 2662, '51853': 2663, '4958': 2664, '5852': 2665, 'E0009': 2666, '9952': 2667, '79415': 2668, '51633': 2669, '96909': 2670, '7933': 2671, '29522': 2672, '20302': 2673, '75510': 2674, '57491': 2675, '76416': 2676, 'V537': 2677, '64294': 2678, 'V601': 2679, '78449': 2680, '28249': 2681, '71930': 2682, '5264': 2683, '43300': 2684, '23872': 2685, '8744': 2686, '4957': 2687, '2581': 2688, 'V2652': 2689, '9698': 2690, '1830': 2691, '7218': 2692, '7960': 2693, '9695': 2694, '20973': 2695, '3158': 2696, '43840': 2697, 'E9241': 2698, '7318': 2699, '78341': 2700, '77182': 2701, '8742': 2702, '6146': 2703, '2270': 2704, 'E9444': 2705, '7621': 2706, '28310': 2707, '85189': 2708, '53171': 2709, '37710': 2710, '90301': 2711, '6180': 2712, '7015': 2713, '25073': 2714, '2874': 2715, '64241': 2716, '3439': 2717, '73008': 2718, '8370': 2719, '71907': 2720, '37941': 2721, '05412': 2722, '70701': 2723, '53240': 2724, '7010': 2725, '25071': 2726, '23875': 2727, '85209': 2728, '34680': 2729, '71296': 2730, '44281': 2731, '6028': 2732, 'E9570': 2733, '82320': 2734, '5752': 2735, '57410': 2736, '80132': 2737, '7908': 2738, 'V5302': 2739, '042': 2740, '82120': 2741, '71966': 2742, 'E9209': 2743, '3659': 2744, '80162': 2745, '64833': 2746, '4540': 2747, 'E9315': 2748, '4489': 2749, '4554': 2750, '64103': 2751, '69515': 2752, '61610': 2753, 'V1309': 2754, '28319': 2755, '80224': 2756, 'V5339': 2757, '80422': 2758, 'E8637': 2759, '4219': 2760, '74401': 2761, '59971': 2762, 'E8552': 2763, '81602': 2764, '75617': 2765, '20001': 2766, '83402': 2767, '7462': 2768, '37240': 2769, '78491': 2770, '74763': 2771, '72291': 2772, '71191': 2773, 'E9889': 2774, 'V4575': 2775, '6170': 2776, '82111': 2777, '4561': 2778, '67311': 2779, '5820': 2780, '75564': 2781, '80609': 2782, '9986': 2783, '64931': 2784, '85246': 2785, '7386': 2786, '33920': 2787, '83202': 2788, '5847': 2789, '6953': 2790, '2511': 2791, '81611': 2792, '1962': 2793, '25090': 2794, '40490': 2795, 'V4985': 2796, 'V9010': 2797, '460': 2798, '9725': 2799, '28264': 2800, '5968': 2801, '9899': 2802, '4269': 2803, 'V169': 2804, '8930': 2805, '01215': 2806, '85400': 2807, '5721': 2808, '01805': 2809, 'V6129': 2810, '71236': 2811, '23871': 2812, '20008': 2813, '7852': 2814, '24991': 2815, '75563': 2816, '78072': 2817, '1228': 2818, '1120': 2819, '71432': 2820, '73342': 2821, '2182': 2822, '25060': 2823, '69512': 2824, '44322': 2825, '9012': 2826, '99971': 2827, '64292': 2828, '8961': 2829, '8247': 2830, '27739': 2831, '90181': 2832, 'V168': 2833, '78934': 2834, '81203': 2835, '57142': 2836, '7816': 2837, '2753': 2838, '1715': 2839, '8472': 2840, '2869': 2841, '81405': 2842, '92321': 2843, '37942': 2844, 'V850': 2845, '8064': 2846, '8631': 2847, '1717': 2848, '80312': 2849, '2541': 2850, '6965': 2851, '99589': 2852, '8080': 2853, '11505': 2854, '1919': 2855, '4613': 2856, '7993': 2857, '0309': 2858, '00845': 2859, '4612': 2860, '57480': 2861, '60090': 2862, '7222': 2863, '5569': 2864, 'E8249': 2865, '52109': 2866, '3885': 2867, '3970': 2868, '9961': 2869, '80180': 2870, '1981': 2871, '7572': 2872, '75683': 2873, '30443': 2874, '7797': 2875, '36844': 2876, '6169': 2877, '2143': 2878, '30580': 2879, '6954': 2880, '78097': 2881, '1124': 2882, '36281': 2883, '74423': 2884, '5859': 2885, '86402': 2886, '90442': 2887, '1911': 2888, '65404': 2889, '72781': 2890, '25042': 2891, '7905': 2892, 'V596': 2893, '7746': 2894, '33720': 2895, '71916': 2896, '43822': 2897, '29420': 2898, '47411': 2899, '66632': 2900, '39890': 2901, '73629': 2902, '4748': 2903, '1273': 2904, '41022': 2905, '2889': 2906, '9228': 2907, 'V4282': 2908, '30472': 2909, '80222': 2910, '92411': 2911, '8840': 2912, 'V1201': 2913, '8676': 2914, '29010': 2915, '86501': 2916, 'E8702': 2917, '8921': 2918, '5811': 2919, '36362': 2920, '80485': 2921, '9051': 2922, 'E976': 2923, '20071': 2924, '53530': 2925, 'E8012': 2926, '7932': 2927, '71108': 2928, '86222': 2929, '86100': 2930, '2251': 2931, '937': 2932, '28982': 2933, '64403': 2934, 'V1041': 2935, '4465': 2936, 'V7281': 2937, '3959': 2938, '677': 2939, '64101': 2940, '4258': 2941, '85242': 2942, '36204': 2943, '99859': 2944, 'V1029': 2945, 'E8415': 2946, '4353': 2947, '42290': 2948, '59659': 2949, '7484': 2950, '8404': 2951, '27787': 2952, '83906': 2953, 'E9452': 2954, 'E9283': 2955, '38300': 2956, 'V3200': 2957, '24291': 2958, 'E0076': 2959, '8068': 2960, '80706': 2961, '52800': 2962, 'V8401': 2963, '56722': 2964, '71696': 2965, '07049': 2966, '74769': 2967, '4148': 2968, '27801': 2969, 'V8539': 2970, '5529': 2971, '5646': 2972, '75550': 2973, '1608': 2974, '2800': 2975, 'E8780': 2976, '30752': 2977, 'E9356': 2978, 'V5332': 2979, 'E9102': 2980, '32720': 2981, 'E0030': 2982, '48289': 2983, 'E9387': 2984, '87349': 2985, '20917': 2986, '34404': 2987, '1578': 2988, '59689': 2989, 'V5482': 2990, '7934': 2991, '4241': 2992, '8083': 2993, '5640': 2994, '92701': 2995, '1703': 2996, '37034': 2997, '37431': 2998, 'V5415': 2999, '9582': 3000, '7566': 3001, '86500': 3002, 'E9322': 3003, '7481': 3004, '4233': 3005, 'E8250': 3006, '81351': 3007, '43852': 3008, '4941': 3009, '44421': 3010, '49122': 3011, '30723': 3012, '55202': 3013, '28732': 3014, 'V641': 3015, '61882': 3016, 'E8529': 3017, '71846': 3018, '04119': 3019, '20287': 3020, '64204': 3021, '2532': 3022, '11599': 3023, '2387': 3024, '36840': 3025, '4881': 3026, '94800': 3027, '44102': 3028, '3519': 3029, '99609': 3030, '3239': 3031, '2851': 3032, '3431': 3033, '4659': 3034, '3559': 3035, '99662': 3036, '53340': 3037, '42833': 3038, '5566': 3039, '65221': 3040, '1160': 3041, '99702': 3042, '74343': 3043, '48282': 3044, '85305': 3045, '7236': 3046, '29042': 3047, '34710': 3048, '1452': 3049, '9999': 3050, '71845': 3051, '30471': 3052, '92612': 3053, '85403': 3054, '9895': 3055, '0539': 3056, '30522': 3057, '4264': 3058, 'V1090': 3059, '2163': 3060, '85219': 3061, '68102': 3062, '3013': 3063, '33523': 3064, '3061': 3065, '29620': 3066, '9514': 3067, '9212': 3068, '75566': 3069, '34889': 3070, '23877': 3071, '2598': 3072, '01895': 3073, '4378': 3074, '87364': 3075, '53013': 3076, '5761': 3077, '64782': 3078, '5952': 3079, '75162': 3080, 'V3201': 3081, '8677': 3082, '74741': 3083, '65613': 3084, '4239': 3085, '7793': 3086, '3229': 3087, '4610': 3088, '4243': 3089, '9500': 3090, 'E9329': 3091, '3969': 3092, 'E9308': 3093, '20975': 3094, '25013': 3095, 'E8785': 3096, '7452': 3097, '64511': 3098, '04184': 3099, '30742': 3100, 'E9650': 3101, '36563': 3102, '28412': 3103, '30552': 3104, 'E8058': 3105, '7704': 3106, '501': 3107, '7812': 3108, '30759': 3109, '541': 3110, '01890': 3111, '03842': 3112, '99799': 3113, 'E9208': 3114, '43310': 3115, '31230': 3116, '70712': 3117, '86384': 3118, '8243': 3119, '490': 3120, '53441': 3121, '30789': 3122, '30461': 3123, '33721': 3124, '28803': 3125, '25010': 3126, '71842': 3127, '1300': 3128, '38039': 3129, '3571': 3130, '7099': 3131, 'E8052': 3132, 'E8191': 3133, '52489': 3134, '78343': 3135, '9744': 3136, '6111': 3137, '5846': 3138, 'V644': 3139, '96561': 3140, '79009': 3141, '25072': 3142, '7197': 3143, '29631': 3144, '4778': 3145, '1431': 3146, '82535': 3147, '99984': 3148, '47830': 3149, '81512': 3150, '99741': 3151, '20212': 3152, 'E8663': 3153, '2823': 3154, '81510': 3155, 'V671': 3156, '33701': 3157, 'E8252': 3158, '86419': 3159, '37601': 3160, '52340': 3161, '4210': 3162, '8900': 3163, '6850': 3164, '30520': 3165, '0412': 3166, '2462': 3167, '1880': 3168, '72740': 3169, '86112': 3170, '8770': 3171, '87363': 3172, '46411': 3173, '82020': 3174, '7480': 3175, 'E8543': 3176, '47825': 3177, '9663': 3178, 'E9408': 3179, '49302': 3180, '3108': 3181, '38201': 3182, '99665': 3183, '45381': 3184, '81502': 3185, '94234': 3186, '71945': 3187, '7359': 3188, '5193': 3189, 'E9419': 3190, '53401': 3191, '81381': 3192, '61179': 3193, 'E8122': 3194, '1173': 3195, '4560': 3196, '4271': 3197, '8600': 3198, '40291': 3199, '1589': 3200, 'E9583': 3201, '075': 3202, '37871': 3203, '87330': 3204, '85131': 3205, 'V643': 3206, 'V452': 3207, '5670': 3208, '83908': 3209, '7467': 3210, '4570': 3211, '7600': 3212, '80172': 3213, '0499': 3214, '29383': 3215, '3069': 3216, '37850': 3217, '1510': 3218, '34581': 3219, '1613': 3220, '80853': 3221, '80708': 3222, '30000': 3223, '53021': 3224, 'E8278': 3225, '73301': 3226, '42091': 3227, '8418': 3228, '94840': 3229, '75559': 3230, '1309': 3231, '9132': 3232, '8504': 3233, '5565': 3234, 'E9413': 3235, '32361': 3236, '76072': 3237, '4739': 3238, '1726': 3239, 'V068': 3240, '6040': 3241, '3574': 3242, '9735': 3243, '61172': 3244, '3380': 3245, '3236': 3246, '78443': 3247, 'E8150': 3248, '40290': 3249, '8713': 3250, 'V298': 3251, '0340': 3252, '7795': 3253, '2382': 3254, '81305': 3255, '79092': 3256, '1174': 3257, '45351': 3258, '73020': 3259, '4292': 3260, 'E8384': 3261, '76497': 3262, '77084': 3263, '38918': 3264, '1588': 3265, '88121': 3266, '30591': 3267, '5758': 3268, '36217': 3269, '5692': 3270, '86804': 3271, '1720': 3272, '6201': 3273, '79509': 3274, '4329': 3275, '1618': 3276, '11283': 3277, '74689': 3278, '2972': 3279, '81353': 3280, '80145': 3281, '80228': 3282, 'E9395': 3283, '8745': 3284, '36960': 3285, '2510': 3286, '8448': 3287, 'V103': 3288, '80466': 3289, '71120': 3290, 'V037': 3291, '94203': 3292, '9881': 3293, 'E8794': 3294, '3241': 3295, '4422': 3296, '1571': 3297, '25091': 3298, '81344': 3299, '99813': 3300, '68100': 3301, '67482': 3302, '94850': 3303, '85223': 3304, '27911': 3305, '9515': 3306, '2812': 3307, '2692': 3308, '72669': 3309, 'E8261': 3310, 'E8655': 3311, '81332': 3312, 'V4569': 3313, '24900': 3314, '7936': 3315, 'E9601': 3316, '81400': 3317, '86509': 3318, '2538': 3319, '86512': 3320, '32726': 3321, '60782': 3322, '4430': 3323, '86802': 3324, '32719': 3325, 'V1505': 3326, '72981': 3327, '1440': 3328, '64261': 3329, '3331': 3330, 'V555': 3331, 'V1504': 3332, '42789': 3333, '71849': 3334, '56881': 3335, '74722': 3336, '34210': 3337, '75433': 3338, '8728': 3339, '01190': 3340, '75019': 3341, '9604': 3342, '7765': 3343, '56985': 3344, '29289': 3345, '73671': 3346, '73313': 3347, '4538': 3348, 'E9190': 3349, '2536': 3350, 'V4574': 3351, '20490': 3352, '60000': 3353, '33900': 3354, '1522': 3355, '80301': 3356, '37950': 3357, '51636': 3358, '85185': 3359, 'E9499': 3360, '75269': 3361, '7541': 3362, '20310': 3363, 'E8228': 3364, '99889': 3365, 'E8381': 3366, '95207': 3367, '9164': 3368, '36816': 3369, '71965': 3370, '82523': 3371, '8672': 3372, '81242': 3373, 'V550': 3374, '41032': 3375, 'E9509': 3376, '81002': 3377, '20042': 3378, '9588': 3379, 'V063': 3380, '30573': 3381, 'E9132': 3382, '4350': 3383, '3543': 3384, '85212': 3385, '30001': 3386, '82123': 3387, '40201': 3388, '64933': 3389, '46619': 3390, '83500': 3391, '33520': 3392, '8871': 3393, '8245': 3394, '6141': 3395, '27509': 3396, '4838': 3397, '6270': 3398, '80703': 3399, '25003': 3400, '4449': 3401, '76508': 3402, '82131': 3403, '78863': 3404, '7020': 3405, 'V426': 3406, '64793': 3407, '3079': 3408, '94532': 3409, '01505': 3410, '77989': 3411, 'V222': 3412, '52550': 3413, '3099': 3414, '5269': 3415, '86381': 3416, '2853': 3417, '5282': 3418, '72283': 3419, '8851': 3420, '99651': 3421, 'V8389': 3422, '8089': 3423, '68101': 3424, '5582': 3425, '4231': 3426, '6010': 3427, '8791': 3428, 'V5867': 3429, '64861': 3430, '85182': 3431, '51181': 3432, '1820': 3433, '73712': 3434, '2967': 3435, '7620': 3436, '8738': 3437, '83302': 3438, '56210': 3439, '4162': 3440, 'E9290': 3441, '52540': 3442, '27901': 3443, '86503': 3444, '8406': 3445, '71106': 3446, 'V4283': 3447, '81342': 3448, '33511': 3449, '5298': 3450, '44772': 3451, '32372': 3452, '53011': 3453, '9599': 3454, '86103': 3455, '179': 3456, '3159': 3457, '47871': 3458, '62989': 3459, '05371': 3460, '28860': 3461, '70721': 3462, '99654': 3463, 'V023': 3464, '2588': 3465, '61801': 3466, 'V4501': 3467, '30561': 3468, '9210': 3469, 'V135': 3470, '33828': 3471, '73022': 3472, '65571': 3473, '36211': 3474, '71580': 3475, '47879': 3476, '77581': 3477, '37854': 3478, '8761': 3479, '1330': 3480, '20190': 3481, '3369': 3482, '76510': 3483, '22809': 3484, '6273': 3485, '70909': 3486, '3490': 3487, '36573': 3488, '57411': 3489, '99932': 3490, '65451': 3491, '9747': 3492, '66612': 3493, '2117': 3494, '36813': 3495, '92300': 3496, '3234': 3497, 'E8792': 3498, '80625': 3499, '75567': 3500, '64664': 3501, '53160': 3502, 'V4031': 3503, '96979': 3504, '33522': 3505, '42650': 3506, '1639': 3507, '29572': 3508, '9509': 3509, '1320': 3510, '78340': 3511, '53541': 3512, '80605': 3513, '71616': 3514, '2535': 3515, '37851': 3516, '99833': 3517, '7475': 3518, '66534': 3519, '7238': 3520, 'E9291': 3521, '73739': 3522, '1916': 3523, '80470': 3524, '7580': 3525, '3482': 3526, '92821': 3527, '5199': 3528, '74320': 3529, '27800': 3530, '45181': 3531, '9710': 3532, '8081': 3533, '0530': 3534, '32737': 3535, '1724': 3536, 'V4963': 3537, '85109': 3538, '1640': 3539, '64234': 3540, '83400': 3541, '75511': 3542, '86803': 3543, '78057': 3544, '3332': 3545, '81419': 3546, '2639': 3547, '78702': 3548, '81412': 3549, '2253': 3550, '79400': 3551, '76402': 3552, '59381': 3553, '78441': 3554, '1700': 3555, '30122': 3556, '5642': 3557, '80600': 3558, '7763': 3559, '5296': 3560, '9569': 3561, '7757': 3562, '4720': 3563, '1978': 3564, '1539': 3565, 'V4971': 3566, '65444': 3567, '78703': 3568, 'E9457': 3569, '80473': 3570, '8408': 3571, 'V1069': 3572, '99685': 3573, '7720': 3574, '5959': 3575, '80082': 3576, '4377': 3577, '76381': 3578, '73004': 3579, '74609': 3580, '29680': 3581, '85203': 3582, '1906': 3583, '1763': 3584, '8750': 3585, 'E9466': 3586, '82534': 3587, '34830': 3588, '80366': 3589, '1982': 3590, '769': 3591, '2459': 3592, '9471': 3593, '2828': 3594, 'E9323': 3595, '316': 3596, '29284': 3597, 'E9051': 3598, '7804': 3599, '66914': 3600, '0793': 3601, '99590': 3602, '80604': 3603, '07030': 3604, '6392': 3605, '99680': 3606, '99527': 3607, '7631': 3608, '3592': 3609, '2306': 3610, '99681': 3611, '44321': 3612, '5563': 3613, '85106': 3614, 'V1508': 3615, '36811': 3616, '5738': 3617, '88000': 3618, '51284': 3619, '83300': 3620, '81001': 3621, '2913': 3622, '2863': 3623, '4581': 3624, '72971': 3625, '45376': 3626, 'V714': 3627, '86355': 3628, '76409': 3629, '99663': 3630, '3593': 3631, '6212': 3632, '76495': 3633, 'E9053': 3634, 'E8182': 3635, 'E9809': 3636, 'E9173': 3637, '7454': 3638, '7930': 3639, '28731': 3640, '33399': 3641, '6279': 3642, '99643': 3643, '23873': 3644, '7211': 3645, '3383': 3646, '78760': 3647, '7876': 3648, '44101': 3649, '2721': 3650, 'E9292': 3651, '99689': 3652, '37024': 3653, '65251': 3654, '34211': 3655, '70714': 3656, '3589': 3657, '94100': 3658, '71481': 3659, '3522': 3660, '78939': 3661, 'V4571': 3662, '1570': 3663, '3068': 3664, '72742': 3665, '73311': 3666, 'E888': 3667, '53989': 3668, '42761': 3669, '37700': 3670, '0540': 3671, '2900': 3672, 'E8580': 3673, 'V8533': 3674, '9989': 3675, '99561': 3676, 'E977': 3677, '8052': 3678, '7835': 3679, 'E9438': 3680, '86354': 3681, '80235': 3682, '4733': 3683, '27900': 3684, '8407': 3685, '9971': 3686, '73001': 3687, '2813': 3688, '9671': 3689, '34290': 3690, '6274': 3691, '85144': 3692, '96972': 3693, '7741': 3694, '75430': 3695, '07020': 3696, '41189': 3697, 'V1811': 3698, '76075': 3699, 'E9177': 3700, '72930': 3701, '2930': 3702, '2682': 3703, 'E8500': 3704, '36043': 3705, '30483': 3706, '81212': 3707, '9063': 3708, '2910': 3709, '1531': 3710, '76528': 3711, 'V1087': 3712, '20929': 3713, '36403': 3714, '5194': 3715, '36105': 3716, '7920': 3717, '30151': 3718, '5996': 3719, '6278': 3720, '05472': 3721, '78053': 3722, '4179': 3723, '36203': 3724, 'E912': 3725, '5275': 3726, '4785': 3727, 'V6542': 3728, '56949': 3729, '80013': 3730, '72210': 3731, '7517': 3732, '9239': 3733, '80023': 3734, '2373': 3735, '47834': 3736, '85231': 3737, '08881': 3738, '72211': 3739, '25093': 3740, '82524': 3741, '43311': 3742, '1537': 3743, '85194': 3744, '5933': 3745, '76523': 3746, '7833': 3747, '8053': 3748, '88013': 3749, '3321': 3750, '01485': 3751, '52181': 3752, 'V5481': 3753, '118': 3754, '2699': 3755, '72612': 3756, '2371': 3757, '78831': 3758, 'V4962': 3759, '4168': 3760, '9973': 3761, '5262': 3762, '71536': 3763, '3549': 3764, '79959': 3765, '7563': 3766, '92400': 3767, '70401': 3768, '29384': 3769, '3739': 3770, '55120': 3771, '34501': 3772, '8604': 3773, '5198': 3774, '78837': 3775, 'E8550': 3776, '80135': 3777, '7599': 3778, '9980': 3779, '2880': 3780, '85221': 3781, 'V1541': 3782, '4358': 3783, '41419': 3784, '99941': 3785, '32742': 3786, '6089': 3787, '53311': 3788, '45372': 3789, '7230': 3790, '33812': 3791, '5293': 3792, '9009': 3793, '6984': 3794, '95914': 3795, '86353': 3796, 'V560': 3797, '20290': 3798, '56030': 3799, '2989': 3800, '61189': 3801, '56212': 3802, '1971': 3803, '1764': 3804, '42760': 3805, '2728': 3806, '1490': 3807, '34441': 3808, '1980': 3809, '8502': 3810, '1574': 3811, '77181': 3812, '3824': 3813, '48283': 3814, '71944': 3815, '20411': 3816, 'E8600': 3817, '30391': 3818, '20048': 3819, '2155': 3820, '6210': 3821, '9654': 3822, '76404': 3823, '08882': 3824, '3081': 3825, '8749': 3826, '69289': 3827, '04103': 3828, '77985': 3829, 'E9063': 3830, '2760': 3831, '88112': 3832, '83103': 3833, '7508': 3834, 'V5863': 3835, '95912': 3836, '36233': 3837, '85142': 3838, '5982': 3839, '9033': 3840, '71912': 3841, '7991': 3842, 'V8538': 3843, '7969': 3844, '938': 3845, '9221': 3846, '76505': 3847, '6960': 3848, '7141': 3849, 'E9068': 3850, '79401': 3851, '77081': 3852, '5277': 3853, '34460': 3854, '452': 3855, '86514': 3856, '74330': 3857, '6918': 3858, '81408': 3859, '9513': 3860, '4870': 3861, 'V4614': 3862, '75029': 3863, '7915': 3864, '05320': 3865, 'E8170': 3866, 'E9174': 3867, '73341': 3868, '66614': 3869, '53201': 3870, '80036': 3871, '71101': 3872, '1630': 3873, '73681': 3874, '33119': 3875, '1881': 3876, '60781': 3877, 'V1502': 3878, 'E8801': 3879, '34431': 3880, '3576': 3881, '1479': 3882, '76501': 3883, '37940': 3884, '20022': 3885, '53082': 3886, '30503': 3887, '1511': 3888, '40300': 3889, '80100': 3890, '3568': 3891, '9879': 3892, '65661': 3893, '85184': 3894, '7850': 3895, '80021': 3896, '80114': 3897, '42840': 3898, '1308': 3899, '9351': 3900, '78864': 3901, '1960': 3902, 'V8525': 3903, '94323': 3904, '9993': 3905, '86809': 3906, '63491': 3907, 'E8551': 3908, '1848': 3909, '9766': 3910, '3569': 3911, '41071': 3912, '94320': 3913, '1874': 3914, '64852': 3915, '53087': 3916, 'V4587': 3917, '52310': 3918, '7931': 3919, '542': 3920, '76499': 3921, '56789': 3922, '27402': 3923, '9975': 3924, '94536': 3925, '7714': 3926, '07983': 3927, '78791': 3928, '36205': 3929, '1920': 3930, '43819': 3931, 'E956': 3932, '79399': 3933, '9683': 3934, '59960': 3935, '25021': 3936, '2734': 3937, 'E9345': 3938, 'V1582': 3939, '7811': 3940, '1991': 3941, 'E9288': 3942, '78960': 3943, '80111': 3944, '99580': 3945, '74190': 3946, '99791': 3947, 'V5883': 3948, '81209': 3949, '7286': 3950, '9600': 3951, 'E8498': 3952, '25208': 3953, '4919': 3954, '38401': 3955, 'V145': 3956, 'E8282': 3957, '5280': 3958, '4373': 3959, '75461': 3960, '85183': 3961, '5949': 3962, '5733': 3963, '24200': 3964, '2181': 3965, '78033': 3966, 'V1050': 3967, 'V5419': 3968, '59582': 3969, '57481': 3970, '80024': 3971, '80120': 3972, '7483': 3973, '11281': 3974, '25022': 3975, '29580': 3976, 'E9201': 3977, '7202': 3978, '30011': 3979, '72633': 3980, '42741': 3981, '2554': 3982, '8731': 3983, 'E9274': 3984, '73319': 3985, '2759': 3986, '66331': 3987, '99686': 3988, '86600': 3989, '29650': 3990, '4321': 3991, '45385': 3992, '9948': 3993, '78550': 3994, 'E9689': 3995, '87352': 3996, '71940': 3997, '9911': 3998, '28866': 3999, 'E9420': 4000, 'V045': 4001, '77210': 4002, '76079': 4003, '7730': 4004, 'V1542': 4005, '53642': 4006, '56984': 4007, 'E8799': 4008, '71533': 4009, '71698': 4010, '45340': 4011, '20021': 4012, '29633': 4013, 'E9501': 4014, '83651': 4015, 'V1585': 4016, '34690': 4017, '78093': 4018, '6184': 4019, '90234': 4020, '1707': 4021, '3335': 4022, '74923': 4023, '86120': 4024, '63320': 4025, 'E8343': 4026, '5951': 4027, '85186': 4028, '7790': 4029, '73013': 4030, 'V189': 4031, '74683': 4032, '99565': 4033, '37200': 4034, '9061': 4035, '03840': 4036, 'E8499': 4037, '3209': 4038, 'V110': 4039, '71695': 4040, '2808': 4041, '1713': 4042, 'E8341': 4043, '42843': 4044, '8171': 4045, '9048': 4046, '40599': 4047, '81391': 4048, '7533': 4049, 'V8532': 4050, '48239': 4051, '80171': 4052, '20050': 4053, '41042': 4054, '1748': 4055, '72673': 4056, '5369': 4057, '56031': 4058, '99709': 4059, '8479': 4060, '7324': 4061, '3699': 4062, '27482': 4063, '71195': 4064, '4169': 4065, '71828': 4066, '70705': 4067, '5289': 4068, '82330': 4069, '20060': 4070, 'V5427': 4071, '4242': 4072, 'V8530': 4073, '2564': 4074, '20211': 4075, '75289': 4076, '5265': 4077, '4530': 4078, '37633': 4079, '34551': 4080, '82019': 4081, '1500': 4082, '74359': 4083, '9630': 4084, '57510': 4085, '4409': 4086, '34931': 4087, '2774': 4088, '1638': 4089, '5769': 4090, '2841': 4091, 'V148': 4092, '07951': 4093, '2718': 4094, '82100': 4095, '36522': 4096, 'V8534': 4097, '57400': 4098, '29570': 4099, '20010': 4100, '43812': 4101, '75612': 4102, '41002': 4103, 'E8811': 4104, '37810': 4105, '7671': 4106, 'E0060': 4107, '55300': 4108, '32381': 4109, '65921': 4110, '2727': 4111, '23989': 4112, '80849': 4113, 'V451': 4114, '30531': 4115, '5712': 4116, '80115': 4117, '5643': 4118, '49391': 4119, 'V4984': 4120, '1572': 4121, '6168': 4122, '0091': 4123, '2871': 4124, '7676': 4125, '46611': 4126, 'E8136': 4127, 'E0162': 4128, '37701': 4129, '44502': 4130, '74711': 4131, '449': 4132, '5934': 4133, 'E8169': 4134, '37952': 4135, '5991': 4136, '7904': 4137, '44620': 4138, '36284': 4139, '7570': 4140, 'V655': 4141, '53140': 4142, '9595': 4143, '34401': 4144, '76491': 4145, '2894': 4146, '34550': 4147, '62211': 4148, 'V4961': 4149, '2452': 4150, '76407': 4151, '2669': 4152, '4251': 4153, '80226': 4154, 'V51': 4155, '94127': 4156, '5362': 4157, '99801': 4158, '1765': 4159, 'V090': 4160, '5259': 4161, 'V5391': 4162, 'E0069': 4163, '5602': 4164, '56983': 4165, '30521': 4166, '28984': 4167, 'E8193': 4168, '1504': 4169, '3572': 4170, '72271': 4171, '81251': 4172, '3129': 4173, '1609': 4174, '1429': 4175, '6084': 4176, '52108': 4177, '78066': 4178, '71598': 4179, 'E9479': 4180, '75513': 4181, 'E8232': 4182, '20400': 4183, '9064': 4184, '2273': 4185, '27549': 4186, '1520': 4187, '36841': 4188, '1488': 4189, '69010': 4190, '7722': 4191, '37652': 4192, 'E9320': 4193, '23773': 4194, '9472': 4195, '2794': 4196, '7784': 4197, '31400': 4198, '80238': 4199, '78002': 4200, '1272': 4201, '55220': 4202, '5409': 4203, '96500': 4204, '3210': 4205, '43821': 4206, 'V0253': 4207, '6079': 4208, '05319': 4209, '5966': 4210, '36252': 4211, '92801': 4212, '74600': 4213, '5728': 4214, 'E851': 4215, 'V1044': 4216, '0419': 4217, 'E8130': 4218, '5713': 4219, '20800': 4220, 'V721': 4221, '45910': 4222, '3669': 4223, '99675': 4224, '77012': 4225, '87354': 4226, '1921': 4227, '8702': 4228, '52342': 4229, '74789': 4230, 'E8852': 4231, '64661': 4232, 'V6141': 4233, '28419': 4234, 'V1529': 4235, '5400': 4236, 'V5849': 4237, '05314': 4238, '41040': 4239, '80070': 4240, '80000': 4241, '94224': 4242, '64824': 4243, '7068': 4244, '28730': 4245, '1469': 4246, '28850': 4247, '2271': 4248, '5941': 4249, '75323': 4250, '71843': 4251, '78905': 4252, '66001': 4253, '81318': 4254, '99772': 4255, 'V556': 4256, 'V0481': 4257, '9760': 4258, '53784': 4259, '9984': 4260, '62981': 4261, '70711': 4262, '70901': 4263, '28749': 4264, '1884': 4265, '0239': 4266, '1736': 4267, '3551': 4268, '4380': 4269, '86411': 4270, '5513': 4271, '8679': 4272, '3029': 4273, '72270': 4274, '3541': 4275, '9995': 4276, 'V872': 4277, '7947': 4278, 'V486': 4279, 'E9385': 4280, '71226': 4281, '1728': 4282, '5283': 4283, '53783': 4284, '43380': 4285, '51919': 4286, '73027': 4287, '86130': 4288, '80631': 4289, '30747': 4290, '1481': 4291, '80116': 4292, '1209': 4293, '33381': 4294, '80709': 4295, '0549': 4296, '30029': 4297, '7282': 4298, '86110': 4299, '99657': 4300, '75619': 4301, '04149': 4302, '32724': 4303, '51881': 4304, 'V640': 4305, '3453': 4306, '30401': 4307, 'V4983': 4308, 'V8541': 4309, '44103': 4310, 'V146': 4311, 'E8798': 4312, '70725': 4313, '9678': 4314, '80126': 4315, '4848': 4316, '29540': 4317, '2357': 4318, '36372': 4319, '53291': 4320, '7537': 4321, 'E8761': 4322, 'E8769': 4323, '82532': 4324, '2448': 4325, '82381': 4326, '80416': 4327, '99640': 4328, '2301': 4329, '73345': 4330, '57470': 4331, '9585': 4332, '0785': 4333, '44023': 4334, '38922': 4335, '71897': 4336, '86131': 4337, '80436': 4338, 'V5423': 4339, '99700': 4340, '7992': 4341, '6256': 4342, '29644': 4343, '78907': 4344, 'V454': 4345, '43411': 4346, '9679': 4347, '30431': 4348, '79099': 4349, '3759': 4350, '53490': 4351, '95214': 4352, '27542': 4353, '85314': 4354, '5304': 4355, '4471': 4356, '64943': 4357, '3899': 4358, 'E8146': 4359, '99931': 4360, '3682': 4361, '32351': 4362, '78930': 4363, 'V698': 4364, '30411': 4365, '9180': 4366, '96901': 4367, 'V1242': 4368, '78959': 4369, 'V074': 4370, '7935': 4371, '3561': 4372, '7610': 4373, '33189': 4374, 'V163': 4375, '76527': 4376, '6289': 4377, '6825': 4378, '76525': 4379, '24221': 4380, '72141': 4381, '7881': 4382, 'V0489': 4383, '36231': 4384, '36400': 4385, '1739': 4386, '5679': 4387, '7711': 4388, '90229': 4389, '85404': 4390, '07889': 4391, '37289': 4392, '7469': 4393, '78065': 4394, '1509': 4395, '4780': 4396, '34212': 4397, 'V5844': 4398, '4784': 4399, '8701': 4400, '80603': 4401, '5121': 4402, '80624': 4403, '7504': 4404, '3445': 4405, '1620': 4406, 'V017': 4407, '79380': 4408, '73029': 4409, '43881': 4410, '64131': 4411, '00847': 4412, '7586': 4413, '84503': 4414, '6981': 4415, 'V1086': 4416, '78909': 4417, '2534': 4418, '94209': 4419, '80508': 4420, 'V4973': 4421, '30021': 4422, 'E966': 4423, '4553': 4424, '7131': 4425, '90226': 4426, 'E8668': 4427, '30014': 4428, '9685': 4429, '36970': 4430, '1898': 4431, '83401': 4432, '41511': 4433, 'V8821': 4434, '3562': 4435, 'V448': 4436, '86511': 4437, '80711': 4438, '217': 4439, '9011': 4440, '5940': 4441, 'E9138': 4442, '95893': 4443, '80234': 4444, '6235': 4445, '73679': 4446, '8901': 4447, '45379': 4448, '6116': 4449, '67403': 4450, '7564': 4451, '2778': 4452, '7473': 4453, 'V4450': 4454, '85145': 4455, '7869': 4456, '88120': 4457, '40411': 4458, 'V4585': 4459, '8500': 4460, '57420': 4461, 'E0089': 4462, '80622': 4463, '64892': 4464, '43882': 4465, '5793': 4466, '7505': 4467, '78321': 4468, '37489': 4469, '66131': 4470, 'E0020': 4471, 'V1651': 4472, 'E911': 4473, '59370': 4474, 'E9242': 4475, 'E8716': 4476, '86350': 4477, '87321': 4478, 'V600': 4479, '8912': 4480, '79022': 4481, 'V8381': 4482, '3418': 4483, '04671': 4484, '920': 4485, 'V8522': 4486, 'V091': 4487, 'V1279': 4488, '9082': 4489, 'E9270': 4490, '29625': 4491, '5849': 4492, '7246': 4493, 'V6109': 4494, 'V716': 4495, '41519': 4496, '64663': 4497, 'E8698': 4498, '20240': 4499, '1985': 4500, '4372': 4501, '79093': 4502, '72142': 4503, '72879': 4504, '1538': 4505, '2733': 4506, 'E9301': 4507, '83813': 4508, '9102': 4509, '34831': 4510, '2867': 4511, '52511': 4512, '75160': 4513, '29990': 4514, '37909': 4515, '42490': 4516, '05443': 4517, '2572': 4518, '5780': 4519, '23772': 4520, '5253': 4521, '1329': 4522, '85232': 4523, '7279': 4524, '76511': 4525, '27403': 4526, '42820': 4527, 'E9205': 4528, '53410': 4529, '71902': 4530, '4467': 4531, '6262': 4532, '86383': 4533, '2337': 4534, '82531': 4535, '20000': 4536, '30430': 4537, '3342': 4538, '64814': 4539, 'V1553': 4540, '92811': 4541, '2394': 4542, '9341': 4543, '7828': 4544, '7070': 4545, '30390': 4546, '64243': 4547, '80321': 4548, '64821': 4549, '51631': 4550, '8441': 4551, '1759': 4552, '80410': 4553, '53501': 4554, '73733': 4555, '2166': 4556, '29211': 4557, 'V400': 4558, '2298': 4559, '70900': 4560, 'E9390': 4561, '2779': 4562, '56889': 4563, '2379': 4564, '2589': 4565, '9726': 4566, '76403': 4567, '9583': 4568, 'E8041': 4569, '1471': 4570, '8712': 4571, '9220': 4572, '0388': 4573, '9941': 4574, '01354': 4575, '37000': 4576, '80141': 4577, '41010': 4578, '99779': 4579, '3789': 4580, '37443': 4581, '0844': 4582, '80060': 4583, 'V0261': 4584, '22801': 4585, '36901': 4586, '1629': 4587, '1840': 4588, 'E0161': 4589, 'E8199': 4590, '1536': 4591, '28950': 4592, '8792': 4593, '41411': 4594, '72989': 4595, '72999': 4596, '81501': 4597, '64251': 4598, '27540': 4599, '75521': 4600, '71691': 4601, 'E9293': 4602, '9957': 4603, '04100': 4604, '92720': 4605, 'V0991': 4606, '81503': 4607, '85150': 4608, '67202': 4609, '82101': 4610, '75319': 4611, '1521': 4612, '86801': 4613, '85302': 4614, '1568': 4615, '80124': 4616, '07054': 4617, '29651': 4618, '30928': 4619, '1768': 4620, '1970': 4621, '5790': 4622, '32382': 4623, '30402': 4624, '9046': 4625, '90242': 4626, '7284': 4627, '25542': 4628, '2210': 4629, '5439': 4630, 'E0008': 4631, '7101': 4632, '74685': 4633, '1419': 4634, 'V301': 4635, '33902': 4636, '05440': 4637, '1882': 4638, '40413': 4639, '9053': 4640, '5184': 4641, '66582': 4642, '73620': 4643, '0413': 4644, 'V1551': 4645, '56729': 4646, 'E9330': 4647, '5363': 4648, '2384': 4649, '83311': 4650, '86320': 4651, '2824': 4652, '5988': 4653, 'E9198': 4654, 'V5489': 4655, '8241': 4656, 'E9364': 4657, '0051': 4658, 'E9478': 4659, '3222': 4660, '81244': 4661, '70709': 4662, '9701': 4663, '64653': 4664, '1914': 4665, '34839': 4666, '79094': 4667, 'E0064': 4668, 'E0138': 4669, 'V1253': 4670, '7726': 4671, '79671': 4672, '85100': 4673, '63552': 4674, '44020': 4675, '34500': 4676, '71847': 4677, '20213': 4678, '4111': 4679, '41401': 4680, '80705': 4681, '7622': 4682, '1832': 4683, '32089': 4684, 'E9453': 4685, '86405': 4686, '24230': 4687, 'E9426': 4688, '78050': 4689, '75249': 4690, '3518': 4691, '2112': 4692, '24201': 4693, '683': 4694, 'V181': 4695, '6929': 4696, '5561': 4697, '8261': 4698, '1118': 4699, '28489': 4700, '53085': 4701, '38869': 4702, '37924': 4703, '64904': 4704, '53020': 4705, 'V1022': 4706, '73300': 4707, '2515': 4708, '85405': 4709, '8281': 4710, '87264': 4711, '76518': 4712, '6948': 4713, '69018': 4714, '41041': 4715, '79001': 4716, '65801': 4717, '42981': 4718, '81411': 4719, '0279': 4720, '75651': 4721, '2971': 4722, '1464': 4723, 'E8717': 4724, '81382': 4725, '4431': 4726, '75733': 4727, '34402': 4728, '41072': 4729, '42293': 4730, '3971': 4731, '27953': 4732, '75671': 4733, '24981': 4734, '2690': 4735, '01405': 4736, '85310': 4737, '82132': 4738, '52410': 4739, '7540': 4740, '73600': 4741, '34430': 4742, '49301': 4743, '76513': 4744, '9149': 4745, '515': 4746, '3829': 4747, '74900': 4748, '2920': 4749, '90453': 4750, '7866': 4751, '71213': 4752, '64201': 4753, '7366': 4754, '88020': 4755, '9173': 4756, '99602': 4757, '36232': 4758, '81101': 4759, '70400': 4760, '45119': 4761, 'E8196': 4762, '44283': 4763, '78904': 4764, '37275': 4765, '7623': 4766, 'V551': 4767, '5710': 4768, '24220': 4769, '25801': 4770, '7539': 4771, 'E8790': 4772, '05821': 4773, '3017': 4774, '86102': 4775, 'V1272': 4776, '78651': 4777, '36202': 4778, '83303': 4779, '72751': 4780, '9518': 4781, '8025': 4782, '79923': 4783, '1460': 4784, '2749': 4785, '64303': 4786, '71848': 4787, 'V140': 4788, '95200': 4789, '6192': 4790, '9680': 4791, '32725': 4792, '3370': 4793, '86400': 4794, '99693': 4795, '64224': 4796, '67401': 4797, '29282': 4798, '28240': 4799, 'V5331': 4800, '29560': 4801, '0039': 4802, '87344': 4803, '38610': 4804, '0030': 4805, '4710': 4806, '4466': 4807, '6823': 4808, '43391': 4809, '2540': 4810, '7513': 4811, '53101': 4812, 'V6284': 4813, '2819': 4814, 'E8258': 4815, '60001': 4816, '7589': 4817, '71694': 4818, '1648': 4819, '4552': 4820, 'E8583': 4821, '28521': 4822, '79029': 4823, '5260': 4824, '38870': 4825, '78947': 4826, 'E9460': 4827, 'V5399': 4828, '82331': 4829, '80620': 4830, '33394': 4831, '31239': 4832, '71693': 4833, '49381': 4834, '9092': 4835, '1915': 4836, '82521': 4837, '90221': 4838, '9273': 4839, '28801': 4840, '37751': 4841, 'E9507': 4842, 'E9581': 4843, '2409': 4844, '6149': 4845, '71943': 4846, '3076': 4847, '7831': 4848, 'E9801': 4849, '70700': 4850, '70710': 4851, '1976': 4852, '71821': 4853, '4414': 4854, '0319': 4855, '07989': 4856, '6218': 4857, '07059': 4858, '3310': 4859, '30530': 4860, '0383': 4861, '2939': 4862, '8760': 4863, '1712': 4864, '71596': 4865, 'V1088': 4866, '05410': 4867, '94126': 4868, '3182': 4869, '2189': 4870, '99592': 4871, 'V1506': 4872, '7872': 4873, '4510': 4874, '83903': 4875, '7845': 4876, '78865': 4877, 'V707': 4878, '7863': 4879, '2858': 4880, '20037': 4881, '99603': 4882, 'E964': 4883, '42651': 4884, '4478': 4885, '2706': 4886, 'E9389': 4887, '7961': 4888, '6269': 4889, '6259': 4890, '03641': 4891, '87320': 4892, '36241': 4893, '7949': 4894, '94522': 4895, '79539': 4896, '80476': 4897, '49120': 4898, 'V6289': 4899, '55100': 4900, '41090': 4901, 'E8238': 4902, 'E9295': 4903, '7423': 4904, 'E8842': 4905, '28951': 4906, '25032': 4907, '7079': 4908, '70724': 4909, '3961': 4910, '74761': 4911, '83411': 4912, '8678': 4913, '87353': 4914, '75555': 4915, '37515': 4916, '30501': 4917, '64803': 4918, '80702': 4919, '2765': 4920, 'V0251': 4921, '7265': 4922, '7030': 4923, '36523': 4924, '75481': 4925, '34292': 4926, '37230': 4927, 'E9398': 4928, '5641': 4929, '78604': 4930, '99678': 4931, '8603': 4932, '3090': 4933, '86401': 4934, '1140': 4935, '9618': 4936, '53251': 4937, 'E8694': 4938, '3594': 4939, '80710': 4940, '87361': 4941, '83942': 4942, '7761': 4943, '57441': 4944, '87351': 4945, 'E9192': 4946, '29410': 4947, '8670': 4948, 'V175': 4949, '79389': 4950, '2899': 4951, '7105': 4952, '9598': 4953, 'V6104': 4954, '7592': 4955, '6080': 4956, '75531': 4957, '9094': 4958, '71595': 4959, '59382': 4960, '30016': 4961, '0470': 4962, '30560': 4963, '7627': 4964, '36470': 4965, '57431': 4966, '99591': 4967, 'V561': 4968, '34701': 4969, '6248': 4970, '80341': 4971, '41011': 4972, '3360': 4973, '4263': 4974, '6179': 4975, '78039': 4976, '9035': 4977, '36812': 4978, '80505': 4979, '0338': 4980, 'V4382': 4981, '41091': 4982, 'V642': 4983, 'V1271': 4984, '2302': 4985, '9614': 4986, 'E9451': 4987, '78704': 4988, '76383': 4989, '48249': 4990, '7217': 4991, '07044': 4992, '32721': 4993, 'V052': 4994, '2558': 4995, '8449': 4996, '1974': 4997, '81404': 4998, '1987': 4999, '2118': 5000, 'V3000': 5001, '53130': 5002, '4472': 5003, '71237': 5004, '8821': 5005, '1888': 5006, '20402': 5007, '7515': 5008, 'V632': 5009, '43401': 5010, '71535': 5011, '75689': 5012, '52469': 5013, '86504': 5014, '85304': 5015, 'V610': 5016, '5609': 5017, '34201': 5018, '86101': 5019, 'E887': 5020, '4238': 5021, '80076': 5022, '0382': 5023, '75569': 5024, '7011': 5025, '8783': 5026, '74681': 5027, '72813': 5028, '01325': 5029, '82121': 5030, '85220': 5031, '2548': 5032, '80102': 5033, 'V040': 5034, '80606': 5035, '42491': 5036, '51889': 5037, '27541': 5038, '42822': 5039, '36275': 5040, '38582': 5041, '44381': 5042, '70704': 5043, '2308': 5044, '5368': 5045, '75317': 5046, '5172': 5047, '035': 5048, '88122': 5049, '80701': 5050, 'V1021': 5051, '7678': 5052, '2729': 5053, '7964': 5054, '75432': 5055, 'V8521': 5056, '73729': 5057, 'V6442': 5058, '42841': 5059, '36040': 5060, '00863': 5061, '0490': 5062, '2911': 5063, '7706': 5064, '7873': 5065, '81314': 5066, 'V442': 5067, '5739': 5068, '64934': 5069, '9658': 5070, '64682': 5071, '56489': 5072, '0090': 5073, '74361': 5074, '80061': 5075, 'V138': 5076, '7801': 5077, '71958': 5078, '68601': 5079, '66622': 5080, '60785': 5081, '76517': 5082, '2354': 5083, '94401': 5084, '30543': 5085, '6185': 5086, '53450': 5087, '78652': 5088, '35781': 5089, '72762': 5090, '87365': 5091, '43491': 5092, '71615': 5093, 'E8187': 5094, '38910': 5095, '76502': 5096, '3004': 5097, '34700': 5098, '538': 5099, '94425': 5100, '7593': 5101, 'V6107': 5102, '04102': 5103, '7907': 5104, 'V239': 5105, '65971': 5106, '9627': 5107, '80101': 5108, '8409': 5109, '49300': 5110, '3499': 5111, 'E9447': 5112, '87340': 5113, '4293': 5114, '71901': 5115, '57430': 5116, '5518': 5117, '8073': 5118, '73672': 5119, 'E918': 5120, 'E969': 5121, '6101': 5122, '9393': 5123, '80020': 5124, '78899': 5125, '37852': 5126, '64293': 5127, '43331': 5128, '7636': 5129, '8300': 5130, '88029': 5131, 'V0179': 5132, '78829': 5133, '80121': 5134, '4262': 5135, '7892': 5136, '73719': 5137, '80160': 5138, '75500': 5139, 'E9441': 5140, '7779': 5141, '5855': 5142, '57490': 5143, '1950': 5144, '25052': 5145, '3320': 5146, '90241': 5147, '34282': 5148, '20005': 5149, '9916': 5150, '59655': 5151, '2980': 5152, '6190': 5153, '53500': 5154, '1369': 5155, '1922': 5156, '5244': 5157, '5772': 5158, '9733': 5159, '80152': 5160, 'V8545': 5161, '1519': 5162, '22804': 5163, '2592': 5164, '7602': 5165, '78820': 5166, '6160': 5167, '99674': 5168, 'E8160': 5169, '4551': 5170, '56723': 5171, '9348': 5172, 'E8231': 5173, '9752': 5174, '71230': 5175, '75983': 5176, '77431': 5177, '69589': 5178, 'V4284': 5179, '6182': 5180, '5716': 5181, '0218': 5182, '3839': 5183, '51883': 5184, '30593': 5185, '2825': 5186, '37991': 5187, '8621': 5188, 'E9305': 5189, '9352': 5190, '2167': 5191, '25070': 5192, '8716': 5193, '5122': 5194, 'V6282': 5195, '9531': 5196, '65261': 5197, '20280': 5198, '59984': 5199, '33379': 5200, '1963': 5201, '7534': 5202, '5921': 5203, '56032': 5204, '55000': 5205, 'E9170': 5206, '85196': 5207, '52330': 5208, '71515': 5209, '80110': 5210, '27651': 5211, '6173': 5212, '71534': 5213, '99688': 5214, 'V1581': 5215, '78931': 5216, 'V6110': 5217, '29389': 5218, '80400': 5219, '9964': 5220, '28802': 5221, '3950': 5222, '5307': 5223, '81252': 5224, '6073': 5225, '81393': 5226, '29189': 5227, '36002': 5228, '90089': 5229, '53440': 5230, '47819': 5231, '78551': 5232, '0839': 5233, 'V311': 5234, '9722': 5235, 'E9361': 5236, '13102': 5237, 'V5410': 5238, '80607': 5239, '64891': 5240, '38200': 5241, '7703': 5242, '29621': 5243, '80707': 5244, '78724': 5245, '53210': 5246, '27400': 5247, '90253': 5248, 'E9310': 5249, '81603': 5250, '90003': 5251, '72939': 5252, 'E8581': 5253, '33811': 5254, '28809': 5255, 'V434': 5256, '9080': 5257, '72782': 5258, '99670': 5259, '52400': 5260, '71216': 5261, '86351': 5262, '72789': 5263, '0031': 5264, '73024': 5265, '4841': 5266, '37210': 5267, '2550': 5268, 'V292': 5269, '00869': 5270, 'V638': 5271, '95909': 5272, '20158': 5273, '78721': 5274, 'E8789': 5275, '2252': 5276, '4464': 5277, '01330': 5278, '2149': 5279, '4270': 5280, '99585': 5281, '4253': 5282, '96971': 5283, 'V1255': 5284, '83661': 5285, '37921': 5286, '66602': 5287, '78442': 5288, '9524': 5289, '9528': 5290, '34989': 5291, '42682': 5292, '5964': 5293, 'E9203': 5294, '25512': 5295, '86329': 5296, '78833': 5297, '3101': 5298, '63512': 5299, 'E8708': 5300, '9720': 5301, '75679': 5302, '096': 5303, '80131': 5304, '78834': 5305, '3573': 5306, '7820': 5307, '80080': 5308, '3588': 5309, '42821': 5310, 'E9580': 5311, '80629': 5312, '8710': 5313, '4149': 5314, '8021': 5315, '9578': 5316, 'V1351': 5317, '75479': 5318, '29574': 5319, '5963': 5320, '6824': 5321, '87359': 5322, 'E8132': 5323, '30002': 5324, '32082': 5325, 'E8555': 5326, '3093': 5327, 'V703': 5328, '37702': 5329, 'V142': 5330, '9019': 5331, '5290': 5332, '80239': 5333, '7051': 5334, '94232': 5335, '30500': 5336, '20080': 5337, '37886': 5338, 'V5416': 5339, '2638': 5340, '2180': 5341, '591': 5342, '80623': 5343, '6962': 5344, 'V4611': 5345, '243': 5346, '53540': 5347, '500': 5348, '81102': 5349, '04082': 5350, '80639': 5351, '45183': 5352, '52689': 5353, 'V1302': 5354, '0578': 5355, '7213': 5356, 'E9821': 5357, 'V4972': 5358, '4846': 5359, 'V1011': 5360, '5881': 5361, '1528': 5362, '78841': 5363, '0794': 5364, '81322': 5365, '81402': 5366, '5781': 5367, '78902': 5368, '3109': 5369, '74561': 5370, '38589': 5371, '8029': 5372, '5810': 5373, '20412': 5374, '30441': 5375, '4240': 5376, '6826': 5377, '2893': 5378, '29630': 5379, '4571': 5380, '8797': 5381, '76719': 5382, '78096': 5383, 'V166': 5384, '86344': 5385, '3501': 5386, '5539': 5387, '70722': 5388, 'V2651': 5389, '78722': 5390, '7237': 5391, '2370': 5392, '5990': 5393, '7385': 5394, '73730': 5395, '53390': 5396, '44022': 5397, '30480': 5398, '73382': 5399, '53640': 5400, '71102': 5401, 'V8535': 5402, '9340': 5403, '2126': 5404, '1643': 5405, '30120': 5406, '76406': 5407, 'V1091': 5408, '20510': 5409, '9596': 5410, '82139': 5411, '82002': 5412, 'V4363': 5413, '8072': 5414, '3349': 5415, 'E8508': 5416, 'E030': 5417, '81352': 5418, 'V443': 5419, '1973': 5420, '7587': 5421, 'E9194': 5422, '0362': 5423, 'E8843': 5424, '4588': 5425, '1579': 5426, '92721': 5427, '20040': 5428, '4267': 5429, '20500': 5430, '76498': 5431, 'E9338': 5432, '72741': 5433, '60789': 5434, '9249': 5435, 'V1052': 5436, '586': 5437, '70219': 5438, '33392': 5439, '3432': 5440, '86132': 5441, '3082': 5442, '60783': 5443, '05829': 5444, '4550': 5445, '7764': 5446, '80012': 5447, '30181': 5448, '36189': 5449, '8469': 5450, 'E0011': 5451, '83801': 5452, '9309': 5453, '45989': 5454, '7289': 5455, '7226': 5456, '20380': 5457, '9056': 5458, '0970': 5459, '70723': 5460, '1625': 5461, '44324': 5462, '29285': 5463, '1211': 5464, '29575': 5465, 'E9300': 5466, 'V1002': 5467, '71109': 5468, '7929': 5469, '6851': 5470, '75522': 5471, '8798': 5472, '2891': 5473, '5306': 5474, '53191': 5475, '29699': 5476, '4110': 5477, '60010': 5478, '7732': 5479, '99851': 5480, 'E0070': 5481, 'V271': 5482, '7104': 5483, '41081': 5484, '99529': 5485, '43381': 5486, '2809': 5487, '7841': 5488, '36512': 5489, '27803': 5490, '5754': 5491, 'V1652': 5492, 'V1552': 5493, 'V6142': 5494, '27952': 5495, '99731': 5496, '20033': 5497, '53111': 5498, 'E0039': 5499, '99749': 5500, '6019': 5501, '4254': 5502, '81354': 5503, '8280': 5504, '33821': 5505, '80421': 5506, 'E8839': 5507, '3368': 5508, '90222': 5509, '9099': 5510, '81513': 5511, '44323': 5512, '86389': 5513, '4533': 5514, '27731': 5515, '86404': 5516, '30550': 5517, '87402': 5518, '3558': 5519, '9114': 5520, 'V8811': 5521, '53341': 5522, 'E8002': 5523, '44029': 5524, '2325': 5525, '73026': 5526, '3371': 5527, 'V1643': 5528, 'E8156': 5529, 'V604': 5530, '30300': 5531, '53019': 5532, '73019': 5533, '2218': 5534, 'E8764': 5535, '80500': 5536, '94420': 5537, '41021': 5538, '7380': 5539, '7885': 5540, '83652': 5541, '7018': 5542, 'V183': 5543, '2561': 5544, '80221': 5545, '1737': 5546, 'E9589': 5547, '61804': 5548, '53141': 5549, '1498': 5550, '3581': 5551, 'V1588': 5552, '7520': 5553, '64244': 5554, '86810': 5555, '86352': 5556, '4802': 5557, '0971': 5558, '83805': 5559, '65641': 5560, 'V1365': 5561, '35971': 5562, '3249': 5563, '72401': 5564, '3014': 5565, 'V8523': 5566, '3336': 5567, '5995': 5568, '7990': 5569, 'V1085': 5570, '25200': 5571, 'V1046': 5572, 'E9654': 5573, '5239': 5574, '53511': 5575, 'V4975': 5576, 'E8501': 5577, '4719': 5578, '7775': 5579, '81504': 5580, '9229': 5581, '2931': 5582, '45341': 5583, '71970': 5584, '38842': 5585, '09152': 5586, 'V0262': 5587, '71233': 5588, '60883': 5589, '29043': 5590, '8671': 5591, '72252': 5592, '1716': 5593, '36544': 5594, '80176': 5595, 'V440': 5596, '36815': 5597, '57460': 5598, '45384': 5599, '79901': 5600, '5932': 5601, '5715': 5602, '2110': 5603, '9034': 5604, '76077': 5605, '9392': 5606, 'V502': 5607, 'E8710': 5608, '0543': 5609, '43820': 5610, 'V109': 5611, '3918': 5612, '23874': 5613, '1478': 5614, '7946': 5615, '4731': 5616, '80376': 5617, '85141': 5618, 'E9328': 5619, '7231': 5620, '28861': 5621, '74783': 5622, 'V0254': 5623, 'E8126': 5624, '5853': 5625, '72992': 5626, '5822': 5627, '7108': 5628, '63401': 5629, 'V1741': 5630, '72700': 5631, '7019': 5632, '135': 5633, '81321': 5634, '88101': 5635, '0312': 5636, 'E8704': 5637, '70720': 5638, '2104': 5639, '01300': 5640, 'V8542': 5641, '9610': 5642, '36570': 5643, '9116': 5644, '56882': 5645, '3671': 5646, '5070': 5647, '74687': 5648, '4732': 5649, '87373': 5650, '4611': 5651, '83660': 5652, '74442': 5653, '7069': 5654, '1580': 5655, '85103': 5656, '99653': 5657, '25083': 5658, '3449': 5659, '56969': 5660, 'V431': 5661, '9950': 5662, '6203': 5663, 'V4960': 5664, 'V552': 5665, '3220': 5666, '9839': 5667, 'E9357': 5668, '42731': 5669, '1551': 5670, 'V9089': 5671, '4718': 5672, '9551': 5673, '17342': 5674, '64231': 5675, '04101': 5676, '56942': 5677, '52809': 5678, '75320': 5679, '71238': 5680, '92301': 5681, '4408': 5682, '1975': 5683, '65414': 5684, '2300': 5685, 'E8496': 5686, 'E9178': 5687, 'V5413': 5688, '1723': 5689, '0416': 5690, 'E9505': 5691, '72706': 5692, '63592': 5693, 'V8709': 5694, 'E8138': 5695, '73390': 5696, '5804': 5697, '78099': 5698, '1514': 5699, 'E9410': 5700, 'V461': 5701, '5160': 5702, '92619': 5703, 'E8348': 5704, '80316': 5705, '64111': 5706, '2862': 5707, '79021': 5708, '71918': 5709, 'E9394': 5710, '7142': 5711, '49382': 5712, '75311': 5713, '6002': 5714, '2130': 5715, '20951': 5716, '4291': 5717, '78650': 5718, '25051': 5719, '73005': 5720, 'V4289': 5721, '85204': 5722, '5564': 5723, '96905': 5724, '1893': 5725, '03843': 5726, 'E8796': 5727, '30781': 5728, '72273': 5729, '43410': 5730, '9213': 5731, '1986': 5732, '85306': 5733, '8170': 5734, '8075': 5735, 'V4581': 5736, 'V672': 5737, '4413': 5738, '2123': 5739, '27700': 5740, '57149': 5741, '75322': 5742, '78937': 5743, '30400': 5744, '3348': 5745, '88019': 5746, '75026': 5747, '29690': 5748, '72704': 5749, '2848': 5750, '1503': 5751, '0272': 5752, '9691': 5753, '4590': 5754, '4619': 5755, '2571': 5756, '78900': 5757, 'E9500': 5758, '3411': 5759, '27950': 5760, '4421': 5761, 'V122': 5762, '64864': 5763, '5672': 5764, '4470': 5765, '267': 5766, '76401': 5767, '4257': 5768, '07998': 5769, '73309': 5770, '76515': 5771, '1918': 5772, '9584': 5773, '20301': 5774, '99631': 5775, '75314': 5776, '76524': 5777, 'V433': 5778, '20780': 5779, '34511': 5780, 'V6406': 5781, '4779': 5782, '5829': 5783, '9510': 5784, '3341': 5785, '2391': 5786, '75732': 5787, '08249': 5788, '80662': 5789, '20030': 5790, '86394': 5791, '2903': 5792, '4940': 5793, '17331': 5794, '5581': 5795, 'E8062': 5796, '1505': 5797, '9038': 5798, '3100': 5799, '00581': 5800, '9172': 5801, '71500': 5802, '73018': 5803, '28863': 5804, '6271': 5805, '76504': 5806, '77430': 5807, '1448': 5808, 'V151': 5809, '73393': 5810, '20930': 5811, '0980': 5812, '4829': 5813, '70707': 5814, '37430': 5815, '03841': 5816, '30113': 5817, '2531': 5818, '31289': 5819, 'E899': 5820, '64804': 5821, '9331': 5822, 'E8844': 5823, '73006': 5824, '7799': 5825, '6142': 5826, '41402': 5827, '9013': 5828, '34440': 5829, '99881': 5830, '3207': 5831, '29530': 5832, '9947': 5833, '3674': 5834, '2358': 5835, '56089': 5836, '3595': 5837, 'E8703': 5838, '5718': 5839, '6272': 5840, '36210': 5841, '53150': 5842, '9755': 5843, '75489': 5844, '45377': 5845, 'E9430': 5846, 'E8151': 5847, '80851': 5848, '7441': 5849, 'V161': 5850, '4475': 5851, '9579': 5852, 'E9370': 5853, '4959': 5854, '99652': 5855, '1104': 5856, '25033': 5857, 'V118': 5858, '84500': 5859, '2720': 5860, '35782': 5861, '2395': 5862, 'V4582': 5863, '86812': 5864, '932': 5865, '5678': 5866, '72251': 5867, '8248': 5868, '3089': 5869, '0417': 5870, '3393': 5871, '99771': 5872, '29512': 5873, '2356': 5874, '5130': 5875, '20600': 5876, '6164': 5877, '42612': 5878, '1838': 5879, '5370': 5880, '06642': 5881, '80006': 5882, '4400': 5883, '00804': 5884, 'V5843': 5885, '53641': 5886, '7500': 5887, '72191': 5888, '85301': 5889, '9571': 5890, '7796': 5891, 'V1261': 5892, '5730': 5893, '138': 5894, '04185': 5895, 'E9299': 5896, 'V1859': 5897, '90081': 5898, '8244': 5899, 'E8709': 5900, '72400': 5901, '71697': 5902, '1966': 5903, '73710': 5904, '0318': 5905, '0993': 5906, '1972': 5907, '53531': 5908, '4281': 5909, '4351': 5910, '9020': 5911, 'V4577': 5912, '1103': 5913, '78459': 5914, 'V694': 5915, '7948': 5916, '74510': 5917, '80146': 5918, '04109': 5919, '25023': 5920, '53089': 5921, '5981': 5922, '44382': 5923, '5118': 5924, '30301': 5925, '4555': 5926, '82110': 5927, '99831': 5928, '56200': 5929, '1400': 5930, '4375': 5931, 'E8127': 5932, '2739': 5933, '4556': 5934, '4352': 5935, '1883': 5936, 'V8812': 5937, '42823': 5938, '8860': 5939, '20511': 5940, '2386': 5941, '1727': 5942, '82525': 5943, '4572': 5944, '0778': 5945, '76516': 5946, '7295': 5947, '2281': 5948, 'E0139': 5949, '5552': 5950, '7135': 5951, '3555': 5952, 'E9334': 5953, '94524': 5954, '8601': 5955, 'V626': 5956, '8088': 5957, '2141': 5958, '6805': 5959, '5568': 5960, '45182': 5961, '431': 5962, '36441': 5963, 'V1047': 5964, '48230': 5965, '6258': 5966, '67014': 5967, 'V061': 5968, '7836': 5969, '99564': 5970, '95899': 5971, '35571': 5972, 'V141': 5973, '52406': 5974, '43830': 5975, '36900': 5976, 'E8200': 5977, '29381': 5978, '72982': 5979, '64761': 5980, '33819': 5981, '2820': 5982, '36003': 5983, '78843': 5984, '20891': 5985, '3484': 5986, '56039': 5987, 'E8609': 5988, '5088': 5989, '9529': 5990, '6118': 5991, '69551': 5992, 'V1251': 5993, '2397': 5994, '88002': 5995, '3804': 5996, '86340': 5997, '42511': 5998, '51851': 5999, '7809': 6000, '99661': 6001, '34570': 6002, '1725': 6003, '1800': 6004, '4830': 6005, 'E9673': 6006, '1398': 6007, 'E8603': 6008, '2864': 6009, 'V1659': 6010, '83900': 6011, '0398': 6012, '5224': 6013, '7464': 6014, '3708': 6015, '9832': 6016, '77088': 6017, 'E8188': 6018, 'V195': 6019, '80435': 6020, '2730': 6021, '30563': 6022, '69514': 6023, '3911': 6024, 'E9433': 6025, '8972': 6026, '86403': 6027, '64831': 6028, '4255': 6029, '9539': 6030, '1532': 6031, '2355': 6032, '64623': 6033, '8782': 6034, '71903': 6035, '37556': 6036, '64894': 6037, '78055': 6038, '1943': 6039, '36500': 6040, '05413': 6041, '65441': 6042, '35989': 6043, '40210': 6044, '6232': 6045, 'E8735': 6046, '78600': 6047, '99762': 6048, '0059': 6049, '5191': 6050, '34409': 6051, '51882': 6052, '82301': 6053, '3940': 6054, '33818': 6055, '74762': 6056, '3523': 6057, '4474': 6058, '7294': 6059, '6221': 6060, '7200': 6061, '70719': 6062, '3709': 6063, 'E9585': 6064, 'V549': 6065, '8799': 6066, '80026': 6067, '7847': 6068, 'E8795': 6069, '82522': 6070, '5163': 6071, '41410': 6072, '3452': 6073, '83100': 6074, '700': 6075, '3951': 6076, '78261': 6077, '4150': 6078, '6391': 6079, 'E915': 6080, '2773': 6081, '90140': 6082, 'V1082': 6083, '3300': 6084, '73711': 6085, '30440': 6086, '37882': 6087, '6983': 6088, '74511': 6089, '73722': 6090, '29562': 6091, '92309': 6092, '8442': 6093, '0796': 6094, '73028': 6095, '20278': 6096, '2115': 6097, '8629': 6098, '8056': 6099, 'E856': 6100, 'E8797': 6101, 'E8588': 6102, '7837': 6103, '25202': 6104, '77189': 6105, '5186': 6106, '71906': 6107, '1175': 6108, '7041': 6109, '1208': 6110, '7634': 6111, 'E8163': 6112, '07053': 6113, '3301': 6114, '80372': 6115, '69550': 6116, '71888': 6117, '2707': 6118, '36646': 6119, '20979': 6120, 'E9506': 6121, '9913': 6122, '59371': 6123, '1414': 6124, '71650': 6125, '5760': 6126, '9778': 6127, '7590': 6128, '7913': 6129, '8911': 6130, '46430': 6131, '2798': 6132, '185': 6133, '76384': 6134, '1461': 6135, '28804': 6136, '25001': 6137, '8055': 6138, 'E9211': 6139, '5680': 6140, 'V8543': 6141, '2884': 6142, 'E8809': 6143, '53550': 6144, '90142': 6145, '7450': 6146, '5800': 6147, '56971': 6148, '1735': 6149, 'E8495': 6150, '20201': 6151, 'V4572': 6152, 'V2541': 6153, '1251': 6154, '9617': 6155, '66604': 6156, '7611': 6157, '64263': 6158, 'V1071': 6159, '81221': 6160, '5374': 6161, 'E8748': 6162, '75010': 6163, '85121': 6164, '63412': 6165, '28243': 6166, '85180': 6167, 'V741': 6168, '29623': 6169, '90233': 6170, '7138': 6171, '70703': 6172, '87350': 6173, '514': 6174, 'V536': 6175, '1958': 6176, 'E9393': 6177, '85210': 6178, '2810': 6179, '95919': 6180, '59651': 6181, '311': 6182, '1745': 6183, '1100': 6184, '2443': 6185, 'E8585': 6186, '64913': 6187, '92420': 6188, '47821': 6189, 'E9620': 6190, '2725': 6191, '9721': 6192, '6963': 6193, '3591': 6194, '80841': 6195, '85401': 6196, '72664': 6197, '82310': 6198, '30493': 6199, '4582': 6200, '07810': 6201, 'E9579': 6202, '71699': 6203, '78901': 6204, 'V1005': 6205, '01194': 6206, '0949': 6207, '66561': 6208, '76506': 6209, '53012': 6210, '7742': 6211, '9623': 6212, '99800': 6213, '96569': 6214, '684': 6215, '9570': 6216, '5798': 6217, '44773': 6218, '32362': 6219, '4131': 6220, '80441': 6221, 'E9249': 6222, '5755': 6223, '7963': 6224, 'E882': 6225, 'E9530': 6226, '7817': 6227, '76519': 6228, '73025': 6229, '01123': 6230, '53560': 6231, 'V5049': 6232, 'E9340': 6233, '7771': 6234, '47875': 6235, '8065': 6236, '37887': 6237, '77018': 6238, 'V618': 6239, '78062': 6240, '86343': 6241, '5559': 6242, 'V8409': 6243, 'E8918': 6244, '0520': 6245, '52510': 6246, '3007': 6247, '53561': 6248, '1541': 6249, '30022': 6250, '74259': 6251, 'V1851': 6252, '7582': 6253, '24911': 6254, '2630': 6255, '8242': 6256, '66524': 6257, '5120': 6258, '79409': 6259, 'V1051': 6260, '7728': 6261, '2726': 6262, '28260': 6263, '8730': 6264, '87323': 6265, '60784': 6266, '82122': 6267, '48240': 6268, '7458': 6269, '80350': 6270, '6869': 6271, '7625': 6272, 'V289': 6273, '33709': 6274, '74601': 6275, '2888': 6276, 'E9504': 6277, '99642': 6278, '29382': 6279, '3485': 6280, '1569': 6281, '5848': 6282, '53782': 6283, '7868': 6284, '7789': 6285, '6959': 6286, '53250': 6287, '3941': 6288, '8751': 6289, 'V1089': 6290, 'E8587': 6291, '30562': 6292, '3590': 6293, '5550': 6294, '319': 6295, '9042': 6296, '86510': 6297, '09181': 6298, '82322': 6299, 'V6549': 6300, '78903': 6301, '55200': 6302, '57471': 6303, '20002': 6304, '58181': 6305, '5967': 6306, 'V625': 6307, 'V463': 6308, '42831': 6309, '55011': 6310, '585': 6311, '29590': 6312, '4589': 6313, '81323': 6314, '4738': 6315, '58089': 6316, '9190': 6317, '7038': 6318, '9949': 6319, '75440': 6320, '3862': 6321, '1430': 6322, '59970': 6323, '8872': 6324, '81110': 6325, '71591': 6326, '7823': 6327, '80174': 6328, '34280': 6329, '79551': 6330, '37955': 6331, '1951': 6332, 'V861': 6333, 'E9502': 6334, '73314': 6335, '34461': 6336, '8605': 6337, '36230': 6338, '81303': 6339, '85181': 6340, '86122': 6341, '80229': 6342, 'V198': 6343, '34591': 6344, '5942': 6345, '7542': 6346, '2111': 6347, '82022': 6348, '7955': 6349, '0392': 6350, '2984': 6351, '77982': 6352, '9753': 6353, '79431': 6354, '94332': 6355, '34200': 6356, '5832': 6357, '3441': 6358, 'V4561': 6359, 'V9039': 6360, '2912': 6361, '7887': 6362, '7210': 6363, '99684': 6364, '7766': 6365, '83907': 6366, '72632': 6367, '64914': 6368, 'E9363': 6369, '73329': 6370, '462': 6371, '9095': 6372, '9125': 6373, '79095': 6374, '95202': 6375, '6822': 6376, '58081': 6377, 'V1369': 6378, '35789': 6379, '8208': 6380, '78094': 6381, '1599': 6382, '83313': 6383, '33822': 6384, '7310': 6385, '07999': 6386, 'V503': 6387, '82390': 6388, '2164': 6389, '3548': 6390, '74860': 6391, '56481': 6392, '99687': 6393, '4578': 6394, '19889': 6395, '05329': 6396, '5081': 6397, '9067': 6398, '76512': 6399, '2353': 6400, '25011': 6401, '86612': 6402, '59373': 6403, '33829': 6404, '81103': 6405, '6940': 6406, '1769': 6407, '60499': 6408, '5185': 6409, '0491': 6410, '90225': 6411, '56202': 6412, '2702': 6413, '9831': 6414, '5909': 6415, '27502': 6416, '53789': 6417, '7758': 6418, '2367': 6419, '20220': 6420, '04590': 6421, '72409': 6422, '36106': 6423, 'V252': 6424, '7685': 6425, '84509': 6426, '7670': 6427, '40401': 6428, '496': 6429, '7262': 6430, '8249': 6431, '3313': 6432, '82302': 6433, '3240': 6434, 'E8538': 6435, '80112': 6436, '7721': 6437, '73016': 6438, '3488': 6439, '7149': 6440, '20961': 6441, '27489': 6442, '5845': 6443, '64681': 6444, '2119': 6445, 'E0071': 6446, 'E9348': 6447, '2594': 6448, '8260': 6449, '7514': 6450, '43301': 6451, '85173': 6452, '5078': 6453, '3556': 6454, '9800': 6455, '4171': 6456, '72885': 6457, 'V1584': 6458, 'V8741': 6459, '80227': 6460, 'E8706': 6461, '390': 6462, '37603': 6463, '7856': 6464, '80507': 6465, '2169': 6466, '2599': 6467, '07041': 6468, '2120': 6469, '9233': 6470, '43831': 6471, '78932': 6472, 'V111': 6473, '4568': 6474, '2764': 6475, '5856': 6476, '23331': 6477, 'V4365': 6478, '43330': 6479, '75530': 6480, '1732': 6481, '6150': 6482, 'E9496': 6483, '5809': 6484, '49121': 6485, '73810': 6486, '64621': 6487, '72889': 6488, '85300': 6489, '3732': 6490, 'E8260': 6491, '1702': 6492, '1598': 6493, '29181': 6494, '86603': 6495, '37189': 6496, '67204': 6497, '90210': 6498, '20501': 6499, '20208': 6500, '36221': 6501, '85202': 6502, '5812': 6503, '79955': 6504, 'V446': 6505, '94335': 6506, '5183': 6507, '73743': 6508, '2530': 6509, '36213': 6510, '85101': 6511, 'E9278': 6512, '72703': 6513, '8796': 6514, '80842': 6515, '5082': 6516, '25201': 6517, '0389': 6518, '64121': 6519, '3769': 6520, 'E8490': 6521, '6039': 6522, 'V5412': 6523, 'V4976': 6524, '8630': 6525, '65411': 6526, '80016': 6527, '90001': 6528, '79929': 6529, '78492': 6530, '53300': 6531, '7061': 6532, 'E0291': 6533, '8831': 6534, '193': 6535, '9773': 6536, '81240': 6537, '77087': 6538, '41082': 6539, '80002': 6540, '71815': 6541, 'E9331': 6542, 'E8504': 6543, '37520': 6544, '72990': 6545, 'V0950': 6546, '1543': 6547, '99809': 6548, '6000': 6549, '7624': 6550, '80119': 6551, '470': 6552, '07051': 6553, '40403': 6554, '53290': 6555, '85162': 6556, '11590': 6557, '83809': 6558, '2519': 6559, '85206': 6560, '9075': 6561, '9635': 6562, 'E8908': 6563, '4789': 6564, '8250': 6565, 'V1254': 6566, '7724': 6567, '2768': 6568, 'E9503': 6569, '9534': 6570, '99666': 6571, '7560': 6572, '81302': 6573, '34580': 6574, '59789': 6575, '7944': 6576, '1573': 6577, '4580': 6578, '9252': 6579, '3102': 6580, 'V5864': 6581, '1710': 6582, '5939': 6583, '82321': 6584, 'V4579': 6585, 'E9196': 6586, '05379': 6587, '29041': 6588, 'V011': 6589, '25020': 6590, '82300': 6591, '78832': 6592, '7895': 6593, '6191': 6594, '20210': 6595, '2153': 6596, '5571': 6597, '45389': 6598, '2468': 6599, 'E8242': 6600, '2949': 6601, 'E9550': 6602, '34442': 6603, '83209': 6604, 'V449': 6605, '86602': 6606, '80700': 6607, '1534': 6608, '5178': 6609, 'E959': 6610, 'V441': 6611, 'V5401': 6612, 'V5866': 6613, '92710': 6614, '4374': 6615, '05419': 6616, '7662': 6617, '72611': 6618, '99583': 6619, '7874': 6620, '2793': 6621, '7529': 6622, '2788': 6623, '78933': 6624, '5583': 6625, '33382': 6626, 'V4511': 6627, '2898': 6628, 'E9304': 6629, '38302': 6630, '63451': 6631, '99672': 6632, '9112': 6633, 'E9344': 6634, 'E9220': 6635, '28652': 6636, 'V469': 6637, 'V8489': 6638, '37635': 6639, '0912': 6640, '44489': 6641, 'E8210': 6642, '3340': 6643, '2533': 6644, '7088': 6645, '67451': 6646, '75502': 6647, '7132': 6648, '66911': 6649, '03812': 6650, '8704': 6651, '30393': 6652, '80336': 6653, '9559': 6654, 'E9443': 6655, '9308': 6656, '5902': 6657, '40491': 6658, 'V7219': 6659, '7618': 6660, '99667': 6661, 'E9678': 6662, '8708': 6663, '70702': 6664, '20530': 6665, '30470': 6666, '5579': 6667, '9211': 6668, '72973': 6669, '33183': 6670, '9828': 6671, '7098': 6672, '64883': 6673, '66934': 6674, '7321': 6675, '80022': 6676, '9086': 6677, '2799': 6678, 'E9248': 6679, '55091': 6680, '28851': 6681, '71985': 6682, 'E8541': 6683, '78559': 6684, 'V188': 6685, '24281': 6686, '99586': 6687, '6188': 6688, 'E8783': 6689, '5819': 6690, 'E9384': 6691, '1467': 6692, '9920': 6693, '37716': 6694, '32713': 6695, '76621': 6696, 'V0990': 6697, '20152': 6698, '37853': 6699, '30423': 6700, 'E8230': 6701, '3062': 6702, '86399': 6703, '99594': 6704, '9349': 6705, 'E8503': 6706, '1889': 6707, '9651': 6708, '1718': 6709, '53120': 6710, '68609': 6711, '20200': 6712, '3869': 6713, '5731': 6714, '52460': 6715, '5834': 6716, '53430': 6717, '0038': 6718, '27661': 6719, '2883': 6720, '80236': 6721, '9974': 6722, '7283': 6723, '80325': 6724, '5851': 6725, '99671': 6726, '53260': 6727, '64944': 6728, '5273': 6729, '72672': 6730, '75881': 6731, '1552': 6732, '5711': 6733, '6828': 6734, '45386': 6735, '52331': 6736, '34691': 6737, '74512': 6738, '4379': 6739, '55093': 6740, '7485': 6741, '0579': 6742, 'E8298': 6743, '1744': 6744, '570': 6745, '95208': 6746, '9530': 6747, '37239': 6748, '94128': 6749, '5690': 6750, '34120': 6751, '38619': 6752, '8970': 6753, 'E8902': 6754, '1530': 6755, 'E9397': 6756, '78720': 6757, '7509': 6758, '7774': 6759, '2520': 6760, '1890': 6761, 'V854': 6762, '64822': 6763, '80010': 6764, '0780': 6765, 'E9679': 6766, '55229': 6767, '6821': 6768, '56211': 6769, '9170': 6770, '81403': 6771, '11285': 6772, '99656': 6773, 'V435': 6774, '8850': 6775, '81343': 6776, '605': 6777, '37733': 6778, '8620': 6779, '3536': 6780, '5180': 6781, '1940': 6782, '81500': 6783, '3570': 6784, '95205': 6785, '7780': 6786, '8362': 6787, 'V0381': 6788, '80608': 6789, '7323': 6790, '99664': 6791, '30421': 6792, '38630': 6793, '0700': 6794, '74569': 6795, '5379': 6796, '55329': 6797, '29281': 6798, '73320': 6799, 'E8248': 6800, '20512': 6801, '87322': 6802, '52801': 6803, '49321': 6804, '85309': 6805, '7773': 6806, 'V1352': 6807, 'E8889': 6808, '0622': 6809, '81000': 6810, '3553': 6811, '57440': 6812, '81308': 6813, '90451': 6814, '3643': 6815, 'E9001': 6816, '6183': 6817, 'V660': 6818, '7470': 6819, '2740': 6820, '4266': 6821, '86330': 6822, 'V293': 6823, '8026': 6824, '20921': 6825, '88012': 6826, 'E8197': 6827, '76405': 6828, '41051': 6829, '74720': 6830, '260': 6831, '2940': 6832, '1518': 6833, '5724': 6834, 'V8537': 6835, 'V0739': 6836, '99647': 6837, 'E8181': 6838, '65101': 6839, '05311': 6840, '78461': 6841, '6970': 6842, '34291': 6843, '8910': 6844, '63572': 6845, 'E9429': 6846, '69513': 6847, '7813': 6848, '7569': 6849, '6961': 6850, '5929': 6851, '29592': 6852, '5961': 6853, '05889': 6854, '5309': 6855, '25510': 6856, '77082': 6857, '5200': 6858, '9130': 6859, '5110': 6860, '38906': 6861, '42742': 6862, '4232': 6863, 'V1083': 6864, '04183': 6865, '69581': 6866, '71690': 6867, '3749': 6868, '99604': 6869, '48281': 6870, '68111': 6871, 'E9019': 6872, 'E8042': 6873, '56986': 6874, '8973': 6875, '33372': 6876, '45387': 6877, '79510': 6878, 'E8217': 6879, '73097': 6880, '81307': 6881, '44621': 6882, '7744': 6883, '04186': 6884, '78001': 6885, '64884': 6886, '8470': 6887, '9072': 6888, '1624': 6889, '71105': 6890, '9951': 6891, 'E9346': 6892, '1844': 6893, '28800': 6894, '7422': 6895, '47831': 6896, '220': 6897, '4386': 6898, '42769': 6899, '64271': 6900, '1512': 6901, '95203': 6902, '88003': 6903, '81313': 6904, '53781': 6905, 'V4364': 6906, '1533': 6907, '7767': 6908, '01402': 6909, '45383': 6910, '75329': 6911, '75615': 6912, 'E0062': 6913, '340': 6914, '1149': 6915, '72871': 6916, '81341': 6917}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to create one hot encoding from a list of indices\n",
        "def create_one_hot(codes, code_to_index=code_to_index, code_count=code_count):\n",
        "    ohe = np.zeros(shape=(code_count,), dtype=\"ubyte\") # not sure if i should spec dtype\n",
        "    for code in codes:\n",
        "        ohe[code_to_index[code]] = 1\n",
        "    return ohe"
      ],
      "metadata": {
        "id": "rczjyxo0xxvi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test ohe create function\n",
        "d = data['ICD9_CODE'].iloc[0]\n",
        "ohe = create_one_hot(d)\n",
        "print(\"OHE sums to:\", np.sum(ohe), \"and data has length:\", len(d))\n",
        "print(\"First code:\", d[0], \n",
        "      \"corresponds to index:\", code_to_index[d[0]],\n",
        "      \"\\nOHE at that index:\", ohe[code_to_index[d[0]]])\n",
        "print(type(ohe[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRnZm1BgzJxg",
        "outputId": "2fbd057d-ebfe-4cf8-ce84-53bc6f3c6f53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OHE sums to: 16 and data has length: 16\n",
            "First code: 25013 corresponds to index: 3095 \n",
            "OHE at that index: 1\n",
            "<class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the texts as X and one hot encoding of labels to y\n",
        "X = [] # diagnosis notes\n",
        "y = [] # icd codes represented as multi-label OHE\n",
        "for i, row in data.iterrows():\n",
        "    codes = row['ICD9_CODE']\n",
        "    X.append(row['TEXT'])\n",
        "    y.append(create_one_hot(codes))\n",
        "print(\"Length of texts (X):\", len(X))\n",
        "y = np.asarray(y)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzYwWkL2uFw4",
        "outputId": "acb36e25-e706-48fc-e909-c3bb2df89caf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of texts (X): 52722\n",
            "y shape: (52722, 6918)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(data['TEXT'].iloc[0]) == type(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFH2TEtL4y7T",
        "outputId": "ba4eedb7-2946-4b46-e8c6-8f42b0112751"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create tokenized inputs\n",
        "- from above, stored as X (list of str from \"TEXT\" column of data)\n",
        "\n",
        "reference: https://www.youtube.com/watch?v=pjtnkCGElcE"
      ],
      "metadata": {
        "id": "c9SG0jTJ5RmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg6N6cXw5siO",
        "outputId": "e3ad0c56-1749-4be4-9b4f-a5b7c0d4d6b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "seq_len = 512\n",
        "num_samples = len(X)\n",
        "\n",
        "Xids = np.zeros((num_samples, seq_len))\n",
        "Xmask = np.zeros((num_samples, seq_len))\n",
        "\n",
        "Xids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYf7-rizc0s",
        "outputId": "4eeb8219-68a9-47eb-b771-1b249e1db351"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52722, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "lduHwcRUzy3P",
        "outputId": "e3dd7823-f4aa-4b33-a9a4-9fb5b79e2dbe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Admission Date:  [**2117-9-11**]              Discharge Date:   [**2117-9-17**]\\n\\nDate of Birth:  [**2082-3-21**]             Sex:   F\\n\\nService: MEDICINE\\n\\nAllergies:\\nLevaquin\\n\\nAttending:[**First Name3 (LF) 2195**]\\nChief Complaint:\\nnausea, vomiting\\n\\n\\nMajor Surgical or Invasive Procedure:\\nnone\\n\\nHistory of Present Illness:\\n35F w/ poorly controlled Type 1 diabetes mellitus w/ neuropathy,\\nnephropathy, HTN, gastroparesis, CKD and retinopathy, recently\\nhospitalized for orthostatic hypotension [**2-3**] autonomic\\nneuropathy [**Date range (1) 25088**]; DKA hospitalizations in [**6-12**] and [**7-12**], now\\nreturning w/ 5d history of worsening nausea, vomiting with\\ncoffee-ground emesis, chills, and dyspnea on exertion.  Last\\nweek she had a fall and hit her right face.  she also had 1 day\\nof diarrhea, which resolved early last week.  Found to be in DKA\\nwith AG 30 and bicarb 11.\\n.\\nIn the ED inital vitals were 09:00 0 98.2 113 181/99 22 100% RA.\\nK 4.7, HCO3 11, Anion Gap 30, Cr. 2.7 (baseline 1.6-2.0) She is\\non her 3rd L NS. Insulin srip at 5 units/hr. On home at 22\\nlevemir in am and 12 at with difficult to control sugars. BPs\\nhave been high. Given 30 mtroprolol tartrate in ED.\\n\\nShe was started on an insulin drip at 5 units/hr and 3L NS\\nboluses. Also aspirin 325mg PO and Morphine 4mg IVx1 for pain.\\nCXr was clear.  EKG NAD.\\n.\\nReview of systems: otherwise negative.\\n\\nPast Medical History:\\nType 1 diabetes mellitis w/ neuropathy, nephropathy, and\\nretinopathy - 2 episodes of DKA in [**6-12**] and [**7-12**]\\nHTN - 5 years\\ngastroparesis - 1.5 years\\nCKD - stage III, baseline Cr 2.4-2.5, proteinuria\\nL1 vertebral fracture - [**2117-7-17**]\\nSystolic ejection murmur\\n\\nSocial History:\\nPatient lives at home in [**Location (un) **] with her 8 y/o daughter and\\nboyfriend. She has no history of EtOH, tobacco, or illicit drug\\nuse. She is currently unemployed and seeking disability.\\n\\n\\nFamily History:\\nBoth parents have HTN and T2DM. Grandfather had an MI in his\\n40s.\\n\\nPhysical Exam:\\nGEN: Awake, alert, and oriented\\nHEENT: PERRLA. MMM. no JVD. neck supple. No cervical LAD\\nCards: RRR, S1/S2 normal. II/VI systolic ejection murmur heard\\nbest at the L upper sternal border.\\nPulm: CTABL with no crackles or wheezes.\\nAbd: BS+, soft, NT, no rebound/guarding, no HSM, no [**Doctor Last Name 515**]\\nsign\\nExtremities: wwp, no edema. radials, DPs, PTs 2+.\\nSkin: no rashes or bruising. no skin tenting.\\nNeuro: CNs II-XII intact. Upper extremities: Power [**5-6**]\\nbilaterally. Le: left power: 4.5/5  right: power [**3-6**].  Bilateral\\nsymmetric, reduced sensation distal LE to ankles.\\n\\n\\nPertinent Results:\\nAdmission Labs: [**2117-9-11**] 09:22AM\\nWBC-11.9* RBC-4.58 HGB-13.0 HCT-36.5 MCV-80* PLT COUNT-466*\\nLIPASE-22  ALT(SGPT)-10 AST(SGOT)-16 ALK PHOS-105 TOT BILI-0.5\\nGLUCOSE-260* UREA N-48* CREAT-2.7* SODIUM-137 POTASSIUM-4.9\\nCL-101 CO2-11*\\nLACTATE-1.9\\n\\nDischarge Labs: [**2117-9-16**] 07:10AM\\nWBC-6.8 RBC-3.67* Hgb-10.4* Hct-30.2* MCV-82 Plt Ct-298\\nGlucose-118* UreaN-20 Creat-2.3* Na-137 K-3.7 Cl-104 HCO3-23\\nAnGap-14\\nCalcium-8.7 Phos-3.5 Mg-2.0\\n\\nRadiology:\\nCXR: No evidence of pneumonia or other pathological\\nabnormalities. No\\npleural effusions. No pulmonary edema. Normal size of the\\ncardiac\\nsilhouette.\\n\\nMicrobiology: Urine culture negative, blood cultures no growth\\nto date, stool for C.difficile negative\\n\\n\\nBrief Hospital Course:\\n35 yo F with HTN & poorly controlled type I DM, c/b neuropathy,\\ngastroparesis, nephropathy ?????? CKD, retinopathy presents with DKA\\nand hypertension SBP to 200s.\\n.\\n# Diabetic ketoacidosis: Patient controls diabetes at home with\\nHumalog SS and long acting Levemir.  Sugars at home recently\\nhave been in 250s. In the ED, glucose was 466. UA was +ve for\\nketones ?????? corrected to 200s, but rose again to 300s. She was\\ntreated with an insulin drip which was transitioned to subq when\\nshe tolerated POs. Her electrolytes were repleted and she\\nreceived aggressive volume resuscitation. [**Last Name (un) **] saw her and\\ngave sliding scale recommendations which were implemented. No\\nsource for DKA found, beleived to be [**2-3**] gastroparesis. Nausea\\nmanaged with ativan, compazine, and promethazine. She was\\ndischarged on her home Insulin and sliding scale with\\ninstructions to follow-up with [**Last Name (un) **].\\n\\n# HTN: Hypertensive with SBP in 190s initially, attributed to\\nDKA, as she has experienced in the past. As she improved her\\nblood pressures normalized and she was re-started on her home\\nLopressor and Midodrine regimen.\\n\\n# Coffee grounds emesis: Emesis started off as clear, then with\\nprolonged wretching, she started having coffee-grounds vomiting.\\nThis had also occurred on prior admissions for DKA with\\nassociated vomiting. Her hematocrit remained stable and her\\nhematemesis self-resolved, and so work-up was deferred to the\\noutpatient setting.\\n\\n# Acute on chronic kidney disease, Stage III: Patient's Cr on\\nadmission was 2.7, trending down to 2.1-2.3 following fluids,\\nconsistent with her known CKD secondary to diabetic nephropathy.\\n\\n\\nMedications on Admission:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. Levemir 100 unit/mL Solution Sig: Twenty Two (22) units\\nSubcutaneous every AM.\\n3. Levemir 100 unit/mL Solution Sig: Twelve (12) units\\nSubcutaneous at bedtime.\\n4. Humalog 100 unit/mL Solution Sig: sliding scale as directed\\nSubcutaneous four times a day: Please use sliding scale as\\ndirected by MD [**First Name8 (NamePattern2) 767**] [**Last Name (Titles) **].\\n5. metoprolol tartrate 50 mg Tablet Sig: 1.5 Tablets PO DAILY\\n(Daily): take in the evening.\\n6. promethazine 25 mg Tablet Sig: 0.5 Tablet PO Q8H (every 8\\nhours) as needed for nausea.\\n7. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n\\n12 hours).\\nDisp:*60 Capsule(s)* Refills:*2*\\n8. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: Two (2)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily): Please take\\nonly 1 capsule daily (30 mg) for first 2 weeks of treatment.\\nDisp:*60 Capsule, Delayed Release(E.C.)(s)* Refills:*2*\\n9. oxycodone 5 mg Capsule Sig: One (1) Capsule PO every eight\\n(8) hours as needed for pain.\\n10. midodrine 5 mg Tablet Sig: 1.5 Tablets PO every four (4)\\nhours: Can hold while sleeping.\\nDisp:*270 Tablet(s)* Refills:*2*\\n\\n\\nDischarge Medications:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n12 hours).\\n3. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: One (1)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily).\\n4. metoprolol tartrate 25 mg Tablet Sig: Three (3) Tablet PO\\nOnce Daily at 6 PM.\\n5. midodrine 2.5 mg Tablet Sig: Three (3) Tablet PO DAILY\\n(Daily).\\n6. Levemir 100 unit/mL Solution Sig: As directed by [**Last Name (un) **] units\\nSubcutaneous As directed.\\n\\n\\nDischarge Disposition:\\nHome\\n\\nDischarge Diagnosis:\\nDiabetic keotacidosis\\nHematemesis (blood in your vomit)\\nHypertension\\nChronic renal insufficiency\\n\\nDischarge Condition:\\nMental Status: Clear and coherent.\\nLevel of Consciousness: Alert and interactive.\\nActivity Status: Ambulatory - Independent.\\n\\nDischarge Instructions:\\nYou were admitted to the hospital with DKA, hypertension, and\\nblood in your vomit. You were initially treated in the ICU with\\nan insulin drip, and your blood sugars improved. Your blood\\npressure medications were adjusted to better control your blood\\npressure while you were in DKA, but you were re-started on your\\nhome regimen at discharge. The blood in your vomit was likely\\nsecondary to mechanical trauma from repeated wretching, but you\\nshould follow-up with your primary care doctor to discuss\\nwhether you should undergo further evaluation such as an upper\\nendoscopy. Given your complaints of chronic cough and heartburn,\\nyou should also discuss beginning a trial of a proton pump\\ninhibitor such as Nexium or Prilosec to see if this helps your\\nsymptoms.\\n\\nYour insulin regimen was adjusted by the [**Last Name (un) **] team while you\\nwere here. You should continue to follow-up with them with any\\nquestions or concerns regarding your insulin management.\\n\\nFollowup Instructions:\\nPlease call Dr.[**Last Name (STitle) 805**]' office to schedule a follow-up\\nappointment within 7-10 days of discharge. Her office number is\\n[**Telephone/Fax (1) 85219**].\\n\\nYou should also continue to follow-up with your [**Last Name (un) **] doctors\\nas needed.\\n\\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import DistilBertTokenizer \n",
        "from transformers import AutoTokenizer \n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "KDo8ShVozdYx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ea8cd9a63d9642ac9fd3ab2efbc21e39",
            "dccc6d99748c4864aa45f69157ef9a88",
            "4185f14b94f34595860b8423db36c426",
            "b7a31430cf3440b592785268a07eed46",
            "7d8c677566124fc38393005aaaed4875",
            "b3d74212526147aa9d00b9ae71f68d6a",
            "f2837b51200d46878772d10116b48b8f",
            "68c45272fef445e380b6a473ea08f721",
            "aafca9973ac04fb7b2389d3842528c4b",
            "056b6f9ccbc1478bb38ce940dd589bee",
            "32c15fbd125d41a19477d0c08a81901a",
            "a14ddb82b6ae4cf4b68f78cafd13c4ef",
            "fc10731b33784f2887afe9a766b5adc5",
            "486f4a0e1056420cbfd0890e36985e48",
            "ce1c3f6c63cc4b838f2c6abe662046b5",
            "44312e6719e94822bd70a766e92816b2",
            "15a8cee741944578b738b5c3ecaec74a",
            "ddfa436cb642449aaa64d96d404d6f48",
            "362de3582b5e47bf990573f9321864c8",
            "f718913a03cd49aa9bcb971a52d3cea2",
            "677032e1ec42492c9c4f74cb7a699847",
            "fc6d8a890db44303849130d0e1221de9"
          ]
        },
        "id": "QPtCMO5_zddY",
        "outputId": "d701b987-cce2-453b-d4c7-57561e281d07"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea8cd9a63d9642ac9fd3ab2efbc21e39",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a14ddb82b6ae4cf4b68f78cafd13c4ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, phrase in enumerate(X):\n",
        "    print(i)\n",
        "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
        "                                   padding='max_length', add_special_tokens=True,\n",
        "                                   return_tensors='tf')\n",
        "    # add to zero arrays\n",
        "    Xids[i, :] = tokens['input_ids']\n",
        "    Xmask[i, :] = tokens['attention_mask']"
      ],
      "metadata": {
        "id": "56joPLOczdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WT4h0OCzdm2",
        "outputId": "bf417548-e6c8-4ee4-f0fb-eec9a11ecb03"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, y))\n",
        "\n",
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A7hvr890X0U",
        "outputId": "d2e85e79-e4e2-443e-d6f3-180e95190b78"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(512,), dtype=tf.float64, name=None), TensorSpec(shape=(512,), dtype=tf.float64, name=None), TensorSpec(shape=(6918,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(input_ids, masks, labels):\n",
        "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
      ],
      "metadata": {
        "id": "IFTiAmTm1QRB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(map_func)"
      ],
      "metadata": {
        "id": "BCdAsA021QWc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXSKw8gq1Qb_",
        "outputId": "566d663f-d2a8-4c7f-bf0d-12f31caa532b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.float64, name=None)}, TensorSpec(shape=(6918,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsON1y-n1iKa",
        "outputId": "7a06b503-fb06-4d6b-db28-59a24efc1f29"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 6918), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.9\n",
        "\n",
        "size = int((num_samples / batch_size) * split)"
      ],
      "metadata": {
        "id": "_axv82p_16sM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(size)\n",
        "val_ds = dataset.skip(size)\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "id": "wVmH1-y92DSd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV2gM5DnySB9",
        "outputId": "83688e63-7896-4d31-bf83-bfe621627300"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 6918), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up9tJJxvyUNB",
        "outputId": "3f9a7070-2998-4240-b4d6-d9ef8c89c7c7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 6918), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/gdrive/MyDrive/CSE_244B_W2022/project/train_ds\"\n",
        "val_path = \"/content/gdrive/MyDrive/CSE_244B_W2022/project/val_ds\""
      ],
      "metadata": {
        "id": "5ixIEAtfvX7n"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a dataset\n",
        "tf.data.experimental.save(train_ds, train_path)\n",
        "tf.data.experimental.save(val_ds, val_path)"
      ],
      "metadata": {
        "id": "0vZWcO0u2IyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "train_ds = tf.data.experimental.load(train_path)\n",
        "val_ds = tf.data.experimental.load(val_path)\n",
        "for elem in val_ds:\n",
        "  print(elem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBUG40-ouzLu",
        "outputId": "1fd5d22a-e4ea-4bcc-f591-d0f98f36e5e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   113.,  1460.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   128.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1120.,  1179.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3029.,  4759.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7367.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,   190.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1142.,  1180.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12233.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1113.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  2781.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,   190.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2908.,  1559.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4382.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   187.,  1161.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   172., 11470.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197.,  2328.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  1114., 14715.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1464.,  1103.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 12888.,  1548.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197., 19988.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  1408.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   128.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15892.,  1106.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1780.,  1355.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1185.,  5837.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3059.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   184.,  1477.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,  1195.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1106.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4426.,  1451.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9304., 10950.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1146.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8936.,   176.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1106.,  1126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1204.,  2620.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1395.,  1187.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1110.,  2418.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1830.,  6063.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1110.,  1107.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1185., 14402.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7035.,  1286.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1367.,   120.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 17536.,  2129.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   128.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12211.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1643.,  1542.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3431.,   188.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   172.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10880.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  4252.,  7877.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   123., 14949.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1988.,   123.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4725.,  1229.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1116.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19310.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,  1762.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1204.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1227.,  1314.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10496.,  2918.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3246.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  2068.,  4573.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1108.,  1228.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,   176.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,  1204.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 28027.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1155.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1477.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 23746.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6059.,  1934.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  2418.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1111.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2588.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   122.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1112.,  8508.,   102.],\n",
            "       [  101.,  2587.,  1185., ..., 14402.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3175.,  1106.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   110.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1111.,  1936.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,  4208.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101.,  2587.,  1185., ...,  2897.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10296.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1187.,  1119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1593.,   122.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1127.,  3102.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4091., 12999.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8598.,  6409.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1549.,  1120.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2292.,  1125.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,  1123.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  2781.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7289., 25550.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,  2032.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1119.,  5762.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  1103.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1116.,  1408.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6592.,  7409.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11096.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101.,  2587.,  1185., ...,  2999.,  2603.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2259.,  2568.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,   131.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101.,  2587.,  1185., ...,  5670.,  1679.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1218.,  1106.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1271.,  3434.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1915.,  3596.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,  1268.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   185.,  1204.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,   116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107., 17149.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   129.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1185.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5311.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  2016.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5351.,   112.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1197.,  6873.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1653.,  1892.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17536., 27631.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1204.,  3756.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  8050.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   173.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  1139.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   118.,   126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1572.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10024.,  1918.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  7880.,   177.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   184., 13894.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2527.,  1113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117.,   189.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1161.,   176.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118., 23971.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3908., 13335.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1106.,  1609.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1429.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4267., 19760.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1601.,  8310.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3058.,  6161.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 26588., 16219.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1159.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  1616.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1665.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1883., 15640.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 27631.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1204.,   125.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107., 11850.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  1348.,  9037.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   114.,   115.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   118.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24266.,  1193.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1186.,  3828.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2635.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   111.,   184.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2403.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1915.,  6111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   177.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1549.,  1488.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  7586.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4366.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1830.,  1665.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1119., 21167.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11811.,  1179.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11150.,   120.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2707.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,   174.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   187.,  1161.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2528.,  2093.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11083.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1193.,  3693.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   131.,   175.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4838.,  1110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174.,  1377.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1110.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13335.,  1183.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13306., 24716.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,  1664.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15731.,  1568.,   102.],\n",
            "       [  101.,  3288.,  5890., ...,  1132.,  1969.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   193.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6834., 20695.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119., 26360.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1495.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  6676.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1127.,  4366.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4798.,   122.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1607.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2999.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115., 22148.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1695.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   193.,   166.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7035.,  1132.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4841., 26996.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1134.,  1159.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9301.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120.,   176.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 15276., 15197.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1853.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 25550.,  1182.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,  5507.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   129.,  8898.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,  1137.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1361.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2724.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 21718., 27801.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   131.,  1744.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4404.,  8006.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1775.,  1104.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8212.,  2728.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11824.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 19122.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3385.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174., 26857.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   187.,  1161.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1406.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174.,  1679.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   129.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1679., 24123.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1607.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114., 25021.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17960.,  5048.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   171.,  3488.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   184.,  1477.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  3678.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  6138.,  1316.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117., 23123.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 12149.,  7232.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2489.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   182.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1114.,  5351.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1152.,  1455.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1893., 19860.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1407.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2841.,  2445.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8581., 26862.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 27250.,  4184.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1108.,  1879.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1853.,   102.],\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  1126.,   102.],\n",
            "       [  101.,  2587.,  1185., ..., 13622., 21470.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3101., 27262.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1120.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1152.,  6315.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1205., 25251.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131., 26410.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4252.,  7877.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   170.,  3621.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1170.,  1115.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1268.,  2223.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1260., 27444.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   113.,  8362.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  5034.,   114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1161.,  4366.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1695.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1165.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,  2450.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107.,  1763.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  1131.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17963.,   178.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5599.,  3976.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2320.,  5837.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1403.,  8127.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  6873., 11806.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   175.,  7897.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1549.,   188.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,  1664.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11019.,   129.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2913.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115., 13075.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1174.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1233.,   183.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  6997.,  1183.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  4252.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1117., 11431.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1742.,   125.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 16229.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1348.,  2539.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   166., 24928.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4900.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 16418.,  2050.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2403.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,  1996.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  8710.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   166.,   186.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,   122.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1271.,   113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5048.,  1736.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2571.,  1643.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,   126.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14491.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 25669., 11776.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4443., 14452.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   188.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   177.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8223.,  7393.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   185.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2716.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1545.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  3488., 20083.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 23984., 14525.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20488., 14850.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1268.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5351.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1123.,  5813.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4267.,  9180.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1367.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120.,   172.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1112.,  3365.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3377.,   173.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166., 22895.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 16996.,  1465.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1482.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  4252.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 17993.,  2988.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   128.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13119., 20497.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 20939.,  8277.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3189.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1204.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   187.,  1197.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6540.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,   125.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1322.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24690.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   192.,   120.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1808.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14741.,  1133.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,   172.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101.,  2587.,  1185., ...,  5526.,  7432.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   123.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1406., 17713.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1643.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1320.,  4313.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   118.,   125.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   124.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105., 11850.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1596., 22233.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1378.,  1146.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131., 21359.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   166., 22572.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1969.,  5246.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7777.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  8087.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   176., 21462.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22910.,  1200.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 10347.,  1403.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183.,  3491.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1665.,  1403.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 17713.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2628.,  1114.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 27481.,  2109.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 11955., 12233.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18369.,  1126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1204.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  6354.,  1233.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18593.,  3653.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5129.,   131.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   184.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  1940.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107.,  1197.,   102.],\n",
            "       ...,\n",
            "       [  101.,  2587.,  1185., ...,  2952.,  8179.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8006.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   110.,  1113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1117.,  1112.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18574.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1477.,  1762.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   172.,  6834.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20080.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,   173.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 25930.,  2229.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1185., 23563.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1427.,  1106.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3078.,   110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   189.,  1204.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 13217.,  6787.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1193.,   171.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 16278., 10542.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   118., 14516.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,   187.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13306.,  1171.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1285.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 10423.,  1571.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1714.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1714.,  1586.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6582., 11776.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  5806.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 25550.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1144.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103., 11477.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1596.,   171.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183.,  2952.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   172.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   117.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1251.,  2988.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,   120.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1400.,   124.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1215.,   180.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 16554.,  1358.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 28027.,  1580.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4725.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18965.,  6006.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117.,  2999.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1324.,  8074.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,  1117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 11478.,  4894.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5942.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1476.,  1201.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1348., 18965.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   178.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1104.,  3614.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  5048.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1495.,  3729.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117., 27481.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1196.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1679.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   120.,  2625.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  2112.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115., 21640.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4184.,  7111.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14494.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2382.,  1106.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2145.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19614.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12123.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  9964.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1106.,  5427.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11551.,  1108.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197., 13836.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4832., 23055.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   122.,  1214.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  5351.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,   170.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1181.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6512., 24818.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  5073.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,   178.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 21174.,  1204.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1136.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   177.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  3236.,  2277.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 10468.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10296.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6066.,   172.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  8508.,  4854.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5037.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4184., 25362.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2093.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2386.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  1168.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,   174.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1200.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1811.,  7152.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   113.,  8362.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 25575.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1106.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1406.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11048., 15661.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1218.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1643.,  5706.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4290., 24438.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1119., 26360.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18871., 10294.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4870.,  1106.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,  8756.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 23123., 10024.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1813.,  1197.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3243., 12647.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   188.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,  2286.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  1576.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1779.,  5165.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1111.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1830.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2109.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1679., 22398.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1129.,  3718.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  7648.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11122.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   127.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1144., 23137.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   170.,  1842.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 25550., 17972.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7393.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   170.,  1974.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1489.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   171.,  9952.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1566.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3372.,  1106.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   172.,  1777.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   185.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 22572.,  2087.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1955.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1107.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8671., 21810.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  1111.,  3252.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1775.,  1120.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1197.,  5385.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1853.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,   171.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  5942.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12104., 13632.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1549., 17496.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105., 18152.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1231.,  7050.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  5241.,  1395.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4832., 10387.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1126., 20504.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1892.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,  5674.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1603.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,  2450.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19268.,  1183.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1627.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1105.,   173.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1179.,  1477.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1185., 11139.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   120.,  2448.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1665.,  1306.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3452.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   130.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119., 10427.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1286.,  1981.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2229.,  2997.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5822., 18574.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1103.,  1378.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2211.,  4252.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1964.,  1177.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 16219.,  1863.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1742.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174.,  3101.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   127.,  1233.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2734.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   125.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1137.,  3073.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4902.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1830.,  1665.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   172., 25937.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24928., 11955.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 20557.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   177.,  1403.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5351.,  1108.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1111., 10298.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2229.,  2489.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1580.,   113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1108.,  2991.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1133.,  2418.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,  1108.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 23123., 10024.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  8204.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3586.,  1105.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 22882.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10024.,  1918.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8974.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 16516.,  4567.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4816.,  6718.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17927.,  6354.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1673.,  8191.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2875.,  5025.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  2991.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   190., 11811.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2345.,  1113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   184.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113., 25338.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   125.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1114.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2781.,  2112.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119., 16278.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1197.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   123.,  3824.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 27291., 24928.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1884., 12888.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  9382., 23123.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1405.,  9952.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18881., 14044.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183.,   112.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 14255.,  1204.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8643.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2692.,  4063.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8009.,  2772.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   189.,  1830.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2109.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1119., 26360.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7035.,  9874.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 17713.,   186.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2193.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8401., 11428.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1762.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  2799.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2908.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1103.,  2652.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1603.,  1757.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6834.,  1548.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1268.,  1289.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1631.,  1126.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1103.,  1286.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1119.,  1145.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5552.,  5600.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1110., 21461.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5521., 17536.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   135.,  3434.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  2704.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8005., 10879.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1154.,  1103.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3596.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105., 21852.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,   170.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  7618.,   173.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1129.,  1682.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1106.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 18871., 10294.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1137.,  1894.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174.,  1377.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5942.,  4573.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 16042.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1377.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107.,  1123.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1105.,  2028.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5306.,   110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181., 10973.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1118.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   176., 21263.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  4120.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1121.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1107.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1204.,  1179.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2230.,   113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   122.,   171.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,  1148.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115., 20915.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22148.,  1527.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 12686.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4907.,  5552.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1298.,  1104.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1268.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,  1108.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   131.,  8264.,   102.],\n",
            "       [  101.,  1271.,   131., ...,   188., 25265.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5306.,  6745.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1777.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1207.,   177.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   123.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4517.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   114.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117., 27481.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6055., 14516.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2116.,  1107.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,   124.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2344.,  1110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   170.,  3712.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  5822.,  7777.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2366.,  1443.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1155.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  4573.,   110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4870.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  1185.,  3213.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 12692.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1151.,  1515.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   193.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1475.,  1407.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5182.,  1114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183., 16430.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  6451.,   114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1607.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15207.,  1114.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   113.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1148.,  1271.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1123.,  7621.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   120.,  8716.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  9947.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1105.,  4348.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11439.,  7889.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2999.,  3586.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   170.,  1227.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1643.,   185.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1974.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  3236.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   130.,   114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1775.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   187.,  1830.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8766.,  1830.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1408.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9324.,  1616.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   120.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1403.,  3828.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1821.,  3113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4729., 14618.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,  1477.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 13963.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1964.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   124.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2497.,  3161.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1675.,  2452.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4850.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1314.,  1480.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1185.,  1748.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1105., 15207.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17670.,  1161.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 14987.,   173.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7563., 15622.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   118.,  3140.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1106.,  1243.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   177.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  5351.,  1253.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3127.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1246.,  1134.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2539., 17713.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11850.,  1361.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1477.,  9377.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 14467.,  7867.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20844.,  1964.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131., 28117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  1678.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4667.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   188.,  3121.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 23327., 26052.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1193.,   170.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 1, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1607.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1403.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1231., 25461.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1145.,  3756.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1626.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14494.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1112.,   182.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7231.,  1406.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10296.,   131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,  1127.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   184.,  1884.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  1185., 22895.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1107.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5048.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1884.,  2646.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 23651.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24928.,  1403.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  9556.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1211.,  8080.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1934.,  1607.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   112.,   188.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1108.,  5385.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8508., 11412.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,   131.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   120.,  1476.,   102.],\n",
            "       [  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1152.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   173.,  6834.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1775.,  8005.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1603.,  1757.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   118., 16183.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  2330.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 26795.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2589.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20915.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   189., 19366.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   122.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   193.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  1814.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   185.,  1605.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   187.,  1197.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6066.,   185.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1851.,   110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,  5208.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15937.,  1425.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 19091., 16442.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5412.,   173.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   177.,  1403.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19310.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1127.,  1554.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1512.,  1126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  5306.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   121.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   172.,  4163.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  1464.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1116.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  5172., 12211.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1607.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  1353.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2241.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   136.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  8682.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   177.,  1775.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 1, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,  5306.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24486.,  1665.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4063.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  6138.,  6111.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12602.,   174.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   189., 19366.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   188.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  2129.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24507.,  1116.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  9071.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1201.,  2403.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1676.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1197.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13200.,  1465.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1117.,  2495.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7404.,  5765.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1405.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   181., 22106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   113.,   189.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1849.,  1107.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  5706.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 26601.,  1918.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20519.,   185.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  7880.,  8643.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14491.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   128.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   118.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1111.,  1134.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3507.,  4807.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  7967.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1936.,  1884.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4179.,  3318.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   113.,  2856.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15937.,  1425.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6126.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   189.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 12686.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,   177.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4611.,  4724.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2568.,  2946.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1348.,  1444.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 17713.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183., 11439.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5521., 17536.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1852.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1306.,  1403.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12734., 13064.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1559.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  1401.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1120.,  1313.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 26360.,  2793.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   113.,  5351.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,   181.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  4267., 20080.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24928., 11955.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 1, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1964.,  1233.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5768., 15631.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3545.,  1104.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1110.,  2418.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1214.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1519.,  7111.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1106.,  1129.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12211.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  1241.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1204., 17428.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  9808.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3498.,  2528.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1275.,  2312.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2913.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2130.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  3383.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,  3245.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  5172.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 23235.,  1290.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3805.,  1131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   189.,  1465.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1659.,   113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6275.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1112.,  1103.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1123.,  1920.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7939.,  1549.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   118.,   126.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1324.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120., 16530.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   193.,   122.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4393.,   181.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1679.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 16664., 14846.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1479.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1126.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4214.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11965.,   171.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7535.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   118.,  4533.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4287.,  1115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1129.,  1167.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 13286.,  1155.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2793., 16320.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1616.,  1105.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3112.,  1607.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1116.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   183.,  1204.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1161.,  1934.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1682.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  8661.,  2879.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,   176.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  5618.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   176.,  3069.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   193.,  3740.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1197.,  1627.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1940.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2366.,  2603.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 14516., 21919.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2916.,  6834.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  8006.,   102.],\n",
            "       ...,\n",
            "       [  101.,  2587.,  1185., ...,  1111.,   178.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19856.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1298.,  1106.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   123.,  3824.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1123., 23123.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,  1271.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2599., 21844.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   126.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22910.,  1200.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1107.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1115.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1297.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1395.,  1586.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1549.,  2229.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10468.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,   172.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5246.,  1679.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1286., 21828.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1104.,  1117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1195.,  6354.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,   187.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  7810.,  1193.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   127.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1110.,  3385.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1113.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  4742.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7791.,  1982.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   122.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   135.,   129.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10024.,  1665.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  8239.,  1891.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,  1314.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1674.,  1136.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  8340.,  1108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119., 26360.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  4870.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 18901.,  2093.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6006.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3070.,  2568.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   129.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,  1107.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10209.,  1105.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1219.,  1142.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6145.,  1114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7779.,   193.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2116.,  1114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1116., 10182.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   177., 24312.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131.,  8682.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13753.,   174.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1193.,  3052.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1554.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1125., 23657.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1643.,  1830.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4120.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19515.,  1703.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1110.,  2418.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   131.,  5351.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2455.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1186.,  1477.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1596., 12935.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1127.,  1439.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120.,  2625.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1268.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5832.,  3653.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10122.,   170.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   113.,  1271.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3976.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1129.,  4554.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 13119., 20497.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4863.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   113.,   122.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   171.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1170., 24992.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1969.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,  1204.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101.,  2587.,  1185., ...,  3805.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10387.,  1742.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   185.,  3069.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  7621.,  2068.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7877.,  9084.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3220.,  1105.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2495.,  5822.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3434.,  1233.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   187., 11096.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1477.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4422., 23123.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1329.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1257.,  3210.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  3236.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1231.,  2155.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 23901., 22233.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   126.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1549.,   122.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7035.,  7808.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8683.,  5822.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1110.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115., 22588.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2087.,  1104.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2411.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3975.,  1964.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1495.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1884.,  4934.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  8340.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3452., 17792.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   130.,  3330.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   186.,  1285.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1405.,  1306.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1548.,  2032.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2767., 26754.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1174.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5855.,  4638.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 25098.,  2913.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 15504., 14516.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 1, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1830.,  1665.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12736., 21462.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  2704.,  1736.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 24024.,  1127.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1110.,  2609.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   164.,   193.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  2922.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1200.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 12415.,  7880.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3112.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20839.,  1777.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   126.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10510.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1105.,  1108.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1899., 11239.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1429.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   114.,  3140.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1151.,   170.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1204., 11889.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22192.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1679., 21089.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24928.,  1403.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  9016.,   176.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1204.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7125.,  1883.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   187.,  2599.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,  1134.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19557., 18965.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,   130.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2109.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1429.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 14850.,  4907.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5422.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1113.,  2080.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2606.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1883.,  3807.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1106.,   184.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13335.,  2881.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1131.,  1144.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,     0.,     0.,     0.],\n",
            "       [  101., 10296.,  2236., ...,  1183.,   108.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1361., 10936.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  1119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22723.,  1495.,   102.],\n",
            "       [  101.,  2236.,  1104., ...,  1137., 23097.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4584.,  3252.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9014.,  8191.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,  1893.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 17713.,   186.,   102.],\n",
            "       [  101.,  2587.,  1185., ...,  3385.,  1111.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1216.,  1112.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  6062.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  3175.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   172.,  1121.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3325.,   188.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10468.,  1329.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 21943., 18574.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,   164.,   102.],\n",
            "       [  101.,  2587.,  1185., ..., 22904.,  8167.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   115.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   188.,   120.,   102.],\n",
            "       [  101.,  1271.,   131., ...,   172.,  4934.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   178.,  1964.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2772., 20917.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4419.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   131.,  2588.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   170., 12148.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  4267.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101.,  2587.,  1185., ...,   131.,  2999.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   182.,  1665.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2093.,  8191.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 21073.,  1181.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 27466., 14721.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   181., 26868.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1120., 13119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   166.,  1115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7877., 15455.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1114.,  4366.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1133.,  1144.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1114., 11019.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 13753.,  1112.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5552.,  1396.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10649.,  6834.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2130.,  1150.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2370.,   113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   181.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2495.,  2728.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1185.,   177.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   120.,  2455.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9299.,  1166.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1286.,  1981.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 10294.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   193.,   166.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1821.,  1105.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1181.,   186.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1114., 19122.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   164.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2793., 10880.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8183., 21177.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 19192.,  2781.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1679.,  1285.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1286.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,   173.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   172.,  4163.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   116.,   185.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1830.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117.,  1110.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24119.,  1120.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1110.,  1562.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1175.,   102.],\n",
            "       [  101.,  2587.,  1185., ..., 23123., 10024.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1105.,  9302.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   108.,  1492.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   113.,   164.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   120.,   185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  2623.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11955.,   131.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1607.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 24312., 23826.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14255.,  2050.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2997.,  1104.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7779.,   193.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20636.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1973.,  1113.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  4910.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1733.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  5199.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1103.,  3908.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1534.,  1105.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2445.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2608., 20557.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14741.,   117.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  3385.,  1467.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1162.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 21040.,  1477.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1402.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1183.,  5674.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1145.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   120.,   187.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,  1314.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2386.,  4911.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   117., 13753.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1168.,  5789.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  2991.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  6791.,  1137.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 26777.,  1518.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 22578.,   117.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 14459.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4943.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   129.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1105.,  2236.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1559.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1106.,  2080.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1231.,  7050.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   183.,  9654.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   174.,  8508.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1201.,  2403.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   117., 16278.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   131., 26707.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2774.,  1279.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9285.,   171.,   102.],\n",
            "       ...,\n",
            "       [  101.,  2587.,  1185., ...,  1904.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2246.,  3596.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5627.,  1107.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1107.,  8828.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1121.,  7866.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  4275.,   127.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   166.,  2455.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1852.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 20636.,   118.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1231., 21240.,   102.],\n",
            "       [  101., 10572., 14940., ..., 10427.,  2130.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   126.,   118.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ..., 13830., 10805.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  3756.,  7262.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118., 16308.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 14846.,  2386.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   191., 27547.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1131.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  1185.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1106.,  1248.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   182.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  3052.,  1120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1233.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   131.,  8264.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1107.,  3309.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1116.,  1137.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1231., 20064.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 18570., 12985.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,  2704.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   126.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1757.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1108.,  1408.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 26360., 20295.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1582., 16071.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9301.,  5300.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  5385.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 11048.,  4182.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1127.,   131.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4252.,  7877.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5899.,   110.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5311.,   119.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1964.,  7641.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  5165.,  1114.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   123.,   119.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,   119.,  1123.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1513.,  2765.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   115.,   115.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   175.,  7897.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1120.,   164.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1679., 24123.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ..., 10456.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1527.,   118.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14255.,  4814.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  8649.,  1193.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   177.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   179.,  1964.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1121.,  1476.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   122.,   115.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 12745.,  1106.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   126., 17713.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9324.,  1616.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1151.,  1113.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  2492.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  2492.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17688.,  5855.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1424.,   131.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1277.,  1106.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  6066.,  1465.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  4252.,  1204.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 17683.,   120.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,   130.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  1665.,  1643.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  7779.,   117.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  1104.,  5318.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  1822., 16557.,   102.],\n",
            "       [  101., 10296.,  2236., ..., 14375.,  1116.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  8828.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,   119.,  2231.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  4942.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   119.,  1103.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n",
            "({'input_ids': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[  101., 10296.,  2236., ...,  9081.,  1477.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  9964.,   119.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   118.,   181.,   102.],\n",
            "       ...,\n",
            "       [  101., 10296.,  2236., ...,  2445.,  1137.,   102.],\n",
            "       [  101., 10296.,  2236., ...,  8745., 12576.,   102.],\n",
            "       [  101., 10296.,  2236., ...,   170.,  5303.,   102.]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=float64, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.]])>}, <tf.Tensor: shape=(16, 6918), dtype=uint8, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j7qylpBRyqzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create model"
      ],
      "metadata": {
        "id": "Io20NXv6y93E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = y.shape[1]\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWOSaI2m1All",
        "outputId": "7e2412f6-1f15-4c36-f546-285d6f3b405f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6918"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT Model"
      ],
      "metadata": {
        "id": "3b__H277HIVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel, TFBertForSequenceClassification\n",
        "bert = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "#bert = TFAutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "#bert = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "4bc92657cee34705833178195d1154f4",
            "8ddc813ade7a4a08b92becb1315ff110",
            "a8a8669974e947358612067f6546ec2a",
            "f68ed96dceab421ca0beed203fa69e43",
            "7402a4ee46174cf58cd76b104a16506c",
            "c8713c78f66a46aab8417821f1e5d57b",
            "037a3360953249d5b5b083f1e7a1255e",
            "890b1aff425a43adbeade261b255a0a2",
            "65d04c9204d846debda066ab59526f74",
            "bf1c9571d97a4385a3e351cb82b18a8e",
            "18abb3254b2649ac95f9e47ff9cc177a",
            "5ea0efa4c93d4c0183c63a129200c801",
            "d2e9a975483c43f89daaf6d744c1a003",
            "cce224aa4a6645f8b797386f961bd2c6",
            "e89c14ac2c384647bf51b84d459470ae",
            "3b2d763527a44e6eb198f5d02b73d642",
            "737252d098834af484f5279207803952",
            "214e7b4b15fa4371b4c356bcc6a7fc84",
            "2374a933323544fe8bdcc6d36f3b065e",
            "fb8e25f452bd4221a584278fdc3ba9f8",
            "b98a8c84c0c645c0a7ba1db76d17a4de",
            "5fa5abe7f60542d796f4228ebe5271ff"
          ]
        },
        "id": "-nZC2Khyy_F-",
        "outputId": "55922f7a-758a-40a9-b571-e13d9e999f0c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bc92657cee34705833178195d1154f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ea0efa4c93d4c0183c63a129200c801",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFTYgJ2zX_e",
        "outputId": "2997ce2d-96c4-4ab1-b97a-507253037f99"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,482,240\n",
            "Trainable params: 109,482,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32') \n",
        "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "\n",
        "embeddings = bert.bert(input_ids, attention_mask=mask)[1]\n",
        "\n",
        "#x = tf.keras.layers.Dense(1024, activation='relu')(embeddings) # training is 2 days rn\n",
        "#y = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='outputs')(x)\n",
        "y = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='outputs')(embeddings)"
      ],
      "metadata": {
        "id": "l6lzYCKFzZcM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs=[input_ids, mask],  outputs=y)\n",
        "\n",
        "# if you dont want to train the 100M bert params\n",
        "model.layers[2].trainable = False\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36DCVVSC28wg",
        "outputId": "9b40afd4-a665-4536-c5f9-3e7ba28cbc81"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 6918)         5319942     ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 114,802,182\n",
            "Trainable params: 5,319,942\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Model"
      ],
      "metadata": {
        "id": "hl6BVsRlHMQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32') \n",
        "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "\n",
        "x = tf.keras.layers.Conv1D(128, 3, activation='relu')(input_ids)\n",
        "x = tf.keras.layers.MaxPooling1D()(x)\n",
        "x = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling1D()(x)\n",
        "x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)\n",
        "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "y = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='outputs')(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "e9RkXMM-HWjR",
        "outputId": "cd7b23bf-ab03-4a38-ca22-93a7a37866d9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-a18cc8b1ceba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    229\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32') \n",
        "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(input_ids)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "y = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='outputs')(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "wP-WZZOXJ0wk",
        "outputId": "c8b6828a-28a2-4a0f-b260-ecc4a9341b47"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-43707eae20a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    215\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                          \u001b[0;34mf'expected ndim={spec.ndim}, found ndim={ndim}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"bidirectional\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# screw everything just do classification on the X input statements like the fake news detection hw\n",
        "# create the model\n",
        "# https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "embedding_vecor_length = 32\n",
        "top_words = 5000\n",
        "max_diagnosis_length = 512\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32') \n",
        "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "x = tf.keras.layers.Embedding(top_words, embedding_vecor_length, input_length=max_diagnosis_length)(input_ids)\n",
        "x = tf.keras.layers.LSTM(64)(x)\n",
        "y = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='outputs')(x)"
      ],
      "metadata": {
        "id": "gx6xU3xHHWoQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs=[input_ids, mask],  outputs=y)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naHQ85g4HWts",
        "outputId": "e4ccb043-2d0b-4a68-9445-ab09512a5425"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 512, 32)      160000      ['input_ids[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  (None, 64)           24832       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 6918)         449670      ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 634,502\n",
            "Trainable params: 634,502\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "dhSMWil2HXWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
      ],
      "metadata": {
        "id": "e-WkxNY63Nj8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
      ],
      "metadata": {
        "id": "L7LR0kOI7wvh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMPXRvuS723M",
        "outputId": "e710a161-a9ef-4633-aa9e-d464facd87ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2965/2965 [==============================] - 320s 107ms/step - loss: 0.3675 - accuracy: 4.4266e-04 - val_loss: 0.1416 - val_accuracy: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save()"
      ],
      "metadata": {
        "id": "G_TgEX_f78VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aIUPncKYHNpC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "wE_t5ajlLhZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peaHal7LLlmn",
        "outputId": "5d210656-e386-45ba-a6a3-d7bac84b8ed3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Accuracy vs Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend(['acc', 'Val_acc'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "gB3oTgzQLiwL",
        "outputId": "0ed18651-e44a-45e9-f314-69474685071f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "MR8Nz2LvLrDS",
        "outputId": "8346d93e-0311-4327-a796-f5273dcbf51d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "odOT24adLsVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
